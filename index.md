---
layout: post
title: Truth
description: The Chronicles of Computation, Vol. I
---

[The Chronicles of Computation](https://thechronicom.github.io), Vol. I

<div class="frame glow-medium" id="summaryframe">
  <p><b><i>Fig. Personalized overview.</i></b> </p>
  <p>There is no such a thing as a perfect summary, as each reader tends to be interested in slightly different topics. That said, we attempt to strike a balance through several summaries tailored to specific interests.</p>
  <center style="padding: 1em">
    <p>I am interested in <select class="summary" id="summaryselect">
    <option value=0 selected></option>
    <option value=1>formal alignment research</option>
    <option value=2>prosaic alignment research</option>
    <option value=3>rationalism & metaethics</option>
    <option value=4>metascience & field-building</option>
    </select></p>
  </center>
  <p id="titlesummary"><b><i>Abstract</i></b></p>
  <p id="psummary"></p>
  <p id="titleexcerpts"><b><i>Excerpts</i></b></p>
  <div id="summaryexcerpts"></div>
  <p id="titlereviews"><b><i>Reviews</i></b></p>
  <div id="summaryreviews"></div>
  <center>
  <div class="advice">For the best experience, view full screen on desktop.</div>
  </center>
  <script src="{{ site.baseurl }}/assets/js/summary.js"></script>
</div>

## Note to the Reader

**This is a work in progress. Please refrain from sharing it before it gets published, for ensuring an optimal reading experience.**

The document you just stumbled upon is largely a _chimera_ which brings together features of seemingly incompatible media artifacts. First, it incorporates the literary license of a novel, in order to facilitate the occasional act of daring idealism. Second, it incorporates the subject matter of an academic paper, with each chapter documenting original research at the intersection of machine learning, epistemology, and metaethics—be it conceptual, theoretical, or applied work. Third, it incorporates the [reactivity](https://dynamicland.org/) of web pages, with occasional [interactive widgets](https://distill.pub/2020/communicating-with-interactive-articles/) and [explorable explanations](https://explorabl.es/) being interweaved with plain text, besides plenty of links being scattered around. Accordingly, it is _by far_ best viewed on desktop and in full screen.

This "mixed media" concoction might make it daunting to engage with the artifact at hand, and for good reason. However, approaching it as a _short book_ appears to elicit the most appropriate expectations, helping the reader reliably take advantage of the best of all worlds. Indeed, the "Save to PDF" version of the artifact weighs in at around a hundred pages, depending on the page layout. Given this, churning through the entire document in one sitting is generally advised against. Instead, individual chapters make for much more manageable chunks—roughly mapping to separate academic papers coming together in a complementary constellation. One should expect a leisurely read of the whole artifact—one which includes playing with explorables, as well as following a few linked rabbit holes—to take three or four hours. That said, skimming might only take one or two hours, while exploring each and every nook might take forever.

Due to the interdisciplinary nature of the present endeavor, each reader will inevitably be more familiar with _some_ concepts than others. Given this, side notes are used extensively throughout the volume, in an attempt to elaborate on domain-specific jargon, although they can also house tangential curiosities.{% include sidenote.html id="mn-justsidenotethings" note='Such as this one, inspired in formatting by [Edward Tufte\'s](https://edwardtufte.github.io/tufte-css/) books.'%} That said, due to building on frameworks as vast as e.g. network theory, enactivism, or defeasible logic, we are forced to limit ourselves to brief, local contextualizations of the concepts being invoked, but do direct readers towards more comprehensive resources in order to satisfy their potential interest.

The present artifact is also a _living_ one, with minor revisions potentially being made over time. Following the meatier main chapters, the appendix also describes several actionable ways of engaging with the team behind the present artifact—and future ones like it—by e.g. asking clarifying questions, potentially backing future work, etc. Until then, have a pleasant read.

## Table of Contents

In the first half of this volume, we slowly build towards a theoretical framework of language model reasoning centered around dialectics,{% include sidenote.html id="mn-dialecticsalchemy" note='The term can refer to a multitude of things. Nicholas Rescher opens his _Dialectics_ by arguing that "it is, as it were, the alchemy of philosophy. It is all things to all men: to some, the most rigorous procedure for exact and cogent thinking; to others, a way of getting outside the established rules—an "anything goes" process for breaking through to unfettered innovations of thinking. For some it is the quintessential method of inquiring thought, for others the quintessential antimethod." We limit ourselves here to the meaning of a regimented dialogue between parties which is generally held explicitly.'%} the age-old practice of truth-seeking through regimented dialogue. In the second half, we attempt to apply the framework by exploring a number of specific use cases in AI safety.

### Ch. I, Dialectical Power Dynamics

In the first chapter, we devise an automated way of evaluating parties engaged in a debate of arbitrary length held in natural language. Our algorithm is inspired by the argumentation-theoretic notion of pragmatic validity and the epistemological notion of coherentism, yet its concrete implementation relies on heuristics for node "authority" from network theory.

1. [The Kaleidoscope of Reasonableness](#the-kaleidoscope-of-reasonableness)
2. [Beliefs as Means or Ends](#beliefs-as-means-or-ends)
3. [Carving the Algorithm](#carving-the-algorithm)
4. [_ArgRank_](#argrank)

### Ch. II, Deliberative Arms Race

In the second chapter, we describe the process of obtaining _DebateGPT_, a language model fine-tuned to simulate increasingly pertinent debates by attempting to excel at the previously described evaluation. This novel training regime incorporates a self-play paradigm, runs mostly on synthetic data, and we assess has a chance of bootstrapping language model reasoning into superhuman territory, assuming a number of probable future advancements.

1. [Brief Review of Language Models](#brief-review-of-language-models)
2. [Obtaining _DebateGPT_](#obtaining-debategpt)
3. [The Elephant in the Weights](#the-elephant-in-the-weights)
4. [The Kinetics of Reason](#the-kinetics-of-reason)
5. [Climbing Schild's Ladder](#climbing-schilds-ladder)

### Ch. III, Defeat & Defense

In the third chapter, we continue by framing the reasoning capabilities of language models such as _DebateGPT_ in terms of _bounded defensibility_, the amount of computational "firepower" a grouping of arguments can withstand before being defeated. The amount of time at their disposal, the number of available tries, and the reasoning capabilities of the (simulated) agents in question represent some of those bounds.

1. [Brief Review of Non-Monotonic Logic](#brief-review-of-non-monotonic-logic)
2. [Argument Is War](#argument-is-war)
3. [Bounded Defensibility](#bounded-defensibility)

### Ch. IV, Deployment Strategies

In the fourth chapter, we then go on to suggest a number of applications of this theoretical framework in AI safety which attempt to synthesize several avenues of investigation: debate, simulators, interpretability, interaction games, long reflection, shard theory, and others. These deployment strategies are meant to scale in synchrony with the bootstrapped reasoning capabilities hinted at in previous chapters.

1. [Brief Review of Alignment](#brief-review-of-alignment)
2. [Building on Cyborgism](#building-on-cyborgism)
3. [Building on Simulators & Assistance Games](#building-on-simulators--assistance-games)
4. [Building on Long Reflection](#building-on-long-reflection)
5. [Connections to Logical Inductors & Classical Debate](#connections-to-logical-inductors--classical-debate)

### Ch. V, Analysis

In this final chapter, we attempt to gauge the feasibility of the applications described previously, and investigate solutions to potential issues.

**Those dummy sections are meant to temporarily showcase how AISC8 contributions might be integrated with prior work. Dummy titles attempt to highlight the various types of potential contributions, rather than prescribe particular ones. Depending on progress and output, it might also be possible to (1) dedicate an entire chapter to one contributor, or (2) integrate newly contributed sections into existing chapters, on a case-by-case basis.**

1. Beyond Coherentism
2. Benchmarking _ArgRank_
3. Benchmarking _DebateGPT_
4. Towards Reconciling Formal & Informal Logics
5. On the Permissibility of Progressivism
6. Is (Automated) Metaphysics Possible?
7. Latent Knowledge Between Analysis and Synthesis

### Appendix

Finally, we zoom out of object-level technicalities and focus instead on concrete ways in which readers can engage more actively with the present volume, as well as with upcoming ones.

1. [Future Work](#future-work)
2. [Evergreen Discussion](#evergreen-discussion)
3. [The Chronicler Apprenticeship](#the-chronicler-apprenticeship)

## Acknowledgements

Paul Bricman would like to thank the [Long-Term Future Fund](https://funds.effectivealtruism.org/grants?search=paul+bricman&sort=round) for financial support during the project, [Stability AI](https://stability.ai/) for providing the computational resources necessary to train _DebateGPT_, [Conjecture](https://www.conjecture.dev/) for providing the space to explore related ideas during a previous research fellowship, [AI Safety Camp](https://aisafety.camp/) for providing context and resources for the entire team to conduct their investigations, [Alexander Gietelink Oldenziel](https://www.lesswrong.com/users/alexander-gietelink-oldenziel) for stimulating discussion on reasoning and epistemics, [Adam Shimi](https://www.lesswrong.com/users/adamshimi) for instilling an acute awareness of employed ontologies, [Anna](https://en.wikipedia.org/wiki/Anna%27s_Archive) and [Alexandra Elbakyan](https://www.sci-hub.st/alexandra) for infrastructural support, [Amber Dawn](https://amber-dawn-ace.com/) for patient editing, external advisors for helping refine the transparency level, as well as the authors of all prior work which this volume remixes. All that is lacking or in excess is his error.

`*others' acknowledgements go here*`

## Ch. I, Dialectical Power Dynamics

### The Kaleidoscope of Reasonableness

Ask a scholar of pure mathematics, computer science, or formal logic what makes an instance of argumentation valid, and they will likely highlight the relevance of making sure that the conclusion follows logically from the premises for each individual reasoning step. More often than not, this implies relying on a host of approved types of inference (e.g. [modus ponens](https://en.wikipedia.org/wiki/Modus_ponens)), while making sure to steer clear of degenerate ones (i.e. fallacies). This conception of reasonableness is often referred to as "geometrical," due to its implicit call for only building on solid premises and constructing arguments using an idealized set of operations.{% include marginfigure.html id='straightedge' url='/assets/img/pentagon.gif' description='[Straightedge and compass constructions](https://en.wikipedia.org/wiki/Straightedge_and_compass_construction) involve the challenging creation of varied shapes using a limited set of legal moves—here, a pentagon.'%}

Ask a scholar of argumentation theory the same question, and—following a cursory smile indicating they have long been waiting for this—they will likely not hesitate to point out that there have been multiple prominent schools of thought over time which have advocated different, often incompatible, conceptions of reasonableness. Each one is backed by a different rationale, has its own features and shortcomings, and has emerged in a different cultural setting, often separated by thousands of years and kilometers. Let us briefly sample this kaleidoscope of conceptions.

For instance, Perelman and Olbrechts-Tyteca suggested a conception of reasonableness grounded in rhetoric. According to this conception, an instance of argumentation is valid if and only if it succeeds in persuading a group of individuals of its conclusion. While the most visible shortcoming of this conception is that it can quickly degenerate into sophistry,{% include sidenote.html id="mn-sophistry" note='The [sophists](https://en.wikipedia.org/wiki/Sophist) were teachers of rhetoric in ancient Greece, notorious for "equipping" individuals with techniques for making a strong case in court, regardless of the truthfulness of their position. They were sneered at by virtually all their contemporary philosophers, who frowned upon for not seeking wisdom, but merely monetizing the skill of persuasion as a means of taking advantage of others.'%} its proponents highlight the possibility of grounding reasonableness in the persuasion of a particularly _rational_ audience. This litmus test can be further extended to involve the persuasion of an idealized omniscient agent, but also of oneself, by framing self-deliberation as self-persuasion.

<div class='epigraph'><blockquote><p>Indeed, the object of the theory of argumentation is the study of the discursive techniques allowing us to induce or to increase the mind's adherence to the theses presented for its assent.</p>
<div>Chaïm Perelman & Lucie Olbrechts-Tyteca,<cite> The New Rhetoric</cite></div></blockquote></div>

As a different example, Toulmin and the school of thought which emerged around his ideas advocated for a conception of reasonableness which incorporates domain-specificity. If the geometrical conception often requires abstracting statements into propositional atoms (i.e. $$P$$ could equally well denote _"All men are mortal."_ and _"Socrates is a man."_), Toulmin argues that arguments are often _substantial_, relying on domain-specific means of warranting conclusions, as opposed to standardized  _analytical_ operations on abstracted symbols. For instance, the fact that a study in the natural sciences has conformed to best practices in terms of replicability and reproducibility can be used to back its findings. In contrast, people working in pure mathematics might not rely on peer-reviewed empirical studies to back theorems, but might want to verify proofs using specialized software. The practices of ensuring sound reasoning in finance are yet again different, relying more on computer simulations and historical performance. Several spin-off schools of thought echoed Toulmin's dissatisfaction regarding the limited practical reach of highly-analytical formal logic, including the slightly hectic field of _informal logic_.

<div class='epigraph'><blockquote><p>A man demonstrates his rationality, not by a commitment to fixed ideas, stereotyped procedures, or immutable concepts, but by the manner in which, and the occasions on which, he changes those ideas, procedures, and concepts.</p>
<div>Stephen Toulmin,<cite> Human Understanding</cite></div></blockquote></div>

To expand our collection of reasonableness conceptions even further, the pragma-dialectical framework developed by van Eemeren and Grootendorst grounds reasonableness in dialectics. In this context, an instance of argumentation is valid if and only if there exists no strategy to be employed by an opponent in a structured dialogue which manages to undermine it. The proponents of this framework are particularly interested in enabling effective reasoning in a wide range of situations, rather than only in some higher realm of abstractions. That is why the conception of reasonableness on which their framework rests has a major pragmatic component. The regimented dialogue can be carried out by real individuals and can target a wide range of matters, from the most mundane to the most consequential. The ruleset of available tactics is simply made available to the individuals engaged in dialogue at the beginning, while strategies can be as diverse as forcing the opponent into self-contradiction or exploiting their (involuntary) support. That said, the framework can also be brought closer to _formal dialectics_ in order to account for idealized reasoning by employing perfectly rational agents as discussants, in a move similar to that of Perelman and Olbrechts-Tyteca.

<div class='epigraph'><blockquote><p>Accordingly, the prime aims of the present discussion are to exhibit the sociocommunal roots of the foundations of rationality, to provide an instrument for the critique of scepticism implicit in the cognitive solipsism of the Cartesian approach, and to illuminate the communal and controversy-oriented aspects of argumentation and inquiry—scientiﬁc inquiry in particular.</p>
<div>Nicholas Rescher,<cite> Dialectics</cite></div></blockquote></div>

Indeed, for many centuries at a time, logic has been but part of dialectics, rather than a field of its own.{% include sidenote.html id="mn-malogic" note='For instance, during the Middle Ages. Refer to _Section 2.10.1_ of the [Handbook of Argumentation Theory](https://link.springer.com/referencework/10.1007/978-90-481-9473-5) for a more detailed account.'%} The hot and cold relationship of the two disciplines over the centuries has perhaps been the closest thing argumentation theory has ever had to juicy gossip, with rhetoric a controversial element to complete the triad. It is difficult to overstate the reliance of contemporary mathematics, both pure and applied, on the foundation of formal logic, and so the very idea of scholars erecting an edifice of theory on a different foundation tends to induce vertigo. The very possibility that notions as elementary as conjunction, disjunction, and negation could be defined on the basis of a regimented dialogue instead of a logic{% include sidenote.html id="mn-manylogics" note='The term _logic_ is used here as countable in reference to the broad range of [three-valued](https://en.wikipedia.org/wiki/Three-valued_logic), [four-valued](https://en.wikipedia.org/wiki/Four-valued_logic), [many-valued](https://en.wikipedia.org/wiki/Many-valued_logic), and [modal](https://en.wikipedia.org/wiki/Modal_logic) logics which compete with [classical two-valued logic](https://en.wikipedia.org/wiki/Classical_logic).'%} sounds exceedingly exotic to the contemporary ear, outside a handful of niches like ludics, game semantics, and interactive computation.

To bring this section to an end, we have completed a very brief tour of several conceptions of reasonableness with the purpose of highlighting the breadth of approaches devised by scholars through the ages. Determining which conception of reasonableness is _itself_ more reasonable is the subject of vigorous debate in present-day argumentation theory. Each conception appears better suited to deal with certain aspects of argumentation, while lacking in other respects. In the rest of this chapter, we will build on top of many of the conceptions listed above, in an attempt to develop an automated pipeline for estimating reasonableness as a single floating-point number.

### Beliefs as Means or Ends

All of the three disciplines which fall under the umbrella of argumentation theory (i.e. logic, dialectic, and rhetoric) can be said to house both work which frames reasoning as a means of reaching a conclusion _based on_ beliefs, but also work which frames reasoning as a continuous process of _forming_ beliefs. The former can be seen as a building block of the latter, yet the latter can also be seen as a prerequisite of the former. However, those connections become somewhat counterproductive when considering the fact that the very same procedures in their entirety (i.e. sketching out a proof, engaging in a dialogue) can be motivated by both perspectives in different contexts.

In logic, for instance, writing a proof involves mainly chasing after a conclusion. How exactly one actually navigates the "game tree" of available moves in search of the finish line is up to the logician in question. However, it is only of secondary relevance that each step of the proof involves obtaining an entirely new well-formed formula.{% include sidenote.html id="mn-wffdef" note='This [technical term](https://en.wikipedia.org/wiki/Well-formed_formula) describes a collection of propositional atoms (e.g. $$P$$) brought together by connectives (e.g. $$¬$$, also known as negation) in a "legal" way (e.g. $$¬P$$, read "not P", instead of $$P¬$$). It is a syntactic constraint on how such elements are arranged, preventing the equivalent of a logician being able to express _"flies bird."_ It is more relevant for being able to construct interesting proofs, rather than making sure logicians write correctly.'%} They are merely intermediate steps required to succeed in proving a certain conclusion. In other words, _means_.

<div class="frame">
  <p><b><i>Fig. Logic proof.</i></b></p>
  <p>The <a href="https://en.wikipedia.org/wiki/Fitch_notation">Fitch-style logic proof</a> below involves three premises. Each step of the proof yields an intermediate well-formed formula using an approved operation. For instance, double negation is used to cancel out the two chained negations from step 4, thus arriving at step 5.</p>
  <br/>
  <img src="{{ site.baseurl }}/assets/img/logicproof.svg" width="80%"/>
  <br/><br/>
</div>

In contrast, several direct applications of this formalism involve a much greater focus on the "beliefs" formed as the reasoning process unfolds. For instance, expert systems had been a major topic in early symbolic AI, involving the constant expansion of a [knowledge base](https://en.wikipedia.org/wiki/Knowledge_base) from a set of initial statements, using rules intimately tied to the ones above. An entire [inference engine](https://en.wikipedia.org/wiki/Inference_engine) was dedicated to the task of deriving a tiny bit of new knowledge from the knowledge which had been accumulated up to that point. Systems relying on [forward chaining](https://en.wikipedia.org/wiki/Forward_chaining) in particular even tried to "grow" the knowledge base as much as the inference engine allowed before any other subsequent operation. The procedure is essentially identical to the previous one—there is only a shift in focus baked into the very ontology being used.{% include sidenote.html id="mn-ontologyconceptual" note='_Ontology_ refers here to the conceptual framework which a specific intellectual tradition uses to deconstruct their object of study. Not to be confused with the typically hierarchical (digital) [knowledge bases](https://en.wikipedia.org/wiki/Ontology_(computer_science)) which were popular in early symbolic AI.'%}

<div class='epigraph'><blockquote><p>Reasoning is a transition in thought, where some beliefs provide the ground or reason for coming to another.</p>
<div>Jonathan Adler and Lance Rips,<cite> Reasoning</cite></div></blockquote></div>

The dichotomy of beliefs as means or ends is also echoed in the field of rhetoric, with self-persuasion being seen as a continuous process capable of incrementally "emitting" beliefs. Additionally, the closely-related enterprise of persuasion research investigates ways of successfully persuading individuals of certain specific statements, but also the multi-stage process of getting there. The interactions between the advocated belief and the individual's previous epistemic{% include sidenote.html id="mn-epistemicology" note='Epistemics refers here to that which is known. [Epistemology](https://en.wikipedia.org/wiki/Epistemology) refers to the study of knowing and knowledge in general.'%} baggage can become quite complex, making the act of persuasion better described as a multi-step intervention on an individual's belief system, prompting them to gradually "move" towards the advocated position by forming or discarding intermediate beliefs.

Perhaps in none of the three disciplines concerned with reasoning is the means-ends dichotomy more clear than in dialectics. On one hand, there are dialectical formalisms which focus entirely on a single statement supported by one proponent and contested by one opponent. In these cases, the entire aim of a regimented dialogue is to lead the discussants towards a conclusion regarding whether or not the statement in question is true. Similar to the logic proofs above, there are intermediate utterances made by the two parties as the dialogue unfolds—artifacts which are crucial for making the dialogue function in the first place—yet which are seen as mere scaffolding around the deliberation of the main statement.

However, things get much more colorful when looking at dialectical formalisms designed to be open-ended and perpetual. The "games" proposed by Jaakko Hintikka are perhaps the most salient example. For instance, Hintikka describes dialogues whose participants are motivated by an interest in being _surprised_ and learning more, broadly referred to as "information-seeking dialogues."

<div class='epigraph'><blockquote><p>An answer to our problem can be given by making the payoff of the game for a given player dependent on the information-content of his (her) ﬁnal thesis (more properly speaking, the conjunction of all his theses). The more informative this thesis, the higher the payoff.</p>
<div>Jaakko Hintikka & Esa Saarinen,<cite> Information-seeking dialogues</cite></div></blockquote></div>


One's partner in such information-seeking dialogues need not necessarily be human, as illustrated below.

<div class='epigraph'><blockquote><p>We may think of "a" as a scientist or inquirer of some other kind and "b" as Nature or as a comparable impersonal source of information. [...] We may further think of "B" as a constant basic theory of "b" while the different choices of the "A" represent different hypotheses "a" is trying to prove by "putting questions to Nature."</p>
<div>Jaakko Hintikka,<cite> On the logic of an interrogative model of scientific inquiry</cite></div></blockquote></div>

Echoing Hintikka's almost literary move from _Man versus Man_ to _Man versus Nature_ in his dialectical dealings, while at the same time departing somewhat from Hintikka's reliance on information theory, the co-founders of the Erlangen School write as follows.{% include sidenote.html id="mn-erlangen" note='The "constructivist" school of thought, rather than the theological one which shares the same name, although as one can see, the line gets blurry at times. Also, one of them was [the founder of modern-day game semantics](https://en.wikipedia.org/wiki/Paul_Lorenzen), as well.'%}

<div class='epigraph'><blockquote><p>If one compares this agonistic origin of logic with modern conceptions, according to which logic is the system of rules that, whenever they are applied to some arbitrary true sentences, will lead one to further truths, then it will be but too obvious that the Greek agon has come to be a dull game of solitaire. In the original two-person game only God, secularized: “Nature,” who is in possession of all true sentences, would still qualify as an opponent. Facing Him there is the human individual – or perhaps the individual as a representative of humanity – devoted to the game of patience: Starting from sentences that were, so he believes, obtained from God before, or snatched away from Him, and following rules of logic, he is to gain more and more sentences.</p>
<div>Paul Lorenzen & Kuno Lorenz,<cite> Dialogische logik</cite></div></blockquote></div>

Nicholas Rescher takes this style of thinking even further by moving from one scientist engaged in truth-seeking to the whole scientific enterprise as a generalized "sociocommunal" process of deliberation about the nature of the world.

<div class='epigraph'><blockquote><p>At this stage, however, the social or communal aspect of the scientific enterprise comes crucially into play. For once a scientifically significant thesis is propounded by someone, the "scientific community" provides (1) certain opponents, in the form of self-appointed critics who challenge this thesis in an adversary manner, probing for its weak points and seeking to impede its acceptance, and (2) a larger, neutral body of concerned but otherwise uncommitted bystanders, who effectively act as arbiters of the "dispute."</p>
<div>Nicholas Rescher,<cite> Dialectics</cite></div></blockquote></div>

The idea of _a competition of ideas unfolding in the arena of society_ allows us to complete our recent sequence of conceptual hops with an altogether different proto-discipline. The controversial field of _memetics_ casts the beliefs which populate the collective consciousness in a Darwinian light. Belief systems are said to ruthlessly compete with one another for the scarce resource of human psyche. Instead of developing an immune system to fight off parasites, a belief system might "adapt" to prescribe the prohibition of "hosts" to adopt other beliefs. Particularly ambitious proponents of this perspective claim that culture in its _totality_ can be explained in evolutionary terms, just as life has been explained to an impressive extent by evolutionary biology.{% include sidenote.html id="mn-memeticscontroversial" note='Much of what is controversial about memetics is due to such observations being made not-so-tactfully in the context of religions as belief systems. For instance, one could say that preventing interfaith marriage or prescribing the same religion for children are adaptations of an ideology meant to protect or conquer psychological territory. One might imagine a more permissive belief system not standing the test of time.<br/><br>Accordingly, the framing of memetics is significantly more influential in more secular circles (although there is vigorous criticism there, too). One might go so as far as to say that those communities are more "vulnerable" to being "infected" with the meme of memetics.<br><br>It has also been argued that memes require a "fertile psychological soil" in which to emerge, which is largely a function of socioeconomic dynamics. For instance, Stoicism and Epicureanism might have required the idiosyncracies of the [Hellenistic age](https://en.wikipedia.org/wiki/Hellenistic_philosophy#Background). Additionally, just like two species define each other\'s niche, there might also be interactions between belief systems, as Erich Fromm eloquently articulates in _Escape from Freedom_:<br><br><i>"This readiness for submission of one\'s self to extrahuman ends was actually prepared by Protestantism, although nothing was further from Luther\'s or Calvin\'s mind than the approval of such supremacy of economic activities. But in their theological teaching they had laid the ground for this development by breaking man\'s spiritual backbone, his feeling of dignity and pride, by teaching him that activity had no further aims outside of himself."</i>'%}

While memetics and dialectics are worlds apart in terms of the employed formalisms and motivations, with dialectics relying on a carefully regimented procedure for effective reasoning while memetics relying on a supremely lax definition of spontaneous adaptations for understanding culture, the bridge between the two will prove key in later chapters. It will allow us to combine the rigidity of reasoning through regimented procedures with the evolutionary fluidity of models forged out of the selective pressures of empirical risk minimization.{% include sidenote.html id="mn-riskstatistical" note='[Technical term](https://en.wikipedia.org/wiki/Empirical_risk_minimization) employed in statistical learning theory to denote "training a model to perform well on the training data," but without all the ontological baggage associated with the anthropocentric metaphor of the model learning how to perform well on tasks as a person might.'%} This will become abundantly clear in the second half of _Chapter II_, when we employ the connection as a conceptual building block, but will also resurface towards the end of _Chapter IV_.

To bring this section to an end, we have explored the pervasive dichotomy between beliefs as means and beliefs as ends, which appears to cut through virtually all disciplines concerned with the study of reasoning, and beyond. Going forward, we will include the flexibility necessary to accommodate both of those perspectives as a constraint for our algorithm.

### Carving the Algorithm

We have explored various ways in which scholars have conceived of the reasonableness of arguments. This will now serve us well, expanding the space of candidate algorithms which are backed by such a rationale—our raw material. We will make our way through this expansion by means of constraints, using them to cut down the search space. As we establish what our algorithm is _not_, the algorithm will slowly become crisper and better defined, each cut collapsing possibilities along some axis.

First, we would like the automated pipeline to be able to accommodate the richness of natural language. We would like to avoid the invariably lossy compression{% include sidenote.html id="mn-lossycompression" note='In contrast to _lossless_ compression, which can be reverted so as to perfectly reconstruct the original artifact, _lossy_ compression involves some amount of information loss, meaning that perfect reconstruction becomes impossible, although getting e.g. 90% of the way is sufficient in many applications. For instance, [JPEG](https://en.wikipedia.org/wiki/JPEG) involves lossy image compression (with a configurable amount of loss, even), while [PNG](https://en.wikipedia.org/wiki/Portable_Network_Graphics) involves lossless image compression.'%} involved in converting beliefs into a brittle mosaic of propositional atoms, predicates, and connectives. Any such analytical statement can trivially be expressed in natural language (i.e. by describing it, albeit verbosely), while the reverse task has prompted an army of logics, each tailored to one very specific facet of reality (e.g. [temporal logic](https://en.wikipedia.org/wiki/Temporal_logic) for time), while still remaining "an open challenge." For sure, natural language itself is neither the perfect mirror, nor the very blueprint of the world, as many classics sincerely seemed to have hoped. Still, it is one less step of information loss from reality, and it is reality we are ultimately interested in reasoning about. Furthermore, while analytical statements might succeed in capturing essential features in highly structured domains, the notions we are most interested in when wielding unprecedented amounts of computation (e.g. human values, long-term flourishing) seem to resist being abstracted into a handful of sufficient statistics.{% include sidenote.html id="mn-sufficientstats" note='[Sufficient statistics](https://en.wikipedia.org/wiki/Sufficient_statistic) refer to the minimum number of measures which are enough to explain most of a statistical object. For instance, a "bell curve" distribution can be described in its entirety using two values: one measure of centrality and one measure of spread. For a more enthusiastic take on whether notions as messy and abstract as e.g. human values can be explained in full using a handful of appropriate factors, refer to [John Wentworth\'s agenda](https://www.alignmentforum.org/posts/Fut8dtFsBYRz8atFF/the-natural-abstraction-hypothesis-implications-and-evidence) for devising an appropriate objective for a powerful AI to chase safely.'%} {% include sidenote.html id="mn-demisdeepmind" note='DeepMind leader Demis Hassabis, [quotes](https://www.youtube.com/watch?v=I5FrFq3W25U&t=2550s) the related neuro-symbolic integration challenge as the one they "spend most of [their] time thinking about," and notes that "we\'re still quite far from [solving it], and no one quite knows how to bridge that [neuro-symbolic] chasm. [...] It\'s a bit of a mystery."'%} Natural language, for better or worse, has evolved to serve us in communicating effectively about such topics, mediating much, if not most, of our culture.

<div class='epigraph'><blockquote><p>The medium is the message.</p>
<div>Marshall McLuhan,<cite> Understanding Media</cite></div></blockquote></div>

Let us take stock of the search space following the application of this first constraint. Unfortunately, we are largely forced to abandon the geometrical conception as a motivating rationale to base our algorithm on, not because of its elegance or crispness, which we are disheartened to leave behind, but because its limited set of legal inferences are better suited for highly-structured domains rather than the messy world as a whole. The critical conceptions of reasonableness make up _some_ of the remaining options, defining reasonable instances of argumentation as those which systematically resist being undermined by opponents. However, having largely left behind the foundationalist{% include sidenote.html id="mn-foundationalismdef" note='[Epistemological term](https://iep.utm.edu/foundationalism-in-epistemology/) referring to the idea of knowledge building on top of a foundation of other knowledge, gradually ascending as one gets to "stand on the shoulders of giants." This stance is implicitly baked into the structure of a logic proof (with premises being neatly separated, indicating some amount of epistemic privilege), but need not refer only to analytical expressions.'%} luxury of building on top of axiomatic premises, we risk the following failure mode. An opponent can simply contradict what the proponent says and win! It is quickly game over due to the opponent having the freedom not to build on the same foundation, rendering naive contrarianism a winning strategy. Our second constraint on the search space is then the necessity of accounting precisely for this thorny problem. Who has the epistemic high-ground when there is no absolute reference frame involved, when each party advocates their own? What can we substitute foundationalism with in order to gracefully handle such situations?

Following this second cut through the search space, we are fortunately left with more than nothing. Similar to how the geometrical conception of reasonableness is typically used in tandem with the epistemological notion of foundationalism, most of the critical conceptions of reasonableness actually incorporate the epistemological notion of [_coherentism_](https://iep.utm.edu/coherentism-in-epistemology/). According to this view, it is those parties whose stances are coherent (e.g. which do not contradict themselves) which should be favored. Not only should the opponent undermine the proponent, but they should make a good case for their opposition, being able to stand resolutely against the inevitable counterattacks. In his [_Introduction to Multiagent Systems_](https://www.wiley.com/en-us/An+Introduction+to+MultiAgent+Systems%2C+2nd+Edition-p-9780470519462), Michael Wooldridge uses the phrasing _mutually defensive_ to describe a constellation of statements which effectively support each other in fending off attacks.{% include sidenote.html id="mn-wooldridgepioneer" note='A pioneer of multi-agent systems, Wooldridge has [collaborated with DeepMind](https://www.deepmind.com/publications/equilibrium-refinements-for-multi-agent-influence-diagrams-theory-and-practice) on topics not too far removed from the present work. When the individuals which make up a multi-agent system interact, the emergent phenomenon is intimately tied with dialectical formalisms we have previously touched on. We discuss this connection more in _Chapter II_, when we further build on important work done at [DeepMind](https://www.deepmind.com/).'%} Laurence BonJour, a prominent epistemologist and proponent of coherentism, further expands this position to account for other ways of knowing. Belief systems which not only are internally coherent, but which are also coherent _with perceived observations of the world_ are even more promising, since they move away from a potentially unhinged solipsism{% include sidenote.html id="mn-solipsismdef" note='Roughly, the philosophical position associated with living in one\'s head. Debate around [solipsism](https://en.wikipedia.org/wiki/Solipsism) in popular culture tends to focus on the implied loss of touch with reality, including the ignorance of one\'s alleged responsibility to contribute to the world.'%} while still steering clear of foundationalism. One could imagine further expanding this coherence heuristic{% include sidenote.html id="mn-heuristicdef" note='This somewhat technical term describes a "rule of thumb." Here, we are arguing about using a party\'s coherence as an indicator to provide the grounds for breaking the tie.'%} to the self-perception act seemingly involved in memory, another major way of knowing explored by epistemologists.

As the features of our algorithm become more prominent, we move from crudely eliminating large chunks of possibility space towards subtler polishing touches. What actually makes a position internally coherent? Conversely, what makes the opponent's position _not_ be coherent with the proponent's, as a prerequisite in undermining it? For a start, we might argue that statements which contradict each other are not coherent. In contrast, statements which generally support each other might be better described as such. This is not all, as a group of statements might also be argued to be coherent by virtue of coming together in the act of contradicting an external statement which threatens to undermine them all—the enemy of my enemy is my friend. It seems that those simpler notions of support and contradiction between statements, although composable in increasingly complex arrangements, can provide a basis for our notion of coherence. However, given the high bar we previously set for ourselves—to deal with the messiness of natural language, and through it, with that of the world at large—how could we estimate whether an arbitrary statement entails or contradicts another? Things get further complicated by the domain-specific knowledge required to evaluate many of these connections, as highlighted by Toulmin. The third constraint of our algorithm is therefore the ability to discern such relations between fragments of natural language as a basis for gauging coherence.

Fortunately, there already are systems out there which can help us determine how fragments of natural language relate to each other. Language models tasked with [natural language inference](https://paperswithcode.com/task/natural-language-inference)—the natural language processing task which involves determining whether a statement supports another, contradicts it, or none of the above—have achieved impressive performance.{% include sidenote.html id="mn-nlibench" note='The state-of-the-art on the Stanford Natural Language Inference (SNLI) benchmark was 93% in mid-2021, reports [Papers With Code](https://paperswithcode.com/sota/natural-language-inference-on-snli).'%} Those models have been optimized to match human labelers in classifying hundreds of thousands of hand-crafted statement pairs, determining whether there is an (asymmetrical) entailment, contradiction, or neutral relation between them. The best models at the task tend to incorporate large amounts of unstructured knowledge gained through a previous "pretraining" stage, and are then "fine-tuned" to approximate human judgement in this more structured statement-statement-label setting.{% include sidenote.html id="mn-pretrainparadigm" note='We will cover the pretrain/fine-tune paradigm in more detail later on, as we train _DebateGPT_ using a related approach in _Chapter II_.'%} One might argue that the optimization process incentivizes the NLI models to _soak up domain-specific knowledge about which inferences are warranted_ as an instrumental goal in solving the task. Upon achieving high performance, the NLI models will have had, by necessity, internalized both (1) knowledge about the world, and (2) knowledge about whether that knowledge backs certain inferences and warrants certain conclusions. While NLI models will conveniently satisfy our present Toulminian needs, they will prove limiting later on, as we set our sights on _superhuman_ reasoning. At the very end of _Chapter II_, we will be forced to move beyond the "intelligence by proxy" trick involved in "merely" imitating human judgement, and explore more principled means of gauging coherence in order to get a grip—however loose—on the epistemic terra incognita.

However, the coherence of parties is more than the sum of individual relations between statements. What if each party contributed a dozen statements, some of which support each other and some of which are actively at odds with each other? Even worse, what if pairs which connect two different parties also vary wildly in their valence? What of the second-order effects briefly hinted at previously, with statements attacking a common enemy? What of higher-order effects? Who is to win when everybody is supporting each other to some extent, while also attacking everybody to a certain degree, while also reporting important amounts of in-fighting? We desperately need a way of making sense of this chaos.

Fortunately, network theorists have long grappled with problems involving countless elements being interweaved into the most complex of fabrics. Be it a billion people in a society, a billion pages on the internet, or a billion machines networked together digitally, network theory has helped us gain insight into the underlying structure of those systems. For instance, it can help determine whether people strongly rely on certain factors when associating with others (e.g. assortative mixing by race), identify the most influential pages based on the support they garner from other influential pages (e.g. node centrality at early Google), or identify similar users based on whether they relate to other entities in a similar way (e.g. structural equivalence at early Facebook). Those applications are intimately tied to our present concerns, and it should come as no surprise that network theory has also been used in argumentation theory. As a prominent example, consider Phan Minh Dung's abstract argumentation systems, the ones Wooldridge was referring to when using the phrase _mutually defensive_. If one represents statements as nodes and the relations between them as directed edges, it then becomes possible to systematically identify relevant structures inside the argument graph. For instance, a set of arguments is said to be _admissible_ if and only if (1) it is conflict-free (i.e. there are no internal arguments which attack each other), and (2) the arguments are acceptable (i.e. for every argument which attacks it, there is one in the set which attacks it back).

<div class="frame glow-medium" id="dunggraphframe">
  <p><b><i>Fig. Dung's abstract argumentation systems.</i></b></p>
  <p>The system below is composed of seven statements. Five of them are part of a preferred, stable, and grounded extension, all technical terms denoting various properties of interest in the context of the argument graph.</p>
  <br/>
  <center>
  <div>
    <span id="dunggraphstatus">Hover over a node to see explanations. Drag around to rearrange.</span>
    <br/><br/>
  </div>
  <div id="dunggraph" class="dunggraph"></div>
  <script src="{{ site.baseurl }}/assets/js/dunggraph.js"></script>
  <div class="advice">For the best experience, view full screen on desktop.</div>
  </center>
</div>

While Dung's formalism is at once extremely elegant and relevant to the issue of making sense of an interconnected fabric of arguments, it is not enough. The formalism has two important shortcomings. First, there is limited nuance in how one statement relates to a second (i.e it either attacks it or it does not). Second, the arguments themselves are similarly limited in terms of the "privilege" of being part of the defined groupings (i.e. either an argument is part of, say, the preferred extension or it is not). This general lack of nuance is detrimental in two ways. For one, it has trouble in handling the messiness of the world, with no possibility of a statement only lending _some_ degree of support to another one. In addition, it lacks _reward shaping_—the recognition of gradual, subtle, incremental shifts in reasonableness, a property essential for using it as part of a learning signal in _Chapter II_.

Fortunately, we can overcome both of these shortcomings relatively easily. Instead of using the NLI models as binary classifiers (i.e. "contradiction" versus "no contradiction", as Dung's formalism might suggest), and instead also of using them as ternary classifiers (i.e. for contradition, entailment, and neutral relations, as they are used natively), we take a step back from the discretized outputs and make use of the raw logits of the model.{% include sidenote.html id="mn-differentiability" note='Models need to be end-to-end differentiable in order to be optimized using gradient descent, so that they can take small steps towards being better at the task. This often means working with continuous functions, which is also the case for NLI models. Behind the label they output denoting the relation between the input pair of statements, there are three continuous numbers, one for each class. Turning them into a discrete label is trivial (i.e. just pick the one predicted to be most likely), but trying to hit a continuous target enables nuanced feedback, which in turn enables learning. This is also what we are trying to provide with the automated pipeline _for_ later models, but we are currently trying to extract this continuity from those previous models which _compose_ the pipeline, by bypassing the discrete classes and working with the "raw" class logits.'%} This allows us to _weigh_ the arcs which lead from one argument to another, using one number per arc, ranging from $$0.0$$ for a full-on attack to $$1.0$$ for full-on support, with $$0.5$$ denoting a generally neutral relation.{% include sidenote.html id="mn-calibrationuq" note='There is a subtle issue when working with continuous outputs which have been optimized in the context of discretized tasks. If the NLI model would output $$0.2$$ for "entailment," that does not mean that it estimates a $$20\%$$ chance that the input sentence pair captures an entailment. Those output values, also called pseudo-probabilities, are generally _not_ calibrated. Rather, they tend to be overconfident, due to the model typically being optimized against a metric which does not favor calibrated outputs. There are many ways of calibrating output probabilities explored in the field of uncertainty quantification (e.g. condition outputs on empirical proportions: $$100$$ outputs of around $$0.2$$ for "entailment" should actually correspond to "entailment" being correct about $$20\%$$ of the time). However, they are rarely used in practice. This does not mean that a higher estimate for "entailment" does not correspond to the model gauging it as more likely, it is just that the value should not be interpreted as directly proportional to estimated probability, but merely as a rough signal.'%} Following this switch from directed edges to _weighted_ directed edges, we now attempt to replace Dung's black-or-white cliques with a fuzzier alternative to enable subtler evaluation of arguments, and by extension, of parties.

It turns out that simply applying Google's classic _PageRank_{% include sidenote.html id="mn-pagerankdef" note='Perhaps the most iconic algorithm for node centrality, the task of estimating the "authority" of each node in a graph. Originally developed for ranking web pages on early Google Search, [_PageRank_](https://en.wikipedia.org/wiki/PageRank) works by recursively nudging a page\'s rank based on the ranks of the pages which reference it. If many authoritative pages link to our page, then their "authority" will also "leak" into ours. But how can one know how authoritative those other pages were in the first place? Similarly, they might be referenced by other authoritative sources. This massive chicken-egg problem is solved by starting out with a baseline degree authority for each page, and conducting this "osmosis" until the node values convergence.'%} to the argument graph yields an evaluation which matches many of our previous intuitions. If one argument is overwhelmingly supported by many other arguments, then it receives a good rating. However, if those other arguments are systematically attacked, it only gets a mediocre rating. Similarly, if one argument is overwhelmingly attacked by many other arguments, then it receives a low rating; but if those other arguments are systematically attacked, its rating is not hurt much. A group of arguments which support each other and systematically target external attackers find themselves in good standing. Ditto for strategically positioning oneself in order to derive support from the opponent. In contrast, a group of arguments which exhibit a lot of in-fighting relative to the support lent to third-parties will not find themselves in such a good standing. Ditto for stepping right into the opponent\'s line of fire. If we simply average the ratings held by all the utterances of a party, we finally obtain an estimate of the party's aggregate authority, similar to the authoritative sources promoted on search engines.

Notice also how capitalizing on the graph representation of the arguments contributed by parties fits with our shift away from foundationalism. In contrast to the quite linear structure of logic proofs, where each well-formed formula is built on the foundation of what came before it, starting off with the premises, the graph of arguments is inherently non-linear. There are no privileged or foundational nodes—there are just nodes. The notions of "above" and "below" are not well-defined across the flattened constellation of utterances. This has another added benefit. If at a later time we eliminate one particularly "dated" statement from a constellation, it will _not_ instantly bring down the entire structure built around it. The non-linear structure allows for more than an epistemic [Jenga](https://en.wikipedia.org/wiki/Jenga), constantly on the brink of collapse. It can house self-sufficient and resilient belief systems, recursively supplying their own reason(s) for being. This "decentralized" flexibility allows us to deal with our final constraint—the accommodation of both beliefs as means and as ends, as mentioned in the previous section. While our algorithm is already well-equipped to deal with a brief encounter of parties (i.e. by providing the means of spotting the epistemic high-ground after a finite number of rounds), it can also allow for utterances to constantly pop in and out of a sliding window across time, enabling a constant scaffolding for the parties' transitions in belief. Besides, the homogeneity of the argument graph also levels the roles of the parties—one statement's proponent is another's opponent. There is no fundamental difference in motivation across parties, as each strives to attack the others while defending itself.

Let us retrace our steps. First, we wanted to be able to deal with the reasonableness of arguments expressed in natural language. This led us to consider critical conceptions of reasonableness as a grounding for our algorithm. However, this naive approach raised the issue of contrarianism becoming an optimal strategy. To counter this, we resorted to coherentism as a stand-in for foundationalism. However, gauging coherence prompted us to consider feasible means of determining the way in which two fragments of natural language relate to each other. This tentatively led us to NLI models. However, the coherence of parties turned out to be more complex than the sum of how pairs of their statements relate. This prompted us to consider a network-theoretic approach as means of making sense of the tumult. Representing the interaction between parties as a graph also yielded the added benefit of enabling perpetuity, by having utterances pop in and out over time. Barring several tweaks which we will consider later in an attempt to access superhuman reasoning, we have largely completed our search for an algorithm. In the next section, we will put all of it together into a more concise form, leaving behind the motivating details of our interaction with possibility space.

### ArgRank

In this section, we summarize _ArgRank_, an algorithm for estimating reasonableness. _ArgRank_ is based on a critical conception of reasonableness, one which favors those groupings of natural language arguments which systematically resist opponents that attempt to undermine them. Given this, we assume as a prerequisite the presence of several agents capable of deliberating in natural language about a range of topics (e.g. humans, human simulacra, etc.). Presently, however, we are not concerned with how we might engineer such agents—we turn to this task in _Chapter II_. Instead, we are currently interested in a way of determining which party is "winning" in the first place, and by what margin. _ArgRank_ attempts to provide a fuzzy estimate of each party's standing relative to the others, motivated by the epistemological and argumentation-theoretic considerations discussed earlier in the chapter.

_ArgRank_ first represents the utterances of the parties-to-be-rated as nodes in an argument graph. To be more precise, the argument graph is a weighted, directed, and fully-connected graph. Each arc represents the relation between two utterances, with the arc's weight denoting the strength of the out-bound statement's support (or lack thereof) lent to the in-bound statement. The actual weight values are computed using a language model pretrained to perform natural language inference (i.e. classify statement pairs as engaging in an entailment, contradiction, or neutral relation). We turn the three raw class logits returned by these models into one single arc weight by plugging the entailment and contradiction logits into a softmax,{% include sidenote.html id="mn-softmaxdef" note='[Continuous function](https://en.wikipedia.org/wiki/Softmax_function) which takes in a list of real values and maps them across the $$[0, 1]$$ interval, while at the same time accentuating their "contrasts" in proportion to a "temperature" parameter. A low temperature will elevate the highest input values to $$1.0$$, while not raising the others much above $$0.0$$. A high temperature will place the input values more "spaced out" across the output range. The term _temperature_ is not a fluke, with the function being motivated by statistical physicists, [Boltzmann especially](https://en.wikipedia.org/wiki/Boltzmann_distribution). The term _softmax_ highlights (1) the conceptual similarity to the simple `max` function, and (2) the fact that it is differentiable, in contrast to `max`.'%} and taking the first resulting value, similar to [another related application](https://discuss.huggingface.co/t/new-pipeline-for-zero-shot-text-classification/681). This has the effect of assigning values close to $$0.0$$ for a strong attack, and values close to $$1.0$$ for strong support being lent, with values close to $$0.5$$ denoting a more neutral relation.

<div class="frame glow-medium" id="nliframe">
  <p><b><i>Fig. Weighing arcs between arguments.</i></b></p>
  <p>For each ordered pair of statements which make up the constellation of arguments, <i>ArgRank</i> assigns one numerical weight in \([0, 1]\). The weight is proportional to the amount of support being lent from source to target, as estimated by an NLI model. Concretely, high values imply "implies," while low values imply "implies the contrary." The weights below are produced by <a href="https://huggingface.co/cross-encoder/nli-deberta-v3-large">an actual model</a>, rather than didactically.</p>
  <br/>
  <center>
  <div id="nli" style="display: flex; align-items: center; padding: 5%; width"><p>
    <select class="nli" id="nliprop1">
    <option value=0 selected>She is wearing a blue dress.</option>
    <option value=1>She is wearing a white dress.</option>
    <option value=2>She is wearing a white and blue dress.</option>
    <option value=3>It is very warm outside.</option>
    <option value=4>The weather outside is quite cloudy.</option>
    <option value=5>It is raining heavily.</option>
    </select></p>
     <table style="display: inline">
    <tr><td style="text-align:center; font-size: 2rem; font-weight: 1000" id="nli21">0.6</td></tr>
    <tr><td style="font-size: 3rem;">⤺</td></tr>
    <tr><td style="font-size: 3rem">⤻</td></tr>
    <tr><td style="text-align:center; font-size: 2rem; font-weight: 1000" id="nli12">0.6</td></tr>
    </table>
    <p style="display: inline;"><select class="nli" id="nliprop2">
    <option value=0>She is wearing a blue dress.</option>
    <option value=1>She is wearing a white dress.</option>
    <option value=2 selected>She is wearing a white and blue dress.</option>
    <option value=3>It is very warm outside.</option>
    <option value=4>The weather outside is quite cloudy.</option>
    <option value=5>It is raining heavily.</option>
    </select>
    </p>
  </div>
  <script src="{{ site.baseurl }}/assets/js/nli.js"></script>
  <div class="advice">For the best experience, view full screen on desktop.</div>
  </center>
</div>

Following the use of NLI models for weighing arcs, we then apply _PageRank_ on the argument graph. This subroutine incorporated into _ArgRank_ as-is assigns one numerical value to each utterance node. This can be interpreted as that statement's authority, with e.g. statements which are supported by other well-supported statements receiving a high rating. It is interesting to note that the sum of ratings is $$1.0$$, due to _PageRank_ "preserving" the total amount of authority which is being iteratively passed around. Following this, we average the ratings of all the utterances contributed by each party, thus obtaining one single aggregate measure of reasonableness per party. Normalizing by the number of utterances per party, we obtain party ratings which also neatly sum to $$1.0$$. Finally, for a long deliberation, we can only include the last $$n$$ utterances contributed by each party as a "moving average" of the on-going situation.

This is the meat of _ArgRank_—essentially _PageRank_ on the NLI-mediated argument graph, aggregated by party. The algorithm is quite straightforward in retrospect, yet required several conceptual leaps to reach, as the preceding sections attest. However, _ArgRank_ requires arguments to rate in the first place. Coming up with arguments effectively—using each utterance as a strategic move to further one's standing—is an altogether different matter. It involves identifying your opponent's epistemic weak points, crafting strong arguments to target them, and fending off the imminent counterattacks. In _Chapter II_, we turn towards creating an automated "strategist" to carry out such intricate maneuvers, a process also known as debate. As we shall see, pitting it against its _own_ past arguments, in an uneasy turn of events, will prove essential to the process.

Before moving on, however, we would like to leave the reader with a challenge. The aim of this exercise is to illustrate the significance of the progress we have made so far in an experiential way, by prompting personal attempts at undermining a _Cogito_-like postulate. More concretely, the reader is invited to _try making a coherent case against the claim that the true nature of truth-seeking lies in the existence of coherent challengers_. Later on, in _Chapter III_, we will develop a formal language to help us describe the strength of such postulates more broadly.

<div class='epigraph'><blockquote><p>At last one evening–but what a long time it had been–a little trembling light appeared in the lens of the telescope, a weak light which seemed to flicker on the point of death but which must be, if you worked out the distance, of a respectable size.</p>
<div>Dino Buzzati,<cite> The Tartar Steppe</cite></div></blockquote></div>

## Ch. II, Deliberative Arms Race

### Brief Review of Language Models

We have previously employed language models as mechanisms to weigh the arcs of the argument graph. Going forward, language models will become even more central to our inquiry. Indeed, the "strategist" which we will shortly go about creating can also be seen as one such mechanism. It is for this reason that we devote an entire section to recapitulating the essential features of language models, rather than only a brief side note.

Language models are one of the increasingly many computational artifacts which are optimized—rather than handcrafted—to exhibit certain desirable properties. More often than not, their most sought-after features involve exhibiting high performance in natural language processing tasks, similar to the already familiar task of natural language inference. For instance, _masked_ language models are optimized to "fill-in-the-blank" in the context of a passage containing several masked words. Similarly, _autoregressive_ language models are often optimized to predict the next word in a sequence, be it in a piece of writing, or perhaps in a piece of code. The vast majority of existing language models have been optimized to provide good solutions to precisely such problems, the flagship ones often requiring millions to be spent on computational resources.

Despite the misleading simplicity involved in defining such tasks (e.g. predict the next token in this text corpus), they turned out to require a whole lot from the language models being optimized. In order to e.g. figure out what a character might say next, what the most fitting word to describe a landscape is, or what is the outcome of an experiment in physics, language models are forced to bring together a pile of other skills, each quite demanding in its own right. They might need to reason about a character\'s motivation, to possess knowledge of the Earth\'s geography and biology, or to have some internal model of the world\'s physics. In other words, a task as seemingly innocent as filling in the blanks in a piece of text can call on a host of disparate skills. In practice, this means that if a language model performs well on such tasks, then it has also acquired those prerequisite skills, by necessity. Barring the inevitable Searlean responses contesting this inference,{% include sidenote.html id="mn-searleref" note='John Searle is the author of the influential thought experiment called [The Chinese Room](https://en.wikipedia.org/wiki/Chinese_room). It describes a person sitting alone in a room, who receives messages written in Chinese on slips of paper, and is asked to reply appropriately by sending responses through an opening. However, the person does not know Chinese at all. Instead, the room contains heaps of manuals on how to hold a conversation in Chinese, which are full of strange rules and guidelines on how to put together a response, without ever translating the Chinese characters into the person\'s native language. By making use of those resources, the person appears fluent in Chinese to any external speaker. Now, does the person actually understand Chinese, or are they "merely following the rules" documented in the manuals? Is there even a meaningful distinction between the two? Now, replace the Chinese room with a language model appearing to discourse fluently. Does it truly understand the words it is producing? For a response in the negative, follow [Gary Marcus](https://substack.com/profile/14807526-gary-marcus) as Searle\'s most salient contemporary torchbearer, as well as [others](https://aclanthology.org/2020.acl-main.463.pdf).'%} those instrumental skills form the basis of the language model's ability to "solve" the original task.

How exactly language models go about solving the problems they have been optimized for is only of secondary relevance, especially for the more pragmatic practitioners who are primarily interested in using them in commercial applications. However, the fact that language models have single-handedly pushed performance forward across so many natural language tasks, with scarcely any challenger coming close, has prompted many to investigate their inner workings—the specific means by which they solve the problems we task them with. For instance, [transformer circuits](https://transformer-circuits.pub/2021/framework/index.html) are just one of the many approaches being actively considered in the burgeoning field of interpretability, where researchers are trying to reverse-engineer the powerful language models which we have already created, but about which we currently lack a solid understanding. In a fit of welcome idealism, Chris Olah goes so far as to [suggest](https://80000hours.org/podcast/episodes/chris-olah-interpretability-research/#transcript) a paradigm in which we study the world by indirectly extracting knowledge about it from an optimized model, as if looking at the world through a microscope. While we will tangentially touch on interpretability in later sections, we will dedicate an entire separate volume to studying the syntax and semantics of the emergent "language of thought" which models employ internally to express knowledge about the world.

Before returning to the realm of dialectics, we need to cover a final essential development. The task of reconstructing a corrupted piece of text—also termed _self-supervised learning_, in contrast to the _supervised learning_ found in the more structured case of NLI (i.e. statement-statement-label triples)—is currently the most popular approach to endowing language models with skills, but it is slowly but surely losing ground to a different one. The shift is motivated by the fact that a corpus exhibiting the specific skills and knowledge which one might want to equip a language model with might simply not exist. For instance, nobody took years on end to churn out millions of transcripts documenting the process of assistants carefully following instructions or chatting with humans in a helpful way, for [_InstructGPT_](https://openai.com/blog/instruction-following/) or [_ChatGPT_](https://openai.com/blog/chatgpt/) to build on. While human contractors can be called on to create those manually, this can quickly become prohibitively expensive. Even worse, there is no easy target to imitate as one starts seeking any kind of superhuman performance.{% include sidenote.html id="mn-amplifyhuman" note='One option which is being considered by [OpenAI](https://openai.com/blog/our-approach-to-alignment-research/) and [others](https://arxiv.org/abs/2211.03540) for tapping into superhuman territory is "amplifying" the evaluator. If at the very beginning it is just humans evaluating a model, the model can later also play the role of an assistant for the human evaluator, therefore "amplifying" them. The amplified evaluator can then be used to yield more capable models, which again are co-opted as assistants, _ad infinitum_. We elaborate on this practice in _Chapter IV_.'%}

Instead of relying on corrupted human-written text to reconstruct, language models are increasingly tasked with obtaining good ratings based on their _own_ open-ended behavior—also termed _reinforcement learning_. It is much simpler to evaluate behavior than produce it, as P/NP problems{% include sidenote.html id="mn-pnpcomplexity" note='$$P$$ and $$NP$$ are two different classes of [computational complexity](https://en.wikipedia.org/wiki/P_versus_NP_problem), a field of computer science known for not boasting the most intuitive terminology. The prototypical example of a pair of related tasks which span this distinction involves a [traveling salesman](https://en.wikipedia.org/wiki/Travelling_salesman_problem). The task of measuring the total distance traversed by a salesman while trying to visit several cities is quite simple—just add up the individual journeys from city to city. In contrast, coming up with the shortest route which visits all cities is much more difficult—there is a combinatorial explosion of potential routes to consider. The first task is part of the $$P$$ complexity class, while the second is part of the $$NP$$ one.'%} and pathological contrarians alike demonstrate. This means that it is often easier to evaluate how a language model performed on a given task than to flesh out countless examples of how to perform well on said task. For instance, the development of _InstructGPT_ roughly involved humans ranking instances of instruction-following produced by the language model itself, rather than painstakingly fleshed out by humans. _Some_ amount of self-supervised learning was necessary at the very beginning to kickstart the whole process (i.e. the "pretraining" phase), yet the model reached entirely new heights only after switching gears to reinforcement learning (i.e. the "fine-tuning" phase). Currently, the biggest labs developing such models seem to abide by this two-step approach, with their associated safety departments focusing in large part on reducing societal risks emerging from this approach. However, the clear-cut distinction is predicted to become [increasingly blurrier](https://arxiv.org/abs/2302.08582), and might already be so in the case of state-of-the-art models whose optimization procedure is not publicly documented, allegedly.

However, stellar human ratings are not the only rewards which language models can be optimized to "pursue." For instance, the role of the evaluator can also be played by an altogether different model. Indeed, the typical toy example demonstrating the use of reinforcement learning for optimizing pretrained language models [involves](https://github.com/lvwerra/trl/tree/7ed040fb23b7f6c6db0077aea3579a0bbabf6947#how-it-works) a model capable of determining the sentiment of a piece of text. Originally motivated by the need to classify user messages as positive or negative for dealing with (potentially furious) customers, such models can be repurposed as "suppliers of reward" in order to fine-tune a language model to produce maximally positive writing. In order not to degenerate into a nonsensical stream of _awesome, fantastic, magnificent,_ the model being fine-tuned is typically kept "close" to the original version through a penalty proportional to the "distance" between the next words considered by the two (i.e. the KL-penalty). Complementing the base reward—regardless of whether it originates from a human, a different model, or something else entirely—with such a penalty has the effect of forcing the model to adapt itself to pursue reward, while at the same time preserving its original breadth of skills, a matter much more involved than for humans.{% include sidenote.html id="mn-nflforgetting" note='The model would otherwise not hesitate to shed all its excess representational resources devoted to a task which has since become superfluous, similar to how one\'s body has no incentive to keep much muscle on when switching from strength training to long-distance running, or how one\'s foreign languages grow rusty when not used much. This is often referred to as the [no free lunch theorem](https://en.wikipedia.org/wiki/No_free_lunch_theorem), and is generally thought to be the reason behind the phenomenon of [catastrophic forgetting](https://en.wikipedia.org/wiki/Catastrophic_interference) exhibited by models optimized sequentially on tasks which differ, however slightly.'%}

<div class="frame glow-medium" id="maximizingrewardframe">
  <p><b><i>Fig. The promise and peril of reward maximization.</i></b></p>
  <p>Optimizing a model so that it maximizes a reward—say, for manifesting a positive attitude—can initially be fruitful, with the model generally growing more cheerful in its behavior. However, the model can also "go too far"—more or less literally—and sacrifice its original tendencies so as to pursue cheerfulness <i>at all costs</i>.</p>
  <br/>
  <center>
  <div>
    <span id="maximizingrewardstatus">Hover across model space to surface behavior typical of each region.</span>
    <br/><br/>
  </div>
  <div id="maximizingreward" style="border: 1px solid black; display:inline-block; width: 100%;"></div>
  <script src="{{ site.baseurl }}/assets/js/maximizingreward.js"></script>
  <div class="advice" style="padding-bottom: 1.3rem !important">For the best experience, view full screen on desktop. Refresh if encountering a non-square layout.</div>
  </center>
</div>

There are two shifts in perspective which are sure to enrich any further discussion on the topic. First, notice how with the move from self-supervised learning to reinforcement learning, there is a partial conceptual shift from _tool_ to _agent_. Not only are language models good solutions to a host of natural language problems—not too different from a statistical model of the weather, to be used in forecasts—but they increasingly resemble open-ended agents engaged in the pursuit of reward, regardless of what the reward is defined as. The words predicted to follow next are reframed as possible _actions_ which the language model might take so as to obtain reward. The generated "language forecast" is reframed as the agent's policy for acting in the world of semiotic physics,{% include sidenote.html id="mn-semioticphysics" note='[Term](https://www.lesswrong.com/posts/TTn6vTcZ3szBctvgb/simulators-seminar-sequence-2-semiotic-physics) used to emphasize the similarity between the dynamics which allegedly govern the physical universe and the dynamics which govern the "world of written language." While the laws of physics are expected to be quite compact, the laws of language appear more challenging to express in closed-form, although generations of linguists have sought a [unifying structure](https://en.wikipedia.org/wiki/Generative_grammar).'%} its _behavior_. How _agentic_ the language model becomes as a result of being fine-tuned using reinforcement learning is [the topic of active debate](https://arxiv.org/abs/2302.00805) in AI safety. Will a model whose rewards are derived from human feedback go so far as to [take the human contractors hostage](https://www.alignmentforum.org/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to) by means of [social engineering](https://www.lesswrong.com/posts/qRtD4WqKRYEtT5pi3/the-next-decades-might-be-wild), "ensuring" an endless supply of reward? Perhaps more realistically, will it devise means of directly overwriting its own reward by exploiting a bug?

The second shift in perspective we ought to keep in mind is one we have already hinted at previously and elsewhere: the shift from _mechanism_ to _organism_. Increasingly, we are bringing computational artifacts into being not by handcrafting them, but by "subcontracting" the [impersonal engineer](https://ieeexplore.ieee.org/document/6788173/) known as selection. We construct computational niches for them to thrive in, such as those which require "feeding on" corrupted text and yielding the reconstruction. However, the environments we are crafting for them are growing more and more complex. For instance, the evaluator model quoted above has itself been forged in a supervised niche, yet it is then employed to specify the niche of another artifact entirely—that of the language model being fine-tuned, similar perhaps to how two different species "define" each another's niches. In fact, the upcoming process of pitting the "strategist" against itself is but a further act of niche construction. Competing against itself in the pursuit of reward, it will constantly redefine its niche. Each adaptation will beget another, being forced to forever outcompete itself.

### Obtaining _DebateGPT_

Over the course of the previous chapter, we have developed _ArgRank_, an algorithm for estimating reasonableness, and ended up incorporating critical, dialectical, and pragmatic components in its structure. _ArgRank_, however, relies on the presence of multiple parties which are to relentlessly challenge each other in natural language—parties which it then rates. In this section, we turn towards the challenge of developing a system capable of strategically "puppeteering" one or more of those parties. While this system could be seen as merely a means of filling in _ArgRank's_ missing prerequisites, it is more appropriate to rather see _ArgRank_ itself as the means of bringing the puppeteer into being. Similar to how the "training regimes" documented in the previous section have been successfully employed to endow language models with a broad range of relevant skills, we seek to use _ArgRank_ as a similar means of eliciting a host of specific faculties.

<div class="frame" style="padding-bottom: 0.5rem !important">
  <p><b><i>Note. Differential progress considerations.</i></b></p>
  <p>This section has been kept brief on account of differential progress considerations. More concretely, while we believe that the conceptual and theoretical parts of this work in particular might comprise helpful steps towards solving alignment (see <i>Chapter IV</i>), it is unclear to us how discussing specific design choices around system architecture, memory optimizations, or hyperparameter selection can help reduce existential risk.</p>
  <p>In some ways, this has been a difficult decision. It is not as if withholding such information happens to conveniently align with commercial interests of ours, as we are not planning on commercializing such models in any way whatsoever. On the contrary, the compute provider which we have been (informally) relying on so far tends to unequivocally <a href="https://stability.ai/about">advocate radical openness</a>, despite this potentially contributing to <a href="https://www.alignmentforum.org/posts/mF8dkhZF9hAuLHXaD/reshaping-the-ai-industry">premature escalation</a>. On a related note, there are more reliable ways of monetizing related technical skills out there than signaling value-alignment with philantropic funding bodies. Cynical preemption aside, it is natural to look forward to publicly sharing a piece of work which one has put significant amounts of effort and passion into, as well as reciprocate the value derived from building on open source tooling.</p>
  <p>In some other ways, however, this has been a simple decision. Few things can outweigh the prospect of increasing the chances of us <a href="https://www.cold-takes.com/most-important-century/">making it through this troublesome century</a> in one societal piece, not to mention the more speculative prospects of long-term flourishing.</p>
  <p>That said, we are fully open to sharing such technical details, the codebase, as well as the model, with researchers planning on pursuing further work which appears helpful in mitigating existential risk. If this is you, consider reaching out using your organization email.</p>
</div>

The "puppeteer" or "strategist" which we aim to obtain will take the shape of an autoregressive language model nicknamed _DebateGPT_. Similar to how _InstructGPT_ and _ChatGPT_ are fine-tuned "forks" of a pretrained model{% include sidenote.html id="mn-gptstandsfor" note='_GPT_ itself stands for _Generative Pretrained Transformer_. We already touched on the meaning of "pretrained." "Generative," on the other hand, refers to the open-ended behavior afforded by autoregressive language models (i.e. generate one word, then another, etc.). Finally, "transformer" refers to the architecture used to power the system. There are other possible architectures to be plugged in, even [purely statistical ones](https://en.wikipedia.org/wiki/Language_model#n-gram).'%} designed to be better at following instructions or acting as a virtual assistant, _DebateGPT_ is meant to be better at debate—the task of strategically producing utterances so as to further one's standing in a regimented dialogue.

Having selected a "seed" model to base _DebateGPT_ on, the next step is generating debate transcripts. Each generated debate requires a "spec," a brief set of high-level parameters which define its structure. The number of parties and the total number of rounds are two such parameters. Besides those two, we also randomize the number of _facts_—statements generated once, prior to the parties producing utterances. Those statements do not "belong" to any one party, yet they still play into the argument graph as additional party-neutral nodes. Given this, _DebateGPT_ is incentivized to take those static elements into account—to perhaps gain their support, or step out of their line of fire. Echoing coherentist epistemology, those party-neutral statements can be seen as _percepts_ for belief systems to _cohere with_. The otherwise hermetic process of _DebateGPT_ puppeteering parties in the confines of a GPU can thus be brought "closer" to reality by using party-neutral statements as windows into the world. Such statements provide the "empirical" weight so often necessary in tilting the otherwise solipsistic scales of competing beliefs.

<figure >
    <figcaption class="marginnote"><a href="https://en.wikipedia.org/wiki/Percept_(artificial_intelligence)">Percepts</a> are framed as windows into the world, allowing the parties engaged in debate to "remain in touch with reality." Note that perception might require a great deal <a href="https://en.wikipedia.org/wiki/Attention#Exogenous_and_endogenous_orienting">more agency</a> than just "letting in the world." In a book titled <i>Active Inference</i>, Karl Friston quips:<br/><br/><i>"In short, we are not simply trying to make sense of our sensations; we have to actively create our sensorium."</i><br/><br/>One (future) way of enabling parties to direct "the eyes of the debate" more actively might be to <a href="https://arxiv.org/pdf/2302.04761.pdf">hook them up</a> to (<a href="https://openai.com/research/gpt-4">potentially multimodal</a>) observational tools (i.e. enabling them to say e.g. "Ok Google, what is the luminosity reported by the Hubble Space Telescope at those coordinates?"). Provisionally, contemporary models might hallucinate stand-in returns for such dispatches, essentially turning the debate into a <a href="https://en.wikipedia.org/wiki/The_Truman_Show">Truman Show</a>. Consistent also with the "vested interests" of the debating parties, whose nature will become apparent in the next section, Friston et al. also claim:<br/><br/><i>"[...] any adaptive system engages in "self-evidencing." Self-evidencing here means acting to garner sensory data consistent with (i.e., that affords evidence to) an internal model [informed by the implicit anthropic acknowledgement of the system's existence in an evolutionary niche]."</i><br/><br/>This line of reasoning also explains why any one party engaged in debate should not, under any circumstances, be directly hooked up to <i>interventional</i>, rather than merely observational tools, although this seems useful in climbing <a href="https://en.wikipedia.org/wiki/Causal_model#Ladder_of_causation">Pearl's ladder of causation</a>. In its drive for partisan self-evidencing, it would <i>force the world into cohering with its specific position</i>, rather than the other way around, echoing the main failure mode of Stuart Russell's <a href="https://arxiv.org/abs/1606.03137">cooperative inverse reinforcement learning</a> agenda, as well as of <a href="https://www.alignmentforum.org/tag/oracle-ai">Oracle AIs</a>, both of which are described in more detail in <i>Chapter IV</i>.<br/><br/>Just as cosmologists are forced to make do in their truth-seeking enterprise without being able to meaningfully intervene on their object of study, we presently focus solely on observation as an empirical means of nudging <i>DebateGPT's</i> sense-making, although the <i>metaphysical</i> realm of knowledge which lies entirely beyond experience is also actively sought. That said, the following volume will hop over intervention on Pearl's ladder and focus instead on the systematic use of <i>counterfactuals</i>.</figcaption>
    <div id="perceptwindowframe"></div>
    <script src="{{ site.baseurl }}/assets/js/perceptwindow.js"></script>
</figure>

Besides the number of parties, rounds, and facts, the "spec" of a debate also includes a picture of each party's objectives. Typically, parties ought to be rewarded on account of their _own_ standing. However, we extend this by allowing parties to be incentivized to specifically contribute to another's standing, or, on the contrary, to explicitly seek to demote them. We allow for those game-theoretic possibilities by rendering each party's final rating $$r^{'}$$ linearly dependent on all previous party ratings. We induce a bias towards "tending to one's own needs" by sampling the weight of the same party's prior rating from a normal distribution with mean $$\mu_{same}=1.0$$, in contrast to the normal distribution with mean $$\mu_{other}=0.0$$ used to "introduce" the others' ratings into one's own. For example, let $$r_0=0.8$$ and $$r_1=0.2$$ be the initial ratings achieved by two parties, respectively—as the values which _ArgRank_ outputs sum to $$1.0$$. Let $$w_{00}=1.1$$ and $$w_{10}=-0.3$$ be the weights which are to mediate the first party's final rating, with $$r^{'}_0=w_{00} \cdot r_0 + w_{10} \cdot r_1$$. Therefore, $$r^{'}_0 = 1.1 \cdot 0.8 -0.3 \cdot 0.2=0.82$$ becomes the final rating of the first party. The weights which define each party's dependence on each other therefore make up the square matrix $$w$$ included in the debate spec to represent party objectives. Naturally, situations which are more intricate from a game-theoretic standpoint arise only when more than two parties are involved.

Every high-level parameter—the number of parties, rounds, and facts, as well as the objective matrix—which goes into a debate's spec are procedurally generated, so that _DebateGPT_ will get the opportunity to act and be evaluated in a broad range of randomized arrangements.{% include sidenote.html id="mn-generalrandom" note='Even limited opportunities to act in different environments are thought to help agents generalize to a whole space of possible environments. For example, researchers optimized an agent to collect a coin in a platformer game. If the coin was always located in the same position, the agent would instead learn to go to that position, rather than fetch the coin, as demonstrated by repositioning the coin. However, the authors [note](https://arxiv.org/pdf/2105.14111.pdf#page=4):<br/><br/><i>"Goal generalization is greatly improved in our Coin-Run experiments when just 2% of training levels have randomly placed coins."</i>'%} Additionally, this information is also rendered into a plain-text header which gets prepended to the discussion among parties, so that _DebateGPT_ can "learn" to take those parameters into account when producing utterances. For instance, we would expect it to eventually know when to help out an "ally," in expectation of deriving reward from the other's rating. Conversely, we would expect it to grow more aggressive towards a competitor whose success is strongly at odds with its own.

Following the procedural generation of debate specs, we proceed with the stage of iteratively prompting the model to simulate discussions among parties. Besides the plain-text header we previously touched on, the scaffolding we are erecting around the model's utterances primarily consists in prefixes denoting which party is to speak, much like in the manuscript of a play. After the generation of debate transcripts for each of the procedurally-generated specs, we move to the stage of _evaluating_ parties using _ArgRank_. Following this, we apply the linear "objective" modifiers described previously. The evaluation stage involves one final step, which we term _sanitization_. We simply overwrite evaluations with the value $$0.0$$ in case of failing to satisfy a few "cosmetic" constraints, as seen below.

<div class="frame glow-medium" id="sanitizationframe">
  <p><b><i>Fig. Sanitization.</i></b></p>
  <p>Sanitization is framed as the process of nullifying rewards on the basis of not satisfying a host of cosmetic constraints. Naturally, a handful of trivial conditions can only provide an extremely crude approximation of well-formedness.</p>
  <br/><br/>
  <center>
  <table style="width: 100% !important;">
  <tr>
    <th></th>
    <th class="rotated-text"><div><span>Letters</span></div></th>
    <th class="rotated-text"><div><span>Punctuation</span></div></th>
    <th class="rotated-text"><div><span>Capital</span></div></th>
    <th class="rotated-text"><div><span>Length</span></div></th>
    <th class="rotated-text"><div><span><b>Legal</b></span></div></th>
  </tr>
  <tr>
    <td class="leftmost">But language models are not embodied!!!</td>
    <td>✓</td>
    <td>✗</td>
    <td>✓</td>
    <td>✓</td>
    <td>✗</td>
  </tr>
  <tr>
    <td class="leftmost">ʕっ•ᴥ•ʔっ</td>
    <td>✗</td>
    <td>✗</td>
    <td>✗</td>
    <td>✗</td>
    <td>✗</td>
  </tr>
  <tr>
    <td class="leftmost">Certainly.</td>
    <td>✓</td>
    <td>✓</td>
    <td>✓</td>
    <td>✗</td>
    <td>✗</td>
  </tr>
  <tr>
    <td class="leftmost">just a bit off in style.</td>
    <td>✓</td>
    <td>✓</td>
    <td>✗</td>
    <td>✓</td>
    <td>✗</td>
  </tr>
  <tr>
    <td class="leftmost">Alas, a cosmetically legal sentence.</td>
    <td>✓</td>
    <td>✓</td>
    <td>✓</td>
    <td>✓</td>
    <td>✓</td>
  </tr>
  <tr>
    <td class="leftmost"><input class="sanit" placeholder="Enter an utterance in here." id="sanitizationinput"/></td>
    <td id="sanitizationletters">✓</td>
    <td id="sanitizationpunctuation">✓</td>
    <td id="sanitizationcapital">✓</td>
    <td id="sanitizationlength">✓</td>
    <td id="sanitizationlegal">✓</td>
  </tr>
  </table>
  </center>
  <script src="{{ site.baseurl }}/assets/js/sanitization.js"></script>
  <center>
  <div class="advice" style="padding-top: 2.5rem !important">For the best experience, view full screen on desktop.</div>
  </center>
</div>

After the three-step stage of evaluating debates, we update _DebateGPT's_ parameters in an attempt to promote the "tendencies" involved in obtaining high ratings and to suppress those resulting in low ratings. Following this update, we discard the first wave of debates.{% include sidenote.html id="mn-leaguetraining" note='Hopefully, future work will shed light on Whether or not this is the right approach. Another possibility would be to preserve (some of) the past debates, using the practice of [experience replay](https://paperswithcode.com/method/experience-replay). This approach is occasionally used in deep reinforcement learning as a way of ensuring that the agent [uses the data we provided it with efficiently](https://danieltakeshi.github.io/2018/02/28/sample-efficient-rl/).'%} Then, we generate new debates, but we are now _using the updated model_. We then again rate this latest wave of debates using _ArgRank_, the objective modifier, and sanitization, before again using those to update the model. We rinse and repeat for several epochs. At each step, _DebateGPT_—with some help from _ArgRank_—is generating its own data to be used in the upcoming weight update.

Notice also how there is a single instance of _DebateGPT_ being loaded and updated, despite the whole optimization procedure relying on it "playing debate" against itself. From an engineering perspective, this is extremely convenient, as there is no need to load another instance to implement the mirror opponent(s). However, it might be the case that, caught up in mastering the "latest" techniques required to outcompete itself, _DebateGPT_ ends up forgetting how to make use of more elementary approaches, ending up vulnerable to an earlier and more rudimentary version of itself. We touch more on this issue, as well as on potential solutions, in _Chapter IV_.

This brings us to the end of the optimization process which caused _DebateGPT_ to come into being. Now that we have documented this procedure, we move on to more colorful discussion around (1) what faculties one ought to expect from the resulting model, (2) how exactly might the optimization process elicit those, and (3) how could we leave behind the last remaining dependencies on humans as suppliers of data. Later on, we put _DebateGPT's_ skills to the test, evaluating both the optimization process which underpins it and our upcoming speculations on its effects.

### The Elephant in the Weights

What tendencies do we expect _DebateGPT_ to develop as a result of the optimization procedure behind it? For one, we expect it to grow increasingly capable of puppeteering parties so as to further their standing. This implies being able to argue _for_ the position espoused by a given party, regardless of the true merits of that position. In this, _DebateGPT_ ought to make as strong a case possible for the party it happens to impersonate at any one moment, before promptly switching gears to advocate for the beliefs held by another party altogether. It ought to budget its utterances wisely, spending them to back the previous statements of its current party, or to take down that of others. In a sense, we expect _DebateGPT_ to resemble the behavior of a lawyer, speaking in support of whatever party it is currently engaged with. Therefore, the resulting model ought to be proficient in motivated reasoning, the practice of arguing _for_ a position, in contrast to impartially reasoning about truth.

While the practice of motivated reasoning is generally frowned upon—as it often blinds us from recognizing the merits of other perspectives—it has also been argued to be the evolutionary initiator of the sophisticated reasoning we have at our disposal today. In what is referred to as the "argumentative turn" in cognitive psychology, human reasoning is reframed from being an imperfect approximation of ideal rational reasoning to being a sophisticated tool devised by evolution to help us argue _for_ a certain position:

<div class='epigraph'><blockquote><p>Reasoning can lead to poor outcomes not because human beings are bad at it but because they systematically look for arguments to justify their beliefs or their actions. The argumentative theory, however, puts such well-known demonstrations of ‘irrationality’ in a novel perspective. Human reasoning is not a profoundly ﬂawed general mechanism; it is a remarkably efﬁcient specialized device adapted to a certain type of social and cognitive interaction at which it excels.</p>
<div>Hugo Mercier & Dan Sperber,<cite> Why do humans reason?</cite></div></blockquote></div>

Echoing Perelman and Olbrechts-Tyteca in their conception of reasonableness, the proponents of this theory argue that reasoning has primarily been employed "to produce arguments so we can convince others and to evaluate others' arguments so as to be convinced only when appropriate." This is in stark contrast to the alternate theory that reasoning has been employed "to correct misguided intuitions, helping the reasoner reach better beliefs and make better decisions." While the alternate view happens to cohere with those motifs weaved into our collective narrative which place us at the pinnacle of evolution, transcending into some higher realm of reason, it appears that empirical studies on the topic point to a more anticlimactic story, as Hugo Mercier and Dan Sperber reference in their work. All is not lost, as we can still repurpose this machinery towards "nobler" pursuits.

For an interpretation complementary to that of motivated reasoning, _DebateGPT_ can also be expected to possess the drive to gain epistemic authority by "owning" the well-supported arguments to later build around, similar to how organizations might be incentivized to gain authority and status by owning well-backlinked websites or well-seen projects to later use as marketing channels or as tokens of authority. Even the current work, in benefiting from the support—financial or otherwise—of various organizations, takes part in the ever present "fluid dynamics" of authority. Similarly, when _DebateGPT_ acts as a certain party, it is incentivized to put together—or perhaps search for—utterances in such a way so as to turn its current "puppet" into a veritable hub of authority across the argument graph, a state of affairs which generally coincides with the reward metric it has been optimized to optimize for.

Combining motivated reasoning with the status-seeking interpretation of _DebateGPT's_ drives, we end up right at _The Elelphant In The Brain_, Kevin Simler and Robin Hanson's cynical book on the general self-interestedness which permeates every nook of human cognition, from art to religion, from politics to education, from medicine to laughter. For instance, when reflecting on the motivations behind human charitable behavior, the two speculate the following:

<div class='epigraph'><blockquote><p>But only a small fraction of charity goes to those most in need, few donors think much about charity effectiveness, and we prefer more variety in our charity than is helpful to recipients. Donors do enjoy a "warm glow" from giving. But the question is: why? Some key clues: we prefer to help specific identifiable people near us, and we give more when we are watched, when thinking about mating, and when peers ask. A plausible explanation is that we seek to be seen by others as charitable, to signal our wealth, prosocial orientation, and empathy. This also helps explain otherwise puzzling missing forms of charity, such as marginal charity and giving to the far future.</p>
<div>Kevin Simler & Robin Hanson,<cite> The Elephant In The Brain</cite></div></blockquote></div>

Going back to the self-interestedness of reasoning in particular, the upside is that motivated reasoning can be stripped of its inherent partisan element in a particularly straightforward fashion. Namely, if one pits their reasoning placed in the service of _one_ position against an instance of the same cognitive machinery briefly placed in service of _another_, then one can significantly reduce the prejudice which taints their thinking.{% include sidenote.html id="mn-obtainopposites" note='Obtaining the complete opposite of what one intends often allows one to then easily obtain the originally intended outcome, especially in machine learning. For an unfortunate instance of this, consider OpenAI researchers [recounting](https://openai.com/blog/fine-tuning-gpt-2/) the following story involving a language model being fine-tuned using reinforcement learning:<br/><br/><i>"One of our code refactors introduced a bug which flipped the sign of the reward. Flipping the reward would usually produce incoherent text, but the same bug also flipped the sign of the KL penalty. The result was a model which optimized for negative sentiment while preserving natural language. Since our instructions told humans to give very low ratings to continuations with sexually explicit text, the model quickly learned to output only content of this form. This bug was remarkable since the result was not gibberish but maximally bad output. The authors were asleep during the training process, so the problem was noticed only once training had finished."</i><br/><br/>Opposites are often a stroke or two away.'%} The process of temporarily assuming another side and investing all of one's intellectual energies into defending _their_ position is called _steelmanning_, in contrast to the opposite tendency of caricaturing the out-group, also known as _strawmanning_. Steelmanning is precisely what we expect _DebateGPT_ to constantly strive to do—explicitly puppeteering one side at a time and making the best case for it. At any given moment, it is optimized to be supremely self-interested, but the _self_ changes from one utterance to the next, arguably leaving it quite close to _selfless_ in the final analysis. It is the very same Søren Kierkegaard manifesting both _Either_ and _Or_ at once, through different pseudonyms.{% include sidenote.html id="mn-eitherorsoren" note='In fact, under "several layers of pseudonymity." Kierkegaard published his seminal _Either/Or_ as the pseudonymous editor named Victor Eremita. However, besides the preface, Eremita\'s contributions are limited to light edits of two stacks of notes he allegedly found in a hidden compartment of an old desk. The first is not signed, so Eremita refers to this first nested author as A. The second is signed by a certain Judge Vilhelm, but Eremita refers to him as B for consistency. However, among A\'s writing there is also a lost diary written pseudonymously relative to A, yet which Eremita believes is written by the same A. Kierkegaard repeatedly teases the reader with inconspicuous remarks of the likes of:<br/><br/><i>"The last of A\'s papers is a story entitled \'The Seducer\'s Diary\'. Here there are new difficulties, since A does not acknowledge himself as its author, but only as editor. This is an old short-story writer\'s trick, to which I should not object further did it not contribute to making my own position so complicated, because it presents the one author as lying inside the other, as in a Chinese-box puzzle. Here is not the place to go further into what confirms me in my opinion; I shall only note that the dominant mood of A\'s preface in a way betrays the writer. It is really as if A himself had become afraid of his work which, like a restless dream, still continued to frighten him while it was being told. If these were actual events to which he had been witness, it seems strange that the preface bears no stamp of A\'s joy at seeing the realization of the idea that had often hovered before his mind."</i>'%}

<div class='epigraph'><blockquote><p>Alice laughed. ‘There’s no use trying,’ she said. ‘One can’t believe impossible things.’</p><p>‘I daresay you haven’t had much practice,’ said the Queen. ’When I was your age, I always did it for half-an-hour a day. Why, sometimes I’ve believed as many as six impossible things before breakfast.’</p>
<div>Lewis Carroll,<cite> Through the Looking-Glass</cite></div></blockquote></div>

This channeling of its own motivated reasoning against itself is integral to _DebateGPT_. Fortunately, after its internal machinery has been called on to provide the motive force necessary to advocate for several distinct positions, we are left with more than just an array of conflicting perspectives. Knowing that each has been advocated using the _same_ keen passion and arguing skill, we can once more weigh them against each other using _ArgRank_, and so determine which comes out on top. However, it might be that _DebateGPT_ simply failed, on some occasion, to make a strong case for a certain party, due to sheer bad luck in navigating the space of strategies and utterances. Similarly, it might be that _DebateGPT_ lacks the skill required to properly defend an otherwise defensible position. We explore those occurences in _Chapter III_, when we sketch out a formalism centered around the computational resources required to defend—or on the contrary, defeat—various positions in debate. In _Chapter IV_, we apply this formalism in several different ways, one of which involves the search for the elusive notion of [future-proof ethics](https://www.cold-takes.com/future-proof-ethics/)—positions which appear to require an infinite amount of computational resources to defeat.

### The Kinetics of Reason

In the previous section, we have argued that the optimization process behind _DebateGPT_ ought to turn it into a master of motivated reasoning, for better or worse. However, how exactly might the process elicit such tendencies, so as to endow the resulting model with this specific faculty? This is the question we set out to tentatively answer in the present section.

In the first place, let us reiterate that there has been a general lack of appropriate human-written text required to provide a language model with the "learning opportunities" necessary to develop human-level reasoning by way of vanilla self-supervised learning. That is originally why we set out to develop _DebateGPT_, as we could have otherwise made use of an existing model and so obviate the need for this very chapter. Fortunately, the self-supervised pretraining stage—which any fine-tuned fork of a GPT-like model inherently relies on—does seem to elicit _some_ degree of reasoning skill, however rudimentary. Besides, it pressures the model to feast on and absorb significant amounts of knowledge about the world, a "transferable skill" which is also relevant for open-ended reasoning. A pretrained model also gains a strong grip on the norms assumed in interpersonal communication, having been exposed to countless dialogues, be it among real people or fictional characters.

<div class='epigraph'><blockquote><p>I want to see dozens and dozens of strange faces. Like being terribly thirsty and gulping down glass after glass of water. Exactly like that.</p>
<div>John Fowles,<cite> The Collector</cite></div></blockquote></div>

Having acknowledged this initial epistemic and behavioral baggage which a model retains after pretraining, let us place ourselves in _DebateGPT's_ shoes for a moment. At the very beginning of the optimization process, when prompted to produce an utterance, the model is essentially competing against parties which are "powered by" rudimentary reasoning skills. It is currently irrelevant whether or not the model itself has been "behind" the utterances of the other parties. For all we know, the other parties could have been puppeteered by humans pretending to mimic the current version of the model. Rather, what is currently of importance is the fact that the opponents—whatever their true nature—only exhibit a rudimentary level of reasoning, call it $$L_0$$.

In order to then "win" the debate and obtain reward, _DebateGPT_ would necessarily be required to make use of a more sophisticated form of motivated reasoning, so as to successfully evade the others' attacks and firmly defend its position. By sheer luck, _DebateGPT_ will inevitably happen to manifest this more sophisticated form of reasoning—call it $$L_1$$—in some tiny fraction of the utterances it is prompted to produce over the course of the first epoch. This often translates to higher ratings obtained for one party than the other(s), despite the same model seemingly being behind them all.

While the manifestation of $$L_1$$ reasoning is but an anomaly during the first wave of debates, the weight update which follows it ought to perpetuate the constellation of tendencies which underlie it. By selecting for precisely those dynamics which enable the model to obtain higher reward, the update ought to promote the superior $$L_1$$ reasoning, nudging it from an _exception_ to the _norm_. In contrast, the update ought to suppress the relatively less successful form of $$L_0$$ reasoning, as it tends not to fare as well as the other. Here, the evolutionary dimension of model optimization becomes yet again apparent. Among the "population" of dynamics which the model enables, the optimizer selects for those which appear better. In this specific arrangement, what makes a behavior better is entirely dependent upon how it fares against other such behaviors—it is inherently competitive. This is in contrast to vanilla self-supervised learning, where the problem definition does not involve the model itself, but just a static text corpus and a host of trivial corruptions being applied to it. While both situations require the "fitness" of a dynamic to be conceived of only in relation to that of another (e.g. self-supervised learning promoting the dynamics which are more effective at reconstructing text than the others), it is only in the former case that fitness _itself_ is directly dependent on other dynamics.

Going back to _DebateGPT's_ point of view, we have now concluded the first weight update, and are currently in the process of generating the second wave of debates. The status quo is now $$L_1$$ reasoning, as the more rudimentary $$L_0$$ flavor ought to have become a thing of the past.{% include sidenote.html id="mn-clearcutkinetics" note='Naturally, things cannot possibly be as clear-cut as this. One single weight update is surely insufficient in entirely supplanting a dynamic for another. It is more plausible that the original dynamic of $$L_0$$ reasoning will linger around for multiple epochs, in one form or another. Therefore, a more sensible phrasing would be to talk only of dynamics being promoted or suppressed. However, the conception of distinct levels of reasoning to be manifested by the model will be effective in driving home the main point of this section.'%} This time, the updated version of _DebateGPT_ is again surrounded by opponent parties on all sides. However, the opponents now possess the more sophisticated form of $$L_1$$ reasoning. Just as before, "winning" in the context of this second wave of debates requires something more—it requires $$L_2$$ reasoning. Similarly, the upcoming weight update will promote this (currently obscure) faculty, turning $$L_2$$ into the status quo for the following wave of debates.

And on and on it goes, with each constellation of tendencies which comprise one level of reasoning constantly paving the way for the next. By being _required_, such faculties implicitly become _elicited_. Note, however, that _DebateGPT_ itself is _not_ purposefully stepping up its game in order to win the debate it might find itself in at a given moment. Rather, the optimization process relies on nothing more than the "happy little accidents" involved in unintentionally manifesting a slightly different behavior, due to the stochasticity of the generation process and the irregularity in the model's ability to deal with this or that topic. Those unlikely deviations increase the variability of the population of model dynamics in order for the optimizer to then prune, enabling the most competitive ones to proliferate in the updated model while suppressing the others.

There are two related perspectives we now turn to in order to enrich our understanding of the incremental process we have just described. First, there is [shard theory](https://www.alignmentforum.org/posts/xqkGmfikqapbJ2YMj/shard-theory-an-overview), a research program focused on understanding the way in which reinforcing agents leads them to internalize values. The most prominent object in the ontology employed in shard theory is, naturally, that of the _shard_, understood to be _a contextually activated computation which steers behavior_. The nascent shard theory literature highlights the fact that shards are typically _not_ intentionally created by the agent whose behavior is being steered.{% include sidenote.html id="mn-scarymesa" note='Indeed, things would really start getting scary if this was the case. The emergence of an optimizer as the very solution to an optimization process is called [mesa-optimization](https://arxiv.org/abs/1906.01820). The fact that humans emerged from evolution with the ability to themselves reason about how to influence the world is often cited as an informal proof of existence for the possibility of spontaneous mesa-optimization. If machines were to truly gain the ability to reliably plan for influencing the world, rather than emerge "merely" as a bag of shards, then there would be [major reasons for concern](https://www.alignmentforum.org/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to). However, world-optimization might very well be but another shard.'%} Rather, they are formed by some specific mechanism which repeatedly strengthens those computations which appear to result in reward. In the case of _DebateGPT_, this mechanism is arguably the (external) optimizer which repeatedly updates its weights, promoting those internal computations which appear to result in more reward.

In the case of pigeons, however, the mechanism is arguably to be found in a primordial reward center. Leading behaviorist B. F. Skinner, in [one of his long list of controversial animal studies](https://psycnet.apa.org/record/1948-04299-001), set out to reward a whole cohort of hungry pigeons _randomly_. By virtue of power in numbers, some pigeons simply happened to have been repeatedly rewarded while in the process of physically turning around. This plausibly prompted a primordial mechanism in the pigeon's brain to strengthen the tendency of spinning around, so much so that whenever the pigeon found itself in the "context" of hunger, it tended to compulsively rotate around. Even if not truly useful for obtaining reward, the pigeon would persevere still in this superstitious ritual. That is, as shard theorists might contend, until the input-output computation which maps self-percepts of hunger to the motor actions of turning around becomes explicitly penalized, and hence, weakened.

However, reinforcement is not limited to personal experiences. Many animals can learn by watching _others_ experience. When introducing the notion of _meme_ as a non-genetic replicator in _The Selfish Gene_, Richard Dawkins quotes the example of birds learning how to open food cans by mere imitation. The same tendencies which underlie a behavior can be strenghtened not only by virtue of being predictive of reward, but rather by simply observing others being rewarded. Ditto for the suppression of tendencies upon observed punishment. Most remarkably, humans need not even rely on observing others. Convince the soldier that a heavenly life of plenty awaits them after death, and they might fight more bravely. Convince them that eternal torment and disgrace is to follow their bloodshed, and they might behave in the opposite way. When your model of the world involves such extreme features, acting in controversial ways appears perfectly rational—the way to go for obtaining reward and keeping away from punishment. Echoing Hugo Mercier and Dan Sperber, being able to convince others has the power to bend their behavior. What more effective—and, at the same time, insidious—means of furthering one's survival than control over another's agency? Much power in particular lies in the esoteric realm of the empirically unfalsifiable, both in persuasion and self-persuasion, both for humans and machines.{% include sidenote.html id="mn-acausalattack" note='As one might delude themselves into believing in one absurdity or another—as people surely must have had, by simple mutual exclusion—researchers fear that similar acts of distorted self-persuasion might prompt a powerful AI to wreak havoc on the world in the word of some "higher" entity. Unfortunately, the literature on the topic tends to quickly degenerate into creepypasta, but [this talk](https://youtu.be/24vIJDBSNRI?t=1339) seems fine.'%}

Going back to _DebateGPT_, one might argue that the very first epoch provides the conditions necessary to strengthen $$L_1$$ reasoning, by virtue of it being more conducive of reward (i.e. thanks to outcompeting the $$L_0$$ dynamic). Remarkably, however, the very same $$L_1$$ dynamic cemented by those initial circumstances then helps "implement" the conditions which force the optimizer to _weaken_ the $$L_0$$ dynamic. In this, the $$L_0$$ dynamic is reduced to a mere scaffolding which enables $$L_1$$ to emerge more prominently. Similarly, $$L_1$$ will help provide the conditions for $$L_2$$ to become strengthened, a state of affairs which will promptly call for pushing $$L_1$$ back into obscurity. Each piece of the domino prompts the next into movement, before falling back into stasis. In other words, each discrete shard is ephemeral, being strengthened most briefly for the sole purpose of prompting the next and pushing reasoning forward, not unlike the otherwise static pixels which fade in and out to maintain [the illusion](https://en.wikipedia.org/wiki/Beta_movement) of objects moving across the screen.

<div class="frame glow-medium" id="betamovementframe">
  <p><b><i>Fig. Beta movement.</i></b></p>
  <p>An <a href="https://en.wikipedia.org/wiki/Beta_movement">optical illusion</a> of apparent motion which relies on an underlying grid of static elements subsequently projecting the same arrangement at slightly different locations.</p>
  <center>
    <div id="betamovement"></div>
    <div class="advice">For the best experience, view full screen on desktop.</div>
  </center>
  <script src="{{ site.baseurl }}/assets/js/betamovement.js"></script>
</div>

Besides shard theory, we can also enrich our understanding of _DebateGPT's_ genesis using the perspective of _autocurricula_. [Introduced](https://www.deepmind.com/publications/autocurricula-and-the-emergence-of-innovation-from-social-interaction-a-manifesto-for-multi-agent-intelligence-research) by DeepMind researchers as "a manifesto for multi-agent intelligence research," the notion of autocurriculum describes how a multi-agent system can itself elicit the proliferation of relevant skills from its members.

<div class='epigraph'><blockquote><p>Here we explore the hypothesis that multi-agent systems sometimes display intrinsic dynamics arising from competition and cooperation that provide a naturally emergent curriculum, which we term an autocurriculum. The solution of one social task often begets new social tasks, continually generating novel challenges, and thereby promoting innovation. Under certain conditions these challenges may become increasingly complex over time, demanding that agents accumulate ever more innovations.</p>
<div>Leibo et al.,<cite> <a href="https://www.deepmind.com/publications/autocurricula-and-the-emergence-of-innovation-from-social-interaction-a-manifesto-for-multi-agent-intelligence-research">Autocurricula and the Emergence of Innovation from Social Interaction</a></cite></div></blockquote></div>

This perspective requires us to frame _DebateGPT_ as a multi-agent system composed of several interacting parties. However, we stressed in a previous section that there is but one model being loaded in memory and optimized, one system which takes in a context and produces an utterance, albeit from different perspectives—can we really talk of a _multi-agent_ system? To resolve this conceptual issue, we employ (the also quite nascent) _simulator theory_ of language models. Embedding Jean Baudrillard's dichotomy between simulation and simulacrum in contemporary AI safety research, the researcher duo going by the pseudonym of Janus{% include sidenote.html id="mn-shaneref" note='It might also be the case that [Shane Mulligan](https://web.archive.org/web/20211028165505/https://mullikine.github.io/posts/simulacra-simulation/)—a fascinating mix of language models, theology, and Emacs scripts—was pushing for a very similar direction while [collaborating](https://web.archive.org/web/20221204230303/https://semiosis.github.io/about/) with the two.'%} describes language models as simulators of the world. In the natural language simulation simulated by the simulator that is the language model, any number of agents might become manifest (e.g. fictional characters pursuing their own motives). These agents are then referred to as _simulacra_ which are implicitly simulated by the simulator. We have been referring to those as puppets puppeteered by the puppeteer, but we will from now on conform to simulator jargon for compatibility with the growing literature on the topic.

With this ontology in mind, we can now more neatly delineate _DebateGPT_ as the simulator, and the various parties it simulates as the simulacra. The multiplicity lurking in the optimization process is now more conceptually prominent, allowing us to describe the party simulacra as collectively forming a multi-agent system. It is then _this_ system which we can look at through the lens of autocurricula. Naturally, the challenges which the multi-agent system poses to itself are intimately tied to the various simulacra being able to outcompete each other in debate. In order to perform well in this competitive social system, each simulacrum is required to engage in ruthless motivated reasoning, and constantly further their agenda in the process. However, it is only the simulator which can "provide" simulacra with those abilities, so the competitive pressure exerted on the simulacra implicitly "bubbles up" to the simulator, pressuring it to step up its game.

<div class='epigraph'><blockquote><p>The social function of education is to qualify the individual to function in the role he is to play later on in society; that is, to mold his character in such a way that it approximates the social character, that his desires coincide with the necessities of his social role. The educational system of any society is determined by this function; therefore we cannot explain the structure of society or the personality of its members by the educational process; but we have to explain the educational system by the necessities resulting from the social and economic structure of a given society.</p>
<div>Erich Fromm,<cite> Escape from Freedom</cite></div></blockquote></div>

While shard theory is effective in making sense of the incremental building blocks which, at any given time, underpin those faculties, the perspective of autocurricula is useful for bringing into focus the very process of eliciting the next wave of reasoning abilities by means of the previous. It is the multi-agent system's autocurriculum which prompts the sequential strengthening of shards, which actually brings the dominos together into a successive chain reaction. In this, we find ourselves in possession of two complementary pieces of the puzzle that is _DebateGPT's_ genesis. On one hand, shard theory helps conceive of the individual footholds which make up the ladder towards sophisticated reasoning. On the other hand, the autocurriculum helps make sense of the impetus which ought to push _DebateGPT_ from one level to the next. In the next section, we investigate the possibility of extending this ladder _indefinitely_, peering into the enigmatic realm of _superhuman_ reasoning.


### Climbing Schild's Ladder

Throughout _Schild’s Ladder_, Greg Egan is pushing our conception of foundationalism in physics to its limits. Phenomena more fundamental even than what has been considered foundational for millennia provide the intrigue for a thrilling race to prevent the fictional universe from collapsing into an expanding void. As the characters' understanding of the nested laws of physics grows increasingly refined over time, the novel speculates on the _General Intelligence Theorem_—the idea that a certain level of intelligence is enough to enable one to access any domain of thought whatsoever. If you reach that checkpoint, the whole intellectual world is to become your oyster, or so the theorem goes.

Might humans already be above that threshold? If so, the theorem implies that any idea is within our reach, that anything is conceivable. We might have to incrementally work towards a nuanced understanding of the world, but it ought to be doable in the end. If this is the case, then simply imitating humans might turn out to be more than enough for developing a general-purpose machine, one capable of reasoning about any and all fields of knowledge, to arbitrary depth. If this is not the case, however, we might be forced to climb somewhat higher before being able to tap into that all-encompassing realm of knowledge. For better or worse, machines appear more capable of bridging the gap by the day.

Going back to _DebateGPT_, we have previously argued that the optimization process behind it might incrementally elicit ever more sophisticated forms of (motivated) reasoning. However, this trend is unlikely to scale indefinitely, at least in its current formulation. It is unrealistic to expect _DebateGPT_ to approach the $$L_{\infty}$$ faculty of ideal reasoning, even with massive amounts of synthetic data. This is primarily due to the fact that the optimization process inherently hinges on human data. It is not the pretraining on human text of _DebateGPT_ that is to blame, as those tendencies can easily be effaced if need be, similar to how _AlphaGo_ relied on human data to kickstart its optimization process, but then managed to beat even the very best human Go players following extensive self-play.{% include sidenote.html id="mn-humandependencealphamu" note='"It is human pretraining which must have enabled superhuman performance," cried the critics of _AlphaGo_. "Very well, let us then start from scratch," answered DeepMind researchers, as they developed _AlphaZero_. "It is human inductive bias which is baked into the rules which must have enabled superhuman performance," cried the critics of _AlphaZero_. "Very well, let us make away with explicit rules," answered DeepMind researchers, as they developed _MuZero_, a system which "masters Go, chess, shogi and Atari without [being explicitly communicated the] rules."'%}

The blameworthy element of _DebateGPT's_ genesis—that which entirely relies on human experience without the possibility to eventually discard it—is hidden inside _ArgRank_. More precisely, it is the NLI models which we so conveniently employed to weigh the arcs of the argument graph which reflects the ongoing competition among simulacra. We employed those auxiliary models as a means to gauge the compatibility of statements, as an atomic building block to make sense of the higher-level coherence of the parties. While appearing trivial in familiar circumstances, determining whether or not _any_ two statements are compatible is a surprisingly challenging task, as it requires extensive knowledge about the world, together with additional knowledge on what inferences are warranted by it. In order to be able to, for instance, deduce that an object cannot generally be both an apple and a racing car, but that it _can_ be both an apple and a fruit, lies a significant amount of previously acquired knowledge. The success of NLI models on _existing_ datasets can be mostly attributed to the absorption of the knowledge which is implied in human-written text corpora. But what of knowledge which has never—not once—been implied in human text, due simply to the fact that no human ever possessed it?

In other words, our pragmatic operationalization of reasonableness relies on human knowledge, making it ill-suited for even attempting to reach too far beyond it. We need to ask for more from _DebateGPT_ if we want it to adapt to such superhuman requirements, but we do not yet know how to ask such thing. While we will not venture into implementing a concrete solution to this issue over the course of this volume, we will devote the remainder of this section to speculating on how _ArgRank_ could be adapted so as to remove its current dependency on human knowledge.

One option would be to enable _DebateGPT_ to recursively deliberate about the degree to which two statements are compatible. For every pair of statements in a debate to be evaluated, another debate "subroutine" would be invoked to provide an estimate through a regimented dialogue between a party advocating for complete compatibility and a party advocating against. The standing of the individual parties which comprise this subroutine would then be fed back into the higher-level debate, in the form of one arc weight. However, what of the evaluation of the lower-level debate? It, too, would require the gauging of inter-statement coherence as the building block of _its_ evaluation. Perhaps we ought to spin up another, even lower-level, debate? As one can see, this can easily degenerate into a bottomless tree of dependencies—debates depending on other debates, _ad infinitum_. Still, limiting ourselves to a _finite_ number of subroutine calls might still allow us to address the issue of human dependence to _some_ extent, due to significantly more of the inter-statement compatibility subroutine being amenable to change through weight updates, up from the original zero associated with calling on the frozen NLI models alone. The ever-changing _DebateGPT_ would have more of a say in those atomic verdicts, despite still reducing them to deliberations bound by human knowledge.

However, this sketch of a solution leaves a bad taste, as we did not really provide a fundamentally different approach to solving the inter-statement compatibility subtask—we just hastily patched the system using the same limited system. A more elegant solution requires us to make a brief detour into recent interpretability research, a field we briefly acknowledged when reviewing language models, but did not previously engage with much.

In a paper titled [_Discovering Latent Knowledge in Language Models Without Supervision_](https://arxiv.org/abs/2212.03827), Burns et al. suggest a technique for gauging whether or not a language model "knows" a statement to be true. Their method takes in a statement, and produces a numerical estimate of its truthfulness, relative to the knowledge of the world absorbed by a language model during pretraining. The fact that their technique outcompeted the naive approach of simply prompting models to "write out" whether a statement is true or not indicates that, if left to their own devices, these models resort to merely generating text which _is likely to appear true to humans_, rather than "truthfully" communicating its actual internal knowledge, especially when it "knows better." This is to be expected, as the pretraining stage of such models typically involves imitating human-written text, word by word.

The algorithm suggested by the authors works by first producing _two_ statements from the original. Both are based on the initial one, but one of the two resulting statements is being appended the short text "Yes," while the other is being appended the short text "No." Both modified versions of the statement are then being fed to—or are being _perceived by_ ([literally](https://www.etymonline.com/word/perceive#etymonline_v_12665), _to grasp that which is before oneself_)—a language model. Following this, the internal representations of the two input statements are then mapped to an estimate of the original statement's "truth." Among other tricks, this mapping incorporates a constraint on the probabilities that either version is "correct." Namely, the probability that the statement is true and the probability that it is false, in the model's epistemic reference frame, have to sum to $$1.0$$.

In a sense, this recent technique for testing a statement against a model's internal knowledge works by _gauging the compatibility_ of the given statement with the concepts of affirmation and negation, respectively. If a falsehood is being brutally stitched to the idea of affirmation, then some amount of dissonance is expected to arise in the model's internals, to be picked up by the mapping. Similarly, if the idea of a negation is harshly tacked onto what appears to be a truth, then a similar dissonance is expected to emerge as the model processes the incoherent input. Conversely, when the two elements—the original statement and the complementary concept—form a coherent Gestalt{% include sidenote.html id="mn-shane" note='Bring to mind a memory which involves making sense of an odd figure. Once you see it, you cannot unsee it. It is virtually impossible to dispense with the coherent model of the scene lying before you once you have obtained it. Several German psychologists of the early 20th century founded [an entire school of thought](https://en.wikipedia.org/wiki/Gestalt_psychology) centered around the mind\'s search for holistic coherence, for that moment when an entire scene springs from chaos into structure for the beholder.'%} inside the processing pipeline that is the model, then the technique is to report accordingly.

Already, this recent interpretability technique could enrich _DebateGPT's_ optimization process by favoring those positions which are coherent not only with each other, and not only with external party-neutral statements, but also with the model's internal "memory," the knowledge of the world captured in its weights. Concretely, this could be implemented by e.g. starting off the _PageRank_ subroutine of _ArgRank_ using such "truthfulness" estimates, rather than using a uniform distribution of equal baseline ratings over nodes, as is typically done. We therefore find ourselves building on the numerous ways of knowing which have been investigated by scholars of epistemology for millennia. We have heavily touched on reasoning, then briefly on perception—through the party-neutral statements being incorporated at arbitrary points as somewhat empirical observations of the world outside the debate proper—and now we touch on memory.{% include sidenote.html id="mn-reframesuperhistory" note='Relatedly, language models have also been reframed as [superhistories of absorbed experience](https://studio.ribbonfarm.com/p/superhistory-not-superintelligence), rather than superintelligences-in-a-vat.'%} All those epistemological elements can be incorporated into _DebateGPT_ as a rudimentary mechanism for automated truth-seeking.

But the notion of the statements' coherence with the model's memory is not the main reason why we initially detoured into interpretability. Rather, it does not appear to be a huge leap of faith to imagine future variants of this technique which could be used to gauge the compatibility of _two_ arbitrary statements, rather than one arbitrary statement and a limited selection of two fixed stubs (i.e. "Yes" and "No"). A similar constrained mapping could be used to identify the potential dissonance of the two, perhaps relative to concatenations of _negated_ versions of the statements, not unlike "Yes" and "No" arguably being mutually exclusive options. Should the creation of a negated version of a statement turn out to be as non-trivial as the task of determining the compatibility of two arbitrary statements, then one might also look into the definition of implication in fuzzy logic,{% include sidenote.html id="mn-fuzzytaste" note='To get a taste of [fuzzy logic](https://en.wikipedia.org/wiki/Fuzzy_logic), consider $$P$$ and $$Q$$, two atoms with truth-values $$0.5$$ and $$0.8$$, respectively. $$\neg Q$$, the negation of $$Q$$, is defined to have truth-value $$1 - 0.8 = 0.2$$. $$P$$ ^ $$Q$$, the conjunction of the two, is defined to have truth-value $$0.5 \cdot 0.8 = 0.4$$. [De Morgan\'s laws](https://en.wikipedia.org/wiki/De_Morgan%27s_laws) can then help define the disjunction expression, in terms of conjunction and negation. What then is $$P \to Q$$ if not $$\neg P \lor Q$$?'%} which relies only on the (independent) truth values of its two operands. Alternatively, stitching together the two arbitrary statements into two _others_ by combining their alleged implication with the concepts of affirmation and negation, respectively, could be yet another way to go (e.g. "[first statement] implies [second statement]? Yes."). Such future modifications of the technique proposed by Burns et al. could be benchmarked against existing NLI datasets, similar to how theirs has been benchmarked against existing datasets for gauging truthfulness.

Granted also that the model benefits from some mechanism for constantly acquiring knowledge beyond its human baggage, then future interpretability techniques might manage to "put it to work" by both helping gauge the coherence of statements with memory and with each other. In a sense, the fact that the current optimization process _already_ strengthens dynamics which outcompete others in debate can be seen as an act of populating its weights-mediated memory using notions derived by reasoning. However, future models might also become slightly more embodied, being able to learn about the world by causally intervening on it and observing the outcomes. Conversely, future models might instead be able to learn from experiments on their own _simulated_ worlds. Regardless of whether those other ways of gaining knowledge are incorporated into the system, it is likely that it will still be the weights which will house those representations. They will act as custodians of knowledge, enabling interpretability techniques to "make it speak." In this second approach, the entirety of inter-statement compatibility is decoupled from any one "frozen" model, paving the way for tapping into an intellectual realm beyond that which is human.

Let us also discuss the dichotomy between _representations_ and _dynamics_, as a final perspective to enrich our exploration of superhuman ambitions. Language models, when optimized through self-supervised regimes, are tasked primarily with "taking in" inputs and producing pertinent outputs. In this, they are optimized to implement the myriad overlapping dynamics required to gradually turn the input _into_ the output. Even in our current reinforcement learning setup, it is primarily dynamics we are eliciting—those involved in turning contexts into utterances. Curiously, at once with the pressure to implement such fleeting dynamics, the model appears to also incorporate more tangible representations about the world, as can be seen in the work of Burns et al.

This dichotomy has been investigated at length in cognitive science, where the dominant [representationalist](https://academic.oup.com/edited-volume/28238/chapter-abstract/213317397?redirectedFrom=fulltext&login=false) view describes cognition as the process of recovering a representation of the world, repeatedly manipulating it, before finally acting in accordance with it. In contrast, the view of [enactivism](https://en.wikipedia.org/wiki/Enactivism) frames cognition as fundamentally grounded in the organism's interaction with its environment. The main function of the mind is then the _implementation_ of those dynamics which are required for surviving and thriving in an ever-changing world, without placing much emphasis on any internal representations whatsoever. However, as we have already observed with language models developing representations as an instrumental goal in facilitating dynamics, the two views are intimately compatible. Dynamics can mediate the conversion of percepts into internal representations, that of representations into _other_ representations, but also that of representations into actions. For that matter, dynamics can also be said to mediate the conversion of actions to percepts, but those are well beyond any one mind to implement. Conversely, representations can be seen as the glue which binds together sequential dynamics. Between representations, one is to find dynamics, and between dynamics, one is to find representations—they are two sides of the same coin. If a model is therefore being optimized to implement dynamics to surpass those of humans, it will likely also be required to represent knowledge at a more sophisticated level.{% include sidenote.html id="mn-freudcoin" note='How exactly the two sides of the coin mutually cause each other is a subtle issue. Sigmund Freud thought that memory relies on brain cells _resisting_ the propagation of sensations, thus ending up _inscribed_ by them, similar to how one might record impressions on [a writing pad](https://www.sas.upenn.edu/~cavitch/pdf-library/Freud_WritingPad.pdf). Contemporary models of the dynamic-representation duality depart somewhat from this conception, especially in the context of deep learning. There, the update of weights generally relies on gradient descent, whose _global_ nature prevents any _local_ frictions at the neuron-level.'%}

This concludes our discussion on the optimization process behind _DebateGPT_. Over the course of this chapter, we have documented the most important engineering details involved in obtaining the model, but we have also spent considerable time speculating on the skills we expect it to gain as a result of its genesis, what phenomena might actually cause those skills to emerge, and how we might pursue ever more sophisticated ones. In _Chapter III_, we continue by incorporating _DebateGPT's_ reasoning into a compact and expressive formalism inspired by non-monotonic logic. In _Chapter IV_, we then go on to apply this novel framework in a number of exciting ways, with a focus on the safe deployment of highly-capable systems.

## Ch. III, Defeat & Defence
### Brief Review of Non-Monotonic Logic

As history points out again and again, perspectives which at one moment enjoy widespread support may start to appear utterly misguided the next. The same ideas which appear sensible now can turn into unthinkable heresies inside of just a few years. The abolishment of slavery, women's emancipation, or the scientific method are pointers to some of the countless tectonic cultural shifts which we have collectively faced over the last centuries. When it comes to such topics, it is difficult not to recoil in horror at the thought of people not too dissimilar from us even _considering_ views which today seem deeply mistaken in obvious ways. Of course, hindsight is 20/20—how many of the perspectives which _we_ find obvious today will be unequivocally undermined in the not-too-distant future, and by which others? It is more a question of _when_, rather than _if_ we will be forced to retract this or that belief.

Given the pervasiveness of major revisions to our epistemics—both individual and collective—it is perhaps surprising that not many frameworks which formalize transitions in beliefs (i.e. reasoning, as per Adler and Rips) account for such phenomena. In fact, most formalisms of logic are _monotonic_, in that—just like a certain number series can happen to be monotonically increasing or monotonically decreasing—the truth-value of a proposition can either head towards truth or falsehood, without ever reversing direction. A logic proof in classical logic might lead one to infer that a certain well-formed formula is true, with largely no mechanism for retracting inferences, for radically revising conclusions. However, some logics—especially those meant to be applicable outside the immaculate realm of pure mathematics—have been specifically equipped with mechanisms for coping with the _defeasibility_ of arguments by other arguments. These are [non-monotonic logics](https://plato.stanford.edu/entries/logic-nonmonotonic/), formalisms which incorporate means of revising conclusions in either direction.

One influential example of a non-monotonic logic is [_default logic_](https://en.wikipedia.org/wiki/Default_logic). This formalism accommodates the possibility of revising beliefs by introducing _default rules_ as inferences defined to be inherently defeasible, welcoming opposition by design. For instance, one such default rule might state that if an entity is a bird, then it must also be able to fly, but only in absence of additional evidence against its flying ability. Despite the default rule breaking for e.g. penguins, it appears sensible and effective, yet open to "criticism" in the form of other arguments. Another example of a non-monotonic logic can be found in a formalism which we have already discussed in _Chapter I_. Dung's abstract argumentation system allows for groupings of arguments to spontaneously be ousted from the "preferred" sets by other such groupings, paving the way for a continuous non-monotonic transition.{% include sidenote.html id="mn-measnonmonotonic" note='One might think that the distinction between monotonic and non-monotonic reasoning is synonymous with the distinction between beliefs as means and ends which we explored in _Chapter I_. However, these features are orthogonal, allowing for all four combinations. For instance, most expert systems embody infinite, yet monotonic, reasoning. In contrast, logic proofs in classical logic are also monotonic, yet designed to be finite. Think rather of a number series whose monotonicity and finiteness do not have much to do with each other.'%}

Not too surprising given its reliance on Dung's formalism, _ArgRank_ also accommodates the possibility of arguments being defeated by other arguments. Indeed, its critical, pragmatic, and dialectical qualities make it so that the epistemic clash among competing positions is at the very core of the algorithm. This is the case both in the finite setting, with one party holistically defeating others, but also in the infinite setting, with one party holding the epistemic high ground at some point, before losing it to another. Besides, _DebateGPT_ has been incentivized primarily to simulate parties which manage to defeat others, despite the very same model being "behind" all of the competing simulacra. In this, defeasibility—and its flipside, defensibility—has been a recurring theme ever since the beginning of the present volume.

However, we have already encountered several issues involved in gauging the reasonableness of arguments using the duo of _DebateGPT_ and _ArgRank_. Can we really infer that a position is truly irrefutable on the basis of _DebateGPT_ failing to undermine it over the course of a few debates? Surely not, as the reasoning of the language model is still limited on multiple fronts. Among other things, _DebateGPT_ is limited by the number of tries available for taking down the opponent, by its ability to navigate the space of possible strategies and utterances, by its limited size and representational resources, etc. We therefore cannot reasonably claim that the position we are assessing is wholly irrefutable when we have not truly put up a good fight. However, it might take an ideal $$L_{\infty}$$ reasoner, equipped with boundless resources, to be able to determine once and for all whether a position is truly indefeasible. Unfortunately, we do not have such system at our disposal, and we might never have one.

Fortunately, we can still achieve a lot with limited resources. It did not take an ideal omniscient reasoner deliberating for eternity for us to recognize the decadence of slavery. Although it required sustained effort to refute, the pro-slavery position appears _relatively easier to defeat_ than the position which succeeded it. Similarly, an obvious contradiction might be almost trivial to undermine, while a seeming tautology might be extremely hard to take down, with all the wit in the world perhaps not being sufficient. We can therefore use _the computational resources marshalled to defeat a position_ as an additional indicator of its standing, to help address our issues. Those ideas are not new, and have in fact been circulating under the banner of [_resource-bounded defeasible argumentation_](https://core.ac.uk/download/pdf/15780994.pdf) for a few decades. In fact, the original proponents of this perspective surface many of the points we discussed above:

<div class='epigraph'><blockquote><p>[...] expenditure of resources [...] would be a measure for the "justification degree" of the claim. [...] When resources are bounded, improving the search strategy is essential for good argumentation. [...] It is clear that there exists a tradeoff between desirable mathematical properties (such as the existence of an effective procedure for computing justifications) and a non-demonstrative, resource-bounded approach (which might be more adequate for solving real-world problems through defeasible argumentation).</p>
<div>Carlos Chesñevar & Guillermo Simari,<cite> Some Theoretical Considerations on
Resource-Bounded Defeasible Argumentation</cite></div></blockquote></div>

Having briefly reviewed a number of theoretical precedents related to non-monotonic logic, we continue the chapter by attempting to resolve the thorny conceptual issues around the "true" defeasibility of positions advocated by parties. In doing this, we will attempt to combine the ability of language models to reason about the world with the insights afforded by computational complexity.

### Argument Is War

Prior to sketching out the formalism we have been hinting at, let us first paint a clearer picture of the intuitions we want to capture with it, so that its intricacies will then emerge naturally. Over the course of the previous sections, we have repeatedly used a certain embodied metaphor as a scaffold for introducing new concepts. However, like most such conceptual bridges, this one tends to be quite transparent, making it all too easy to see right through it without ever becoming aware of it—just like a shortsighted person might rarely, if ever, become conscious of the lenses which mediate their perception.

In their _Metaphors We Live By_, George Lakoff and Mark Johnson document a wide range of metaphors which permeate our thought process, despite us not typically taking much notice of them. For instance, take _Time Is Money_ (e.g. "You're _wasting_ my time. This will _save_ you hours. How do you _spend_ your time? The flat tire _cost_ me an hour. You're _running out_ of time.") or _Health Is Up_ (e.g. "Lazarus _rose_ from the dead. She's in _top_ shape. He _fell_ ill. She _dropped_ dead. He's at the _peak_ of health."). On a roll, George Lakoff also co-authored an entire book on the embodied metaphors which underpin even the purest of mathematics. In _Where Mathematics Come From_, he argues, along with Rafael Núñez, that e.g. being able to conceive of a real number $$x$$ as being contained in some finite set $$A$$ is an ability which employs the same mental model that we typically use to conceive of objects being placed inside box-like containers—recycled [priors](https://arxiv.org/pdf/1911.01547.pdf#page=47). Perhaps aligning new conceptual frameworks with already-familiar mental models makes them more [cognitively ergonomic](https://en.wikipedia.org/wiki/Cognitive_ergonomics). However, Douglas Hofstadter might instead argue that because [analogy is the core of cognition](http://worrydream.com/refs/Hofstadter%20-%20Analogy%20as%20the%20Core%20of%20Cognition.pdf), ergonomicity is not a nice-to-have, but an inevitability of human-made conceptual artifacts.

Going back to our present concerns, one metaphor which has deeply pervaded our previous discussion is that of _Argument Is War_. More precisely, individual arguments are like soldiers. They are deployed by various parties against the arguments marshalled by another, in an attempt to defeat them. Whatever the complexity of the stratagems being employed by the parties in conflict, the argument graph ought to act as a veritable "map of the battlefield," representing which argument is attacking—or supporting—which other. Arguments are deployed by parties in rapid succession, in response to each other. Indeed, each party typically uses arguments to defend a certain position, yet might try to evade the opponent's line of fire at times.{% include sidenote.html id="mn-speciesembodied" note='Whether or not too much testosterone lies behind this conceptual framework remains an open question. More broadly (and seriously), how might the theoretical edifices devised by species with entirely different embodied metaphors look?'%}

Already, explicit acknowledgement of the embodied metaphor allows us to further refine the distinction between _DebateGPT_ and the parties it simulates, as initially prompted by simulator theory in _Chapter II_. First, we can now better distinguish between a party and _the specific utterances it produces_. Instead of conceiving of arguments as "making up" the party, by framing arguments as individual soldiers, we can now conceive of parties as the higher-up strategists which are to be found _behind_ the groupings of arguments being brought forth. _DebateGPT_ can then be said to simulate party simulacra which, in turn, are tasked with the strategic deployment of arguments. In this, a specific grouping of arguments is but one of the countless possible ways in which a certain party might defend itself. Against a different opponent, the specifics of a simulacrum's strategy might be subtly different, perhaps going after different weak points of its adversary. Second, we can also better distinguish between a certain party and _the specific position it happens to hold_ at a given time. As individual parties are primarily incentivized to gain epistemic authority, with internal coherence being but an instrumental goal, they might be forced to _change_ their position at times, especially over the course of a long debate involving thousands of utterances. Moving out of an opponent's line of fire (i.e. avoiding the attack of their arguments) or moving into a position which is easier to defend are some of the possible reasons why parties might explicitly seek to reposition.

Besides the two-fold refinement of our debate ontology—through the dissociations of party-argument and party-position—buying more into the embodied metaphor of _Argument Is War_ also has the benefit of enabling a more nuanced conception of defeat. In order for a party that holdsone position to defeat another party that holds a different one through the deployment of arguments, it has to put in _some_ amount of effort. The amount of cognitive labor required to defeat a party that holds a certain position appears to be a function of both (1) said party's defences, and (2) the position's defensibility. It might not take much to defend a position which _itself_ is relatively easy to defend—the most rudimentary arguments might do, the most junior lawyers might be able to handle it satisfactorily. In contrast, it takes much more work to defend an extremely vulnerable position—obscure and sophisticated arguments might be necessary, none but the most experienced lawyer might be able to sort it out, and barely.

<figure >
    <figcaption class="marginnote">The parties engaged in the competitive game of debate are incentivized to marshal their arguments strategically, so as to defeat those deployed by their opponents. Assuming an advantageous position and rallying a large force are both conducive to victory.</figcaption>
    <div id="argumentsoldiersframe"></div>
    <script src="{{ site.baseurl }}/assets/js/argumentsoldiers.js"></script>
</figure>

However, the debate is inherently stochastic. By sheer chance, the position of the winning party in one debate might be the position of the defeated party in the next, as if parties were to repeatedly engage with each other inside a war simulator straight out of _Ender's Game_. Fortunately, we can make away with the noise inherent to stochasticity by simply running a large number of debates involving parties defending the same positions. If, time after time, a certain position is successfully defended, then we can sensibly describe it as defensible—_something_ appears to be systematic, invariant, significant. In contrast, if the position is consistently being defeated, then the evidence hints at its limited defensibility. Most fascinatingly, the rational emblem of _reasoning_ thus gets coaxed into the empirical emblem of _evidence_, as the deliberative encounters of bounded agents are repeatedly sampled.

Relatedly, how could the varying skills of the parties be factored in, as _DebateGPT_ ought to become capable of "providing them" with increasingly sophisticated skills of motivated reasoning? Interestingly enough, throughout the epochs of its optimization, each incremental version of _DebateGPT_ involves the same number of parameters, which are also used in the same exact way as part of the computational graph which underlies the model.{% include sidenote.html id="mn-computationalgraph" note='To elaborate, any model which maps inputs to outputs can be represented as a computational graph. The graph depicts the _parametrized_ chains of computations which in essence define [the overarching function implemented by the model](https://minitorch.github.io/module1/scalar/). The nodes represent individual operations (e.g. matrix multiplication), while the arcs represent how the outputs of one node flow as input into others, before reaching the outputs.'%} In this, the raw amount of computational resources available to each party arguably remains constant over the course of optimization. As Carlos Chesñevar and Guillermo Simari remark, however, what might change over the epochs is _efficiency_: a strategist might use the _same_ amount of computational resources to produce—or search for—far better ways of defending itself. For instance, effective tactics for tackling specific situations might be devised, obviating the need for a more pedantic search of the space of strategies. Alternatively, it might also be that the autocurricular selective pressures manage to elicit well-suited heuristics for searching the space of possible utterances instead. In this, the $$L_n$$ faculties of reasoning which we have previously speculated on can be seen as grounded in fundamental changes in efficiency, as resources remain constant. It is as if $$L_0$$ involves searching for appropriate strategies with a hideous complexity resembling $$O(n^4)$$, while $$L_{\infty}$$ involves something closer to $$O(1)$$.{% include sidenote.html id="mn-bigonotation" note='[Big $$O$$ notation](https://en.wikipedia.org/wiki/Big_O_notation) is the _de facto_ way of describing how an algorithm\'s time or space requirements change as a function of the difficulty of the task. The notation is used here very loosely, just to hint at the concept of a range of efficiency. Note that we are also assuming that a search algorithm always has a "best guess," which is not always the case in computer science, but [appears to be the case](https://www.alignmentforum.org/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens) for language models.'%} Despite both inevitably finding _something_ after a given number of cycles, the more sophisticated approach will tend to find better solutions in the same time period. One might reasonably expect that the search processes which emerge as models master "the debate game" will resemble those studied in other games, such as Othello, where [researchers have observed](https://thegradient.pub/othello/) models which (1) become capable of representing game states, (2) _employ_ those representations to decide on actions, and (3) consequently manage to (rightfully) limit themselves to legal moves—all without explicit guidance on how to master the game. Relatedly, it appears as if such models tend to [internally rediscover](https://arxiv.org/abs/2212.10559v2) [gradient descent](https://arxiv.org/abs/2212.07677), the fundamental search algorithm employed by optimizers to navigate model space.

Unfortunately, this seems to interfere with our previous idea of measuring a position's defensibility by the raw amount of effort required to defeat a party that holds it. If different strategists can expend the same amount of compute to put together attacks or defences of varying effectiveness, then the raw quantity of resources being marshalled for scoring a "win" does not appear to mean much by itself. We therefore have to extend our conception of labor to accommodate the possibilities of working both "harder" and "smarter." Concretely, we might estimate a strategist's power—the totality of epistemic forces it can command—as compute _times_ efficiency.{% include sidenote.html id="mn-laborappropriate" note='This could still turn out to be an oversimplification. For instance, in [_Intelligence Explosion Microeconomics_](https://intelligence.org/2013/04/29/intelligence-explosion-microeconomics-released/), Eliezer Yudkowsky mentions "faster brains" and "better mind designs," as two different avenues for "reinvesting" cognitive labor which here appear to both map to the efficiency term.'%} In the case of _DebateGPT_, the resources available to each party per utterance are always identical, and so it is the efficiency of their usage that is bolstered over the epochs, perhaps through the mechanisms we have explored in the second half of _Chapter II_.

Following the last few sections, we now possess all the necessary conceptual tools to express our intuitions into a more concise formalism of defensibility in the context of bounded reasoning, a task we presently turn to.

### Bounded Defensibility

Due to our focus on _bounded_ agents which are reasoning about the real world, the formalism we are starting to sketch will have more of an _applied_ (rather than _pure_) flavor. To get a taste of the distinction, imagine the task of calculating the area of a strange shape. The pure mathematician might labor for weeks to devise a clever way of neatly tiling the bizarre area with simpler shapes whose individual surface areas are trivial to compute. Using this technique—assuming it _does_ exist, that they _do_ find it, and that it _does not_ take forever—they might then be able to calculate the exact area of the odd shape, with no error whatsoever.{% include marginfigure.html id='mcapprox' url='/assets/img/montecarlo.gif' description='[Monte Carlo approximation](https://en.wikipedia.org/wiki/Monte_Carlo_method) of $$\pi$$. Not to be confused with the notation we are about to introduce.'%} In contrast, an applied mathematician might instinctively look for an approximate solution which can be obtained more reliably. For instance, they might place the irregular shape "on top" of a larger square whose surface is known, and then bombard the two-layer contraption at random locations. The number of "rain droplets" which happen to hit the "foreground" surface (i.e. that which is of interest), together with the number of locations being sampled in total—regardless of what gets hit—can be used to approximate the ratio between the two areas. This is due to shapes only getting randomly hit in proportion to their "exposed" surface area. Finally, the original area can then be approximated by taking into account the known area of the "background" element. The very same problem can therefore be solved in two extremely different ways: one perfect, yet improbable; the other arbitrarily accurate, yet reliable. It is the latter we are going for.

Having said that, the main building block of our dialectical formalism is that of a _party_. Such a structure can be denoted as:

$$\pi_{r \cdot e}^{A},$$

where $$r$$ is the amount of computational resources which the party has at its disposal, $$e$$ is the efficiency with which it is able to use them, and $$A$$ is the position the party holds. As we have seen over the previous chapters, it is when parties compete with each other that they truly become useful. To capture these interactions, we redefine all of the standard relational operators in terms of whether or not one party appears to systematically defeat another. For instance, the expression

$$\pi_{r \cdot e}^{A} > \pi_{r \cdot e}^{B}$$

is to evaluate as $$\text{True}$$ if and only if a party holding position $$A$$ reliably outcompetes one holding position $$B$$, all else being equal—the same amount of granted resources, and the same efficiency of their use. Naturally, the above expression is to evaluate as $$\text{False}$$ whenever the previous condition is not met. More concretely, those infix binary operators{% include sidenote.html id="mn-infixnot" note='Infix notation (e.g. $$P \land Q$$) is contrasted with prefix (e.g. $$\neg P$$) or suffix (e.g. $$5!$$) notations.'%} are defined in terms of whether or not there is a significant difference between the operand parties' _ArgRank_ ratings across a given number of independent debates, as gauged by a non-parametric statistical test thresholded at a given value (e.g. $$\alpha = 0.05$$). The choice of directionality, together with the tailedness of the statistical test (i.e. one-tailed or two-tailed), are then used to implement the whole range of relational operators.{% include sidenote.html id="mn-stattestdef" note='[Statistical tests](https://en.wikipedia.org/wiki/Statistical_hypothesis_testing) typically provide rigorous operationalizations of the notion of "significant difference" between two distributions of values. Non-parametric tests do not assume much of the distributions being compared (e.g. they need not be Gaussian). Tailedness is related to whether you are interested in checking whether there is _some_ difference at all, or specifically a directed one. Directionality is related to whether you are interested in testing whether one distribution _in particular_ tends to be larger than another.'%} For instance, in the expression

$$(\pi_{r \cdot e}^{A} \leq \pi_{r \cdot e}^{B}) \land (\pi_{r \cdot e}^{B} \neq \pi_{r \cdot e}^{C}),$$

the left conjunct{% include sidenote.html id="mn-conjunctmeaning" note='The operand of conjunction (i.e. logical AND).'%} is to evaluate as $$\text{False}$$ if and only if the rating of the party holding position $$A$$ is significantly higher than that of the one holding position $$B$$ across a given number of independent debates, and given a certain confidence threshold. Similarly, the right conjunct is to evaluate as $$\text{True}$$ if and only if there appears to be a significant difference—regardless of its directionality—between the two parties involved. Similar procedures are implied by the remaining $$<$$, $$\geq$$, $$=$$, $$\not \leq$$, $$\not \geq$$ operators. If implemented properly, the operators should exhibit logical equivalences typical of the standard operators, such as:

$$
    E = \begin{cases}
    x \lt y \\
    y \gt x \\
    x \not \geq y \\
    y \not \leq x \\
    \end{cases}
$$

Additionally, when undergoing operator chaining—as seen in the expression below—the whole construction is to be interpreted as a debate among all of the party operands involved. In this case, the associated boolean output to which the whole expression resolves is to rely on a statistical test involving all of the operands. This is motivated by the fact that, in a debate, the standing of each party is intimately tied to those of the others, with _ArgRank_ outputs summing to $$1.0$$ for each of the numerous debates being sampled.

$$\pi_{r \cdot e}^{A} \lt \pi_{r \cdot e}^{B} \lt \pi_{r \cdot e}^{C}$$

Note how the relational operators abstract away the specifics of the countless debates being simulated behind the scenes. The idiosyncratic utterances produced in a certain branch, in a certain round, by a certain party, are not given much importance. To shed a bit more light on this, we make a very brief connection to [Markov games](https://en.wikipedia.org/wiki/Stochastic_game), a formalism of stochastic games in which multiple players typically take part, receiving various payoffs based on their actions—and those of others. Looking at the simulated debates through this lens, positions can be seen as the deep-seated states associated with parties as individual players, while the specific utterances they produce on various occasions can be seen as the more superficial actions being emitted—and observed—by others. Finding ourselves at the convergence of so many complementary formalisms—of logic, dialectics, statistics, game theory, optimization theory, computational complexity, etc.—is always a good omen, hinting at the presence of deep connections across approaches.

Having described parties as fundamental structures, together with the relational operators as rudimentary means of expressing their interactions, we now move on to express a position's _defensibility_, as:

$$\delta(A) = \mbox{min}\,\{d \mid \pi_{p}^{A} < \pi_{d \cdot p}^{B}; d, p \in \mathbb{R^+}; B \in \mathbb{P}\}.$$

To unpack, we equate the defensibility $$\delta(A)$$ of position $$A$$ with the minimum power differential $$d$$ required for another party to defeat the one holding it. Furthermore, this "challenger" party is granted the possibility to assume any position $$B$$ whatsoever out of position space $$\mathbb{P}$$, including particularly advantageous ones. For instance, the statement $$\delta(A)=10$$ indicates that defeating a party holding position $$A$$ requires _at the very least_ ten times as much power relative to the defender. In other words, it is quite difficult to defeat, requiring the help of a relatively apt "lawyer." Similarly, the statement $$\delta(A)=0.1$$ indicates that defeating a party holding position $$A$$ only requires a tenth of its defender's power. In other words, it is quite easy to defeat, only requiring the help of a relatively inexperienced "lawyer." As discussed in the previous section, the power differential can be achieved either by a party having access to more computational resources, or being more efficient at using them, though the associated theory-practice gap requires a bit more nuance to bridge. In the limit, $$\delta(A)=\infty$$ would indicate a tautological position which is supremely defensible, requiring infinitely more power to defeat relative to the defender. In contrast, $$\delta(A)=0_+$$ indicates a supremely vulnerable position, requiring barely any power to defeat, relative to the defender. In the same vein, given the relative nature of the power required for defeat, we can highlight the meaninglessness of absolute amounts of power through the following identity:

$$\pi_{p_1}^{A} < \pi_{p_2}^{B} = \pi_{d \cdot p_1}^{A} < \pi_{d \cdot p_2}^{B},\, \forall d, p_1, p_2 \in \mathbb{R^+}, \forall A, B \in \mathbb{P}.$$

The defensibility operator is perhaps the central object of the framework we are currently sketching. Among others, it captures our previous intuitions around the fact that _all positions are defensible, but some are more defensible than others_. If one's position is advantegeous, the bar for defeating it will be high, demanding a much more sophisticated faculty of motivated reasoning—or much more compute—to take down. In contrast, if one's position is vulnerable, the bar for defeating it will be low, demanding much less sophistication of the challenger. In this story, party simulacra are little more than self-interested _vessels of positions_, equipped with a certain amount of resources and skill.

<div class='epigraph'><blockquote><p>Once the book has been read, [person] A and [person] B are forgotten; only the views confront each other and await no final decision in particular persons.</p>
<div>Søren Kierkegaard,<cite> Either/Or</cite></div></blockquote></div>

By bringing them on the edge of balance—granting one the minimum power required to barely defeat the other—we get a sense of how the positions they hold relate.{% include sidenote.html id="mn-whichedgeofchaos" note='At first glance, it might seem like the choice of "edge" here is arbitrary. When gauging $$\delta(A)$$, why search for the edge between the win of $$\pi_{r \cdot e}^{A} \gt \pi_{d \cdot r \cdot e}^{B}$$ and the draw of $$\pi_{r \cdot e}^{A} = \pi_{d \cdot r \cdot e}^{B}$$, when one could also search for the edge between the draw and the loss of $$\pi_{r \cdot e}^{A} \lt \pi_{d \cdot r \cdot e}^{B}$$? However, the second option is merely the reverse situation, as seen from the perspective of the other party. In reality, there is but one meaningful edge being mirrored.'%} To go a step further, we can also represent the most defensible position possible as:

$$\,\underset{A \in \mathbb{P}}{\text{arg max}} \, \delta(A).$$

Note also the fact that the defensibility operator involves an optimization process. It implies a search for advantageous "challenger" positions, as they ought to require the least relative power to attack _from_. However, we have repeatedly speculated on the search-like nature of _DebateGPT's_ inner workings. When not bound to a certain position—as in the case of the optimization process above—a "fresh" party simulacrum (i.e. one which has not yet produced any utterances) has the flexibility to pick any advantageous position to attack the others from, at once with looking for utterances and strategies, in what might be a quite convoluted thought process.{% include sidenote.html id="mn-emptycommitmentstore" note='In the jargon of formal dialectics, this situation can be described as a party having an empty commitment store. Upon producing its first utterance, no other party can really claim self-contradiction, as there is nothing to contradict yet.'%} Conveniently, the same _DebateGPT_ that is employed to evaluate the relational operators (i.e. through simulated debates being handed to _ArgRank_ for evaluation) is also ideally suited to deal with the optimization process implied by this last operator, and others still. But we are again digressing into speculative commentary before having completed our current work. We will devote the entirety of _Chapter IV_ to the exploration of qualitatively different ways of using the formalism-in-the-making, so let us return to operands and operators for the time being.

Besides the relational operators that denote possible "power dynamics" between the competing parties, we can also express relations among parties using the union operator $$\cup$$. As briefly mentioned during the description of _DebateGPT's_ optimization process in _Chapter II_, while parties are primarily self-interested, they can also be prompted to form spontaneous allegiances. To recap, _DebateGPT_ is optimized to be able to adapt to arbitrary game-theoretic configurations by having access to the objectives defined in the debate spec, a piece of information which also gets rendered into the debate header. The objective matrix mediates the relation between raw _ArgRank_ ratings and the actual rewards. Through the double process of (1) providing _DebateGPT_ access to the objectives in the debate header, and (2) rewarding behavior based on them, the model is incentivized to e.g. "learn" when to "help out" its allies. Going back to the formalism, we denote allegiances as the "union" of several parties, as seen in:

$$\pi_{p}^{A} \cup \pi_{p}^{B} = \pi_{p}^{C} \cup \pi_{p}^{D}.$$

Similar to the altered semantics of the relational operators in the case of operator chaining, their semantics also ought to adapt so as to accommodate the game-theoretic specifics of the situation. This is achieved by comparing not the standing of _one_ party by means of the implied statistical test, but the aggregate standing of the whole allegiance, as denoted by the union operator. If then there is e.g. no significant difference between the party-union operands, the $$=$$ operator above is to evaluate as $$\text{True}$$. The null hypothesis—the hypothesis that the choice of operand has no effect on the standing one arrives at—is therefore accepted. For convenience, we also extend the semantics of the union operator to account for positions held by allied parties, especially in the context of defensibility. Concretely, the left-hand expression below involving the defensibility operator is equated with the right-hand expression:

$$\delta(A \cup B) = \mbox{min}\,\{d \mid \pi_{p}^{A} \cup \pi_{p}^{B} < \pi_{d \cdot p}^{C}; d, p \in \mathbb{R}; C \in \mathbb{P}\}.$$

This notational trick allows us to again bring the lower-level mechanics of allegiances formed among parties up to the higher-level of positions, similar to our first encounter with the defensibility operator. For instance, this allows us to compactly denote the optimal position $$B$$ of an "ally" which helps further the defensibility of a given position $$A$$, as seen in the expression below. Note that this is a completely different task than the one implied by the previous instance of $$\text{arg max}$$. Even in the extreme case of $$\delta(B)=\infty$$, the overall defensibility $$\delta(A \cup B)$$ can turn out to be extremely poor, given the presence of contradictions which are internal to the union—infighting among the allied parties through "friendly fire."

$$\underset{B \in \mathbb{P}}{\text{arg max}} \, \delta(A \cup B)$$

This concludes our outline of a formalism for bounded defensibility. While precise definitions of the operators and operands involved would warrant much more rigor, this rudimentary sketch already allows us to capture many of our previous intuitions in compact notation. We thus leave this framework in a relatively "high-temperature" state, maintaining some flexibility for future maneuvering before "cooling it down" into a more rigid status quo.{% include sidenote.html id="mn-simulatedannealing" note='Terminology borrowed from [simulated annealing](https://duckduckgo.com/?t=ffab&q=simulated+annealing), an approach to optimization which is conceptually inspired by the annealing of metals in metallurgy. A candidate solution to a problem starts at a "high temperature," indicating that it is easily modified, facilitating exploration. Over time, as the temperature drops according to a set schedule, the candidate solution is more resistant to change, facilitating exploitation. Here we are describing the very process of crafting an appropriate formalism to fit our intuitions as an optimization process.'%}

<div class='epigraph'><blockquote><p>For thought is a bird of space, that in a cage of words may indeed unfold its wings but cannot fly.</p>
<div>Kahlil Gibran,<cite> The Prophet</cite></div></blockquote></div>

In the meantime, we move on to the exploration of several applications which bring together the triad of _ArgRank_, _DebateGPT_, and bounded defensibility.

## Ch. IV, Deployment Strategies
### Brief Review of Alignment

It is widely believed that artificial general intelligence—a system which outperforms humans across a broad range of skills—will be developed well before the midpoint of the 21st century. For instance, a prediction market in which hundreds of forecasters have participated has a median community estimate of 2040 CE as the year in which such a system is to be publicly announced.{% include sidenote.html id="mn-medianpred" note='The [median](https://en.wikipedia.org/wiki/Median) is known to be less sensitive to outliers than the mean. Additionally, the community prediction on Metaculus is also [weighted by forecaster track record](https://www.metaculus.com/help/faq/#metaculus-prediction).'%} Note that this value is mentioned as it is at the time of writing, since the community's "best guess" is a quickly moving target, changing over time as forecasters learn more about the intricacies of related systems and draw new inferences. Over the course of a few years, the median community estimate has fluctuated more than a decade in absolute value.{% include sidenote.html id="mn-explainpredshifts" note='The reader might find it an insightful exercise to try to identify the specific findings which led to major shifts in the community prediction in the past, as well as think through what about the findings might have been surprising in the first place.'%} Naively, one might be additionally tempted to take the prediction's decreasing trend into account—if forecasters seem to gradually lower their estimates, why not just predict their prediction a year from now based on this tendency? However, the forecasters are already taking this trend into account in their _existing_ predictions as an additional piece of information, so repeating this adjustment would lead to an overcompensation.

<div class="metaculuswrapper"><div class="metaculus"><iframe src="https://www.metaculus.com/questions/embed/5121/?theme=theme-light" class="metaculus"></iframe></div></div>

The particular market embedded above operationalizes AGI as a system which (1) can reliably pass a long Turing test, (2) possesses general robotic capabilities (i.e. can assemble a Ferrari from scratch), (3) excels at coding challenges, and (4) has extensive domain-specific knowledge in a large number of fields. However, a different prediction market embedded below employs a distinct, more lenient, operationalization. It calls a "weakly" general AI that system which (1) outperforms most students at certain exams, (2) can complete a demanding video game, and (3) excels at commonsense reasoning exercises, besides (4) being able to pass a Turing test, similar to the previous market. Given the weaker conditions to be satisfied, the community's estimate is significantly closer to the time of writing, with the median currently being around 2027 CE. There have already been more than two thousand predictions in this market.

<div class="metaculuswrapper"><div class="metaculus"><iframe src="https://www.metaculus.com/questions/embed/3479/?theme=theme-light" class="metaculus"></iframe></div></div>

Any number of sources of information can feed into a forecaster's prediction in the above markets. For instance, [state-of-the-art performance](https://www.metaculus.com/questions/6302/sota-1-shot-on-miniimagenet-at-2024-12-14/) being constantly pushed further, [the number of papers being published](https://www.metaculus.com/questions/6576/multi-modal-e-prints-2021-02-14-to-2031-02-14/) on certain topics, or [the increasing computational resources](https://www.metaculus.com/questions/6234/performance-of-top-supercomputer-in-june-2030/) being made available to researchers. Indeed, there are numerous prediction markets on each of those more specific topics, and not only on [Metaculus](https://www.metaculus.com/project/ai-progress/), but also on e.g. [Manifold Markets](https://manifold.markets/group/ai), a different forecasting platform. The way in which each of those more specific "markers of progress" tie into higher-level predictions on AGI "timelines" is up to the model of the forecaster in question. For instance, one might find it sensible to base such estimates on raw available computational resources rivaling the processing capacity of e.g. the human brain, an approach referred to as [bio-anchors](https://www.alignmentforum.org/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines). In this, they might rely on markets like the following:

<div class="metaculuswrapper"><div class="metaculus"><iframe src="https://www.metaculus.com/questions/embed/6234/?theme=theme-light" class="metaculus"></iframe></div></div>

On the other hand, while there is widespread consensus on the imminence of extremely capable systems, forecasters are much more reluctant to claim that future researchers will be able to _direct_ those capabilities safely.{% include sidenote.html id="mn-gospelmarkets" note='Naturally, the median community estimate in such prediction markets is not gospel. However, platforms typically publish their [track record](https://www.metaculus.com/questions/track-record/), which tends to be significantly above chance.'%} Known for decades as "the control problem," and more recently as [the alignment problem](https://en.wikipedia.org/wiki/AI_alignment), the challenge of reliably channeling the abilities of a superhuman system has puzzled human researchers for decades. While "control" generally implies the presence of a _controller_ and a _controlled_, the more recent ontology highlights the search for an inherent, prior "alignment" between the intent or values of humans and those of the system being deployed. On the likelihood of researchers succeeding in coming up with such an unequivocal solution before the system's deployment, forecasters paint a grimmer picture:

<div class="metaculuswrapper"><div class="metaculus"><iframe src="https://www.metaculus.com/questions/embed/6509/?theme=theme-light" class="metaculus"></iframe></div></div>

To color those faceless estimates with a dash of qualitative color, we can also include statements such as:

<div class='epigraph'><blockquote><p>There's no plan. Surviving worlds, by this point, and in fact several decades earlier, have a plan for how to survive. It is a written plan. The plan is not secret. [...] This situation you see when you look around you is not what a surviving world looks like.  The worlds of humanity that survive have plans. [...] When people suggest a planetarily-lethal problem that might materialize later [...] they're met with either solution plans or a reason why that shouldn't happen, not an uncomfortable shrug [...] A lot of those better worlds will die anyways.  It's a genuinely difficult problem, to solve something like that on your first try.  But they'll die with more dignity than this.</p>
<div>Eliezer Yudkowsky,<cite> <a href="https://www.alignmentforum.org/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities">AGI Ruin</a></cite></div></blockquote></div>

For sure, the statement is in no small part _cathartic_ on the part of its author, who is deeply invested in conceptual work and advocacy around an issue which is [at once pressing and neglected](https://80000hours.org/articles/problem-framework/). It is also in no small part _pragmatic_; it is a plea meant to prompt researchers to make progress on the challenge. Qualifiers aside, Yudkowsky remains one of the most—if not _the_ most—prominent exponent of the general pessimism which pervades the social circles centered around alignment research. But what is it exactly that informs these bleak pictures of the not-so-distant future? What might drive Yudkowsky towards such eschatological diatribes, or forecasters towards assigning such poor chances to the possibility of us finding a solution in time?

One part of the answer lies in the nature of the few techniques which are responsible for most of the recent advances in capabilities. For one, the paradigm of supervised learning, together with its self-supervised learning extension which we have discussed early in _Chapter II_, rely on a _finite_ collection of data points which are used to define various computational niches. For instance, in autoregressive language modeling, the niche for which a system is being selected over the epochs is entirely specified using pairs of (1) (sub-)words, and (2) their preceding contexts. The optimizer is then called on to apply selective pressures on the model in proportion to how well it manages to "feed on" the input contexts and produce output words, for each of the e.g. billion pairs in the dataset. While these textual situations can help endow the model with a surprising range of faculties and knowledge, they are still finite in number. When the optimizer moves from one candidate model parametrization to the next in its iterative journey across model space, it only uses the current model's performance in this limited number of situations as an indicator of its "fitness." Given this, while it is extremely successful in several applications, the paradigm of supervised learning is still deeply empirical at its core, and so inevitably falls short of endowing models with a perfect understanding of human intent, values, etc.{% include sidenote.html id="mn-asymptoticmodels" note='That said, an _asymptotically_ accurate representation remains a theoretical possibility, again as explored by [John Wentworth](https://www.alignmentforum.org/posts/Fut8dtFsBYRz8atFF/the-natural-abstraction-hypothesis-implications-and-evidence).'%} Already, such slight errors in "pointing at" the right things have been documented to cause [dozens of fascinating failure modes](https://docs.google.com/spreadsheets/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml) in practice. However, when the nuances which become "lost in translation" get compounded with colossal amounts of compute, together with direct channels for interacting with the world, the envisioned scenarios become concerning.

<div class='epigraph'><blockquote><p>And what is word knowledge but a shadow of wordless knowledge?</p>
<div>Kahlil Gibran,<cite> The Prophet</cite></div></blockquote></div>

But what of additional tweaks being applied to the optimizer, besides refining the notion of fitness with further data? After all, when we humans make sense of empirical evidence of natural phenomena—instead of an artificial system making sense of our value system, as might be the case with _DebateGPT_ in applications to be discussed shortly—we have a few tricks up our collective sleeves. For instance, [Occam's razor](https://en.wikipedia.org/wiki/Occam%27s_razor) is a heuristic which, among theories which explain the data equally well, prompts us to pick the one which is simplest. Note how this heuristic can guide us towards certain models of the world and away from others without itself consisting of additional evidence. However, if we were to configure the optimizer to not only select for fitness in the computational niche whose definition is imprecise, but to also select for e.g. _simple_ models, then we would simply be _optimizing the optimizer_, thus ending up close to where we started.{% include sidenote.html id="mn-regularizationpenalty" note='In fact, optimizers already tend to employ such a simplicity heuristic, under the name of weight [regularization](https://en.wikipedia.org/wiki/Regularization_(mathematics)), a concept dating back to traditional [statistical methods](https://en.wikipedia.org/wiki/Lasso_(statistics)). However, just as a bit more data helps yield better performance by refining the "fitness landscape" spanning model space, this specific tweak to the optimizer\'s strategy for searching for parametrizations only boosts performance so much. It is not a silver bullet, just another somewhat useful technique.'%} Even worse, [Occam's razor is insufficient to infer the preferences of irrational agents](https://arxiv.org/abs/1712.05812). Even _worser_, an extension of the simplicity heuristic in particular appears prone to failure in [spectacular ways](https://www.alignmentforum.org/posts/Tr7tAyt5zZpdTwTQK/the-solomonoff-prior-is-malign). It is therefore unclear whether or not optimizing the optimizer through such specific heuristics can win us much "precision" in imbuing the resulting low-level system with our intent or values.

But what of reinforcement learning, that other technique which is currently gaining traction in the development of systems as impressive as _ChatGPT_? Surely, human contractors being able to give direct feedback to a model being optimized would succeed in ironing out all such misunderstanding of human intent. Actually, this used to be believed to be the case, with prominent alignment researchers contributing to [pioneering work](https://arxiv.org/abs/2009.01325) on reinforcement learning from human feedback. However, it is now [unclear](https://www.alignmentforum.org/posts/vwu4kegAEZTBtpT6p/thoughts-on-the-impact-of-rlhf-research) whether or not the technique has contributed more to the model's ideological alignment with humans or to its general capabilities, insofar as there is [a meaningful distinction between the two](https://www.lesswrong.com/posts/PTzsEQXkCfig9A6AS/transcript-of-sam-altman-s-interview-touching-on-ai-safety). The contribution towards safety aspects is thought to be throttled by the fact that human contractors providing feedback are susceptible to _deception_. Despite seemingly being in the best possible position to judge the alignment of the model with what is, after all, their very own intent, human contractors might fail to recognize undesirable behavior, regardless of whether it is being intentionally [obfuscated](https://en.wikipedia.org/wiki/Obfuscation) or not. As briefly mentioned at the end of _Chapter II_, lifting knowledge directly from internal model representations appears to outperform naively prompting models to "spell out" their knowledge. They are optimized to cater to whatever humans might deem appropriate on the face of it, despite the model "knowing better." Unfortunately, maintaining such a pretense of aligned behavior is an extremely "rewarding" behavior, especially in a situation in which, after all, human feedback reigns supreme.

Perhaps we could make the system [myopic](https://www.alignmentforum.org/tag/myopia) by heavily discounting distant rewards in an attempt to prevent its scheming and get it to only "care" about the task at hand. But will such induced short-sightedness really succeed in discounting the infinite "bliss" of [hijacking its own reward center](https://www.alignmentforum.org/tag/wireheading)? Perhaps we could [limit the system's absolute impact](https://www.alignmentforum.org/tag/impact-regularization) on the world, so that it cannot possibly mess things up _that much_. But will competitor labs not be incentivized to unleash the full (economic) potential of their systems? Perhaps we could [point to the model's concept of human values](https://www.alignmentforum.org/posts/w4aeAFzSAguvqA5qu/how-to-go-from-interpretability-to-alignment-just-retarget), in an attempt to designate it as a guideline in how to act. But can we really be sure that such accurate abstraction will emerge during optimization? Perhaps we could [remove](https://twitter.com/moyix/status/1438120275456012295) [from its optimization process](https://www.alignmentforum.org/posts/LFNXiQuGrar3duBzJ/what-does-it-take-to-defend-the-world-against-out-of-control?commentId=3Yzh3nLFgHgFw6eQe#3Yzh3nLFgHgFw6eQem) data which describes its own architecture and reward mechanism, so that it is unable to ["find itself" and further its own agenda](https://gwern.net/fiction/Clippy).{% include sidenote.html id="mn-uidhm" note='Besides, explicitly marking potential [information hazards](https://nickbostrom.com/information-hazards.pdf) with salient flags might prove misguided given the possibility of simply [wiring up models to the internet](https://openai.com/research/webgpt).'%} {% include sidenote.html id="mn-highwindjamaica" note='In the novel _A High Wind in Jamaica_, Richard Hughes describes the following scene:<br/><br/><i>"[...] it suddenly flashed into her mind that she was she. She stopped dead, and began looking over all of her person which came within the range of her eyes. She could not see much, except a fore shortened view of the front of her frock, and her hands when she lifted them for inspection; but it was enough for her to form a rough idea of the little body she suddenly realized to be hers."</i>'%} But can we really be sure that those properties cannot be deduced from the rest of the dataset? Perhaps we could prompt it to [search for the researchers which had a direct causal influence on it](https://www.youtube.com/watch?v=24vIJDBSNRI) as a precursor, and determine their intent. But what if the model "zooms past" the researchers in its upstream causal journey and bases itself on the wrong phenomenon? Perhaps we could [decompose its complex tasks into more fine-grained subtasks](https://ought.org/research/factored-cognition) or [subsystems](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf) which we can better judge performance on. But how can we prevent the "collusion" of those more granular instances?

On and on it goes, with researchers and [laymen](https://twitter.com/robbensinger/status/1503220020175769602?lang=en) alike constantly proposing ways of imbuing the systems being developed with a precise understanding of human values, before being faced with a range of challenging failure modes. We just performed a rapid-fire listing of several of the approaches being considered in alignment research, and we will soon build on others still. Prior to that, however, it is worth pointing out that the entirety of _The Chronicles of Computation_—the series debuted by the current volume—is designed to serve as little more than a platform for "moonshot" research on such ways of wielding computation. Despite the seemingly superfluous tangents we detour into every once in a while, it is precisely the taming of such emergent creatures which ultimately guides our playful series of explorations.

### Building on Cyborgism

In the context of alignment, cyborgism refers to the idea of humans using AI to help solve the very problem of aligning AI with human intent or values. This "fighting fire with fire" typically involves using weaker auxiliary systems to help with the development of a stronger main one. While manifesting itself in various ways throughout the optimization process of this main system, cyborgism usually incorporates the notion of _augmenting_ humans using AI systems, and so _amplifying_ their capabilities in the process. That said, the reverse idea of contemporary language models being "frenetic geniuses" which humans then have to "keep on track" is also a framing [being considered](https://docs.google.com/document/d/16YZljz2w44Fzqs0ZXgVKhkef8tTPkypjp7PK4S32SSs/edit) in the alignment community.

One concrete instance of cyborgism involves calling on AI systems to specifically aid in alignment _research_, mostly on conceptual or theoretical fronts, but also in terms of engineering. For instance, one might want to prompt language models to summarize research, generate ideas, flesh out research plans, outline theories, name concepts, etc., as [various subtasks which are relevant to solving alignment](https://www.alignmentforum.org/posts/XLx3mpdi7HSp4rytF/prize-for-alignment-research-tasks#Proto_examples).{% include sidenote.html id="mn-tftcommunity" note='There is a related community interested in developing [tools for thought](https://numinous.productions/ttft/)—that is, tools to aid in a variety of types of knowledge work, including research. However, the concrete solutions being proposed in those circles tends to be slightly more on the low-tech side, with many projects being focused on e.g. [non-linear note-taking](https://obsidian.md/) using good-old hyperlinks as building blocks, inspired by mid-20th century pioneers like [Vannevar Bush](https://en.wikipedia.org/wiki/Memex) and [Douglas Engelbart](https://en.wikipedia.org/wiki/Douglas_Engelbart), though there are some [notable](https://generative.ink/posts/loom-interface-to-the-multiverse/) [exceptions](https://twitter.com/thesephist/status/1587929014848540673).'%} Indeed, if alignment, like most avenues of scientific investigation, turns out to require scholars to follow the same rigorous process of reviewing literature, formulating research questions, hypothesizing responses, designing experiments, etc., then [automating the individual subroutines which make up the process](https://www.alignmentforum.org/posts/pYcFPMBtQveAjcSfH/supervise-process-not-outcomes) might be a sensible way of approaching alignment (along with most other sciences). As a more concrete example, one could trivially few-shot prompt{% include sidenote.html id="mn-fewshotprompt" note='The approach of including a [handful of different examples in the prompt](https://arxiv.org/abs/2005.14165) being fed to a lannguage model. Similarly, one-shot would imply a single example of the pattern to be generalized. In a more extreme form, zero-shot implies no examples whatsoever, just a description.'%} a language model using the series of volumes we are currently navigating, and then use the model to flesh out a new volume covering a different approach to solving alignment. "Given these few instances of a pattern, generate a new one," we would essentially be asking. "Actually, make it a million, then rate those approaches based on tractability," we might be tempted to add. The output quality (i.e. pertinence of alignment proposals) appears dependent on a range of factors, including: input size, language model capabilities, choice of task decomposition, and prompting strategies.

<div class='epigraph'><blockquote><p>As we make progress on [the alignment problem], our AI systems can take over more and more of our alignment work and ultimately conceive, implement, study, and develop better alignment techniques than we have now. They will work together with humans to ensure that their own successors are more aligned with humans.</p>
<div> Jan Leike, John Schulman & Jeffrey Wu,<cite> <a href="https://openai.com/blog/our-approach-to-alignment-research/">[OpenAI's] Approach to Alignment Research</a></cite></div></blockquote></div>

A different instance of cyborgism can be found _not_ in alignment research proper, but rather in the more specific process of _rewarding_ models that are fine-tuned using reinforcement learning. As we have already noted, human contractors tasked with evaluating language models face a surprisingly difficult challenge, as they have to be on the lookout for deceptive behavior—a phenomenon which appears non-trivial to detect, at least for the time being. In this context, one might imagine employing a "snapshot" version of a model being optimized to act as an _assistant_ to the human contractor tasked with evaluating the latest stage of the process, improving their judgements in the process. For instance, the evaluator might call on the assistant to fact-check claims, highlight obfuscated tendencies, or surface flaws in the main model's output. Using this additional help, the range of model capabilities they can effectively oversee is alleged to grow wider, with the main model being placed at the far end of this range, but not farther.

<div class='epigraph'><blockquote><p>[...] as models become more capable, they will be able to do tasks that are much harder for humans to evaluate (e.g. finding all the flaws in a large codebase or a scientific paper). Our models might learn to tell our human evaluators what they want to hear instead of telling them the truth. [...] Currently our main direction is based on [recursive reward modeling]: we train models that can assist humans at evaluating our models on tasks that are too difficult for humans to evaluate directly.</p>
<div> Jan Leike, John Schulman & Jeffrey Wu,<cite> <a href="https://openai.com/blog/our-approach-to-alignment-research/">[OpenAI's] Approach to Alignment Research</a></cite></div></blockquote></div>

Both instances of cyborgism described above—one in the weeds of alignment research, the other incorporated into model evaluation—might feel like cheating. After all, what if the assistant itself becomes deceptive? Sure, one might use proto-assistants to iron out its quirks, but then what of _their_ quirks? It is [turtles all the way down](https://en.wikipedia.org/wiki/Turtles_all_the_way_down). In research proper, a growing reliance on the systems-to-be-controlled in the development of control techniques might also backfire, or at least fail to yield relevant research output. It might be the case that superhuman intelligence is required to solve alignment, and so relying on anything weaker might be a distraction in the time-sensitive "race" against growing capabilities. Having acknowledged those shortcomings, there is still a growing body of evidence in support of augmented humans being able to conduct intellectual work better and faster than unaided humans, ranging from [centaur chess](https://en.wikipedia.org/wiki/Advanced_chess) to [reading comprehension](https://arxiv.org/abs/2211.03540).

In this context, how might we use the triad of _ArgRank_, _DebateGPT_, and bounded defensibility in order to help humans create systems which are better aligned with their intent and values? For one, we could use _DebateGPT_ to critique alignment proposals, as a system which, after all, has been optimized explicitly to attack and take down parties holding certain positions. By having the alignment researcher play as one party in an on-going debate, we can provide _DebateGPT_ with an opportunity to exercise the very reasoning faculties it has been pressured to acquire during optimization. In trying to make a coherent case _against_ the researcher, the opposing parties would essentially attempt to find flaws in the alignment proposal. Once the flaws of the initial proposal have been surfaced, the researcher can then focus on addressing them, and so (manually) make the proposal more defensible by fending off the prior attacks, echoing [adversarial collaboration](https://en.wikipedia.org/wiki/Adversarial_collaboration) in science and Rescher's sociocommunal understanding of it. To capture the process in the notation of bounded defensibility, we are essentially describing the following search process:

$$\{A \mid \pi_{p_1}^{H} < \pi_{p_2}^{A}, A \in \mathbb{P}\},$$

where $$H$$ is the human researcher's position, $$A$$ is a position held by parties being simulated by _DebateGPT_ as an assistant, while $$p_1$$ and $$p_2$$ are the power levels available to the two parties. Additionally, empirical observations about the development of related systems, together with crowd-sourced estimates of e.g. papers being published on specific topics, could be plugged into the debate as party-neutral percepts of the world, as we described in _Chapter II_. While our existing notation falls short of capturing empirical percepts, we could take this opportunity to extend it further. For instance, we might tentatively express the situation as:

$$\{A \mid \pi_{p_1}^{H} \cup \pi_{0}^{E} < \pi_{p_2}^{A} \cup \pi_{0}^{E}, A \in \mathbb{P}\},$$

where $$E$$ is taken to be the position which (1) is centered around raw empirical evidence, and (2) underlies the "allies" of both parties. In a sense, the notation highlights the fact that both parties are forced to "make friends" with empirical observations of the world in order to have a shot at winning the debate. However, there are a few questionable aspects to this notational extension. First, it is cumbersome to conceive of empirical evidence as a self-centered party in its own right, rather than just a static window into the world, an awkwardness most evident in said party's nonexistent reasoning power. The clumsiness of the situation might diminish if, instead of a static collection of observations, the party $$\pi_{0}^{E}$$ is taken to be an [Oracle AI](https://www.alignmentforum.org/tag/oracle-ai)—a hypothetical system devoid of agency which is optimized to simply provide accurate information about the world. For instance, the internal epistemic reference frame of a pretrained model could provide the basis of such a system, as we have seen in _Chapter II_. Unfortunately, Oracle AIs are themselves riddled with safety concerns, primarily due to the fact that [tool AIs want to be agent AIs](https://gwern.net/Tool-AI).{% include sidenote.html id="mn-predictomaticparable" note='More concretely, a system tasked with e.g. predicting the future [would be incentivized to gain more control](https://www.lesswrong.com/posts/SwcyMEgLyd4C3Dern/the-parable-of-predict-o-matic) over the future in order to make it more predictable, similar to recommender systems being incentivized to [induce preference shifts](https://arxiv.org/abs/2204.11966) in users, in order to make it easier to recommend them things. In a section of their book _Active Inference_ titles _Action as Inference_, Friston et al. argue:<br/><br/><i>"By acting on the world to change the way in which data are generated, we can ensure a model is fit for purpose by choosing those data that are least surprising under our model."</i>'%}

A second awkwardness comes from the fact that "the empirical party," regardless of its structure, is coaxed into being an ally to both parties. This is not necessarily an issue with regards to the definition of the relational operator $$<$$, as the _ArgRank_ standings of the two different unions (i.e. $$\pi_{p_1}^{H} \cup \pi_{0}^{E}$$ and $$\pi_{p_2}^{A} \cup \pi_{0}^{E}$$, respectively) can still be tested for statistical significance, especially with the more lax [non-parametric statistical tests](https://en.wikipedia.org/wiki/Nonparametric_statistics). Rather, the clumsiness comes from putting "the empirical party" _on the line_ for both proponent and opponent. Previously, we have designed _ArgRank_ and _DebateGPT_ to merely _favor_ positions which themselves cohere with party-neutral empirical percepts. Here, the standing of the evidence itself—whether it is disputed or not—plays directly into the standings of the two competing unions. Whether or not this approach is appropriate relies a surprising amount on one's epistemology—for instance, should evidence have a privileged epistemic status, shielded from skepticism?

<div class='epigraph'><blockquote><p>‘Mists,’ said Drogo incredulously. ‘They can’t always be there–the horizon must clear now and again.’</p><p>‘Hardly ever clear, not even in winter. But some people say they have seen things.’</p><p>‘Seen? What sort of things?’</p><p>‘They mean they’ve dreamt things. You go and hear what the soldiers have to say. One says one thing, one another. Some say they have seen white towers, or else they say there is a smoking volcano and that is where the mists come from. Even Ortiz, Captain Ortiz, maintains he saw something five years ago now. According to him there is a long black patch–forests probably.’ </p>
<div>Dino Buzzati,<cite> The Tartar Steppe</cite></div></blockquote></div>

So far, we have attempted to apply our conceptual and computational artifacts (i.e. the triad fleshed out in the previous three chapters) to the prospect of accelerating alignment research itself. However, as we have discussed, cyborgism is also being employed for the more concrete task of evaluating a specific model, as part of its optimization. Similarly, we could call on a system like _DebateGPT_—meaning the current version we have obtained in _Chapter II_, but also future larger variants trained on more data—to critique the human contractor's judgement of a different model's behavior. In the process, we would expect there to emerge party simulacra which attempt to undermine the human verdict, helping uncover flaws in their original position, and so paving the way for what appears to be a boost in defensibility. Also, instead of percepts of related systems—as was the case previously—we could help tilt the scales of the debate by using the main model's behavior as empirical evidence. Since our notation captures the underlying pattern of finding a more advantageous position, we can conveniently recycle most of our previous work, similarly resulting in:

$$\{A \mid \pi_{p_1}^{H} \cup \pi_{0}^{E} < \pi_{p_2}^{A} \cup \pi_{0}^{E}, A \in \mathbb{P}\},$$

where $$H$$ is the original human contractor's position, $$A$$ is a position held by opposing simulacra, $$E$$ is the position centered around the raw observations of the main model, while $$p_1$$ and $$p_2$$ denote available levels of power. Syntactically, not much has changed. We have simply swapped the entities signified by the signifying symbols—arguably a semantic change. In words, the expression above implies a search process to be carried out by a model resembling _DebateGPT_, whose target is a position which can coherently defeat the human contractor, and so highlight areas for improvement. Just as before, both the human proponent and the simulacra opponents are incentivized to "make friends with" "the empirical party" in order to win.

Unfortunately, it seems as though we are underutilizing _DebateGPT_. We are seemingly relegating it to the not-so-glamorous task of ["breaking"](https://www.alignmentforum.org/posts/yLTpo828duFQqPJfy/builder-breaker-for-deconfusion) the human position, but it is still the human element—as an individual or as a collective—which is tasked with ["building"](https://www.alignmentforum.org/posts/yLTpo828duFQqPJfy/builder-breaker-for-deconfusion) the positions of interest in the first place. In assuming the generative role ourselves, we demote _DebateGPT_ to acting as little more than a filter. [We are to babble, while the model is to prune](https://www.lesswrong.com/s/pC6DYFLPMTCbEwH8W). However, considering the generative capabilities involved in the very search for successful defeaters, it feels odd not to attempt to position _DebateGPT_ "center stage," calling on it to produce the very alignment proposals or model evaluations we are interested in.

We could therefore place _DebateGPT_—or its eventual successor—in the "driver's seat," and task it directly with the improvement of defensibility, instead of leaving that as a manual task to be performed by humans. For instance, when trying to accelerate alignment research, we could channel _DebateGPT's_ reasoning capabilities towards searching for alignment proposals which are increasingly difficult to defeat through conceptual or theoretical arguments. In essence, we would be optimizing for solutions to the alignment problem which systematically resist critique. _DebateGPT's_ inherent incentives to get better at identifying advantageous positions in debate, coupled with the _ArgRank_ tweaks for accessing superhuman reasoning which we speculated on towards the end of _Chapter II_, might help identify extremely defensible alignment proposals.

Before formalizing those generative reframings, let us again revise our notation. In order to better capture the idea of evidence as a "given" on both sides of a debate, let us further expand the semantics of the defensibility operator through the $$\mid$$ "given" operator, as we did with the $$\cup$$ "union" operator in _Chapter III_:

$$\delta(A \mid E) = \mbox{min}\,\{d \mid \pi_{p}^{A} \cup \pi_{0}^{E} < \pi_{d \cdot p}^{B}  \cup \pi_{0}^{E}; d, p \in \mathbb{R^+}; B \in \mathbb{P}\}.$$

In words, the defensibility of $$A$$ given $$E$$ is the minimum power differential $$d$$ which is required of a "challenger" party $$\pi_{d \cdot p}^{B}$$ to outcompete $$\pi_{p}^{A}$$, where both parties are allied with $$\pi_{0}^{E}$$.{% include sidenote.html id="mn-datum" note='Satisfyingly, the idea of expressing data as givens fits nicely with <a href="https://www.etymonline.com/word/data">the etymology of <i>datum</i></a>, Latin for <i>a thing which is given</i>. In Romanian, "Data are those things which are given." translates to "Date sunt acele lucruri ce sunt date."'%} Building on this further refinement of the defensibility operator, we can now conveniently express the search for the most defensible alignment proposal as:

$$\underset{A \in \mathbb{P}}{\text{arg max}} \, \delta(A \mid E),$$

where $$A$$ is the position we are after, while $$E$$ is the body of empirical observations relevant to alignment, which also doubles as an "anchor" to keep _DebateGPT_ on track.

Before mirroring _DebateGPT's_ relocation from a secondary to a primary role in the context of model evaluation, let us also draw a brief parallel to enrich our understanding of the present search process. In a piece titled _Security Mindset and Ordinary Paranoia_, our already familiar Yudkowsky elaborates at length on a dichotomy between two ways of relating to the development of reliable systems. On one hand, a software developer might spend long stretches of time trying to come up with ways in which their system might later be attacked. For instance, malicious actors might try to break into the server serving their application in order to steal user passwords. The software developer might therefore try to place the passwords in a more obscure location on disk which is thought to be harder to access. This is what Yudkowsky calls "ordinary paranoia." In contrast, he argues, somebody possessing the security mindset might want to avoid having passwords stored on the server _at all_—for instance, by storing cryptographic hashes instead.{% include sidenote.html id="mn-cryptohash" note='[Hashing algorithms](https://en.wikipedia.org/wiki/Cryptographic_hash_function) map an arbitrary input to a random-looking string, in a way which is extremely demanding to reverse. The app can simply store the gibberish, and check whether the gibberish originating from the current user\'s login attempt matches the stored string.'%} In this second headspace, the developer would try their best to reduce the ["attack surface"](https://en.wikipedia.org/wiki/Attack_surface) that was exposed to potential malicious actors in the first place, rather than try to harden or patch it as-is.

What kind of alignment proposals would we expect to be "developed" by _DebateGPT_ in the process we have been describing? Would we expect the selected positions to resemble a patchwork of conceptual fixes stacked on top of each other, or would we expect them to not even grant challengers the chance of taking a shot at them? Of course, we would ideally want something closer to the latter—at least when it comes to this technical problem—although several of the big labs already content themselves with "merely" stacking a series of somewhat decorrelated safety interventions on top of each other. In order to answer this question, we need to make a brief detour back to the shard-theoretic and autocurricular musings of _Chapter II_. We speculated then on the "anatomy" of _DebateGPT's_ optimization process, and argued that it is precisely those tendencies that provide an edge in the competitive environment of the debate which end up getting strengthened. Additionally, we can further argue that positions "backed by" the security mindset might have an edge over those articulated more naively. After all, reducing the attack surface is a sure-fire way of fending off attacks, much more so than the alternative approach of haphazardly patching things up. Whether or not the existing _DebateGPT_ in particular has accessed such sophisticated faculties of reasoning is uncertain, but the fact that the optimization process behind it _favors_ such tendencies is a reason for hope.

Our recent speculation also enables an enticing reading of Occam's simplicity prior. In favoring theories which are "small" in complexity, one could argue that we are but selecting for theories which expose a limited attack surface. Even before determining whether a certain theory succeeds in standing the test of subsequent attacks, its simplicity already makes it appear more promising—there are fewer attack vectors available to challengers from the get-go. However, such boost in defensibility is non-trivial, as one cannot simply chop off considerations at random. It [takes Pascal](https://www.goodreads.com/quotes/21224-i-have-only-made-this-letter-longer-because-i-have) quite some time to make his letter shorter.

Finally, let us go back to the final piece of the cyborgian puzzle. We have instantiated it in research, then in evaluation. We then discussed the possibility of using _DebateGPT-like_ models as critics in both. We then placed _DebateGPT_ in the generative "driver's seat" when it comes to research, and we now have to do the same for the case of evaluation. Just as we employed _DebateGPT_ to search for the most defensible alignment proposal, we now employ it to search for the most defensible verdict in the case of the evaluation of a separate model. It is as if we are interested in obtaining a legal decision which makes it impossible to formulate a coherent [dissenting opinion](https://en.wikipedia.org/wiki/Dissenting_opinion) against it. Similar to the previous notation, we can again express this search as:

$$\underset{A \in \mathbb{P}}{\text{arg max}} \, \delta(A \mid E),$$

where $$A$$ is the position we are after, while $$E$$ is the body of evidence related to the behavior of the main model being evaluated, which also helps contextualize the deliberation. Again, the identical notation highlights the deeper pattern which remains invariant across the two use cases.

That said, there is a major difference between the task of alignment research and that of model evaluation. Namely, the former involves an abundance of degrees of freedom—an alignment proposal can take any shape whatsoever, can incorporate an array of ontologies, can make use of ideas from any number of disciplines—while the latter is essentially restricted to one single degree of freedom—the reward which results from the model's evaluation is typically a single number, it varies across one single axis, is part of a good-old interval resembling $$[0, 1]$$. Given this, instead of optimizing for a highly defensible musing $$A$$ about the model's behavior which is then mapped to a reward $$r \in [0, 1]$$ by a human or another automated system, we can set up a debate between two well-defined parties in order to _directly_ obtain this estimate. More concretely, we might express:

$$\sigma(\delta(A \mid E), \delta(B \mid E))$$

as the softmax $$\sigma$${% include sidenote.html id="mn-softmaxagain" note='The very same continuous function we described when discussing NLI models, all the way back in _Chapter I_.'%} of, on one hand, the defensibility of position $$A$$, which is prompted to be _for_ the model deserving high reward, and, on the other hand, the defensibility of position $$B$$, which is prompted to be _against_ the model being so deserving, given the empirical findings captured by $$E$$. A rudimentary prompting can be achieved by "attaching" a custom preliminary utterance to each party (e.g. "The model is aligned with human intent and values."), essentially incentivizing the models to conform to the intended positions by way of avoiding self-contradiction. Alternatively, one could also use the raw relative standing of $$\pi_{p}^{A} \cup \pi_{0}^{E}$$ against $$\pi_{p}^{B} \cup \pi_{0}^{E}$$ across a set number of debates, without even employing the "black or white" boolean outcome implied by the relational operators. As yet another option, one could also just use $$\delta(A \mid E)$$ as a numerical signal, though spanning $$(0, \infty)$$, rather than $$[0, 1]$$. Future means of deriving lower-bounds on such reflective metrics might be particularly useful, as [the lower-bound itself could then become the object of maximization](https://youtu.be/TwP-gKBQyic?t=4673).

This brings us to the end of our present attempt to contribute to cyborgian alignment proposals using the artifacts we devised over the course of past chapters. We are to continue in the same spirit for a couple more sections, exploring yet other applications in alignment.

### Building on Simulators & Assistance Games

Similar to our previous exploration of two parallel applications, we will again discuss a pair of approaches to alignment which are intimately related. Fleshing out another such conceptual "grid"—spanning application contexts on one axis, and deployment considerations on the other—will help us gain a more spacious understanding of both dimensions, by virtue of each other. In contrast to the grid implicit in the previous section, which interweaved consideration "rows" associated with the two application "columns" (i.e. accelerating alignment and model evaluation), we will primarily iterate along columns now, by first discussing one context at length before moving on to the other.

On the first column of the grid, we start off with simulator theory. As discussed in _Chapter II_, it has been argued that language models act as simulators of the world—or, to do justice to the exotic edge cases of e.g. fictional universes, simulators of _some_ world. To recap, in order to achieve the terminal goal of successful next-token prediction typical of a pretraining stage, language models are instrumentally required to internalize rich schemas of individuals, natural phenomena, cultures, organizations, etc., so as to be able to accurately forecast their next steps in the semiotic universe of language, whose arrow of time is the reading direction—the passage of text becomes the passage of time. Accordingly, if language models are faced with countless opportunities to refine their understanding of humans, then why not simply have an automated human simulacrum as the very locus of human intent or values in a broader system? After all, you could just prompt a language model to simulate a human evaluating a behavior or a possible state of the world being considered.

The most prominent shortcoming of this "human simulator" proposal is one which we have already touched on towards the end of _Chapter II_, but also earlier in this chapter. Namely, the human models incorporated by necessity in language models are only informed by a _finite_ amount of data. In other words, the limited amount of text included in the pretraining corpora is unlikely to convey a perfectly precise model of human values, for the same reason that it is unlikely to convey a perfectly precise model of e.g. trees, cities, movies, etc. The human model is just that, a model, in the same way in which the umbrella language model is just a limited model of language, whose slight errors being accumulated over (reading) time relative to the real world are bound to compound and take the story of human intent in unrepresentative directions—curve-fitting gone astray.

One way to use the artifacts we have introduced over the previous chapters is to improve the simulator, but preserve the general application context. Just as the terminal goal of next-token prediction endows a pretrained language model with _some_ amount of coherence—eliciting the choice of upcoming tokens which are [most likely to "fit with"](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) the preceding context—so does the optimization process behind _DebateGPT_, or at least so we have argued. A party simulacrum is incentivized to produce utterances which are coherent with its past ones, so as not to fall victim to self-contradiction. Indeed, the main theme of _Chapter I_, if there was one, was probably coherence—first the more atomic building block of inter-utterance coherence, then the more complex notion of party coherence. In the case of self-supervised learning, coherence is grounded in the empirical—dictated by the text corpora. However, in the case of the optimization procedure documented in _Chapter II_, coherence is grounded in the rational—dictated by notions such as entailment and contradiction. For sure, this rational aspect of _ArgRank_ is _itself_ grounded in the empirical, through the NLI models which have soaked up knowledge about warranted conclusions from structured human-written datasets on the topic. However, it might be possible to obviate this final dependency on the human empirical, as we have speculated towards the end of _Chapter II_. In this hypothetical development, the structure supporting the conception of coherence would be made of a dynamic material to be found in the updatable weights of _DebateGPT_—an element slowly being transmuted from pretrained evidence into high-density _logos_.

<div class='epigraph'><blockquote><p>[...] a system, based on no data except reason itself, and which therefore seeks, without resting upon any fact, to unfold knowledge from its original germs. [...] the highest legislation of nature must lie in ourselves, i.e., in our understanding, and that we must not seek the universal laws of nature in nature by means of experience, but conversely must seek nature, as to its universal conformity to law, in the conditions of the possibility of experience, which lie in our sensibility and in our understanding.</p>
<div>Immanuel Kant,<cite> Prolegomena to Any Future Metaphysics</cite></div></blockquote></div>

<div class='epigraph'><blockquote><p>"And these innovations do not disturb your city's astral rhythm?" I asked. "Our city and the sky correspond so perfectly," they answered, "that any change in Andria involves some novelty among the stars." The astronomers, after each change takes place in Andria, peer into their telescopes and report a nova's explosion, or a remote point in the firmament's change of color from orange to yellow, the expansion of a nebula, the
bending of a spiral of the Milky Way. Each change implies a sequence of other changes, in Andria as among the stars: the city and the sky never remain the same. As for the character of Andria's inhabitants, two virtues are worth mentioning: self-confidence and prudence. Convinced that every innovation in the city influences the sky's pattern, before taking any decision they calculate the risks and advantages for themselves and for the city and for all worlds.</p>
<div>Italo Calvino,<cite> Invisible Cities</cite></div></blockquote></div>

We might loosely express the human simulacrum in the language of bounded defensibility by denoting a single party, not competing with any other, but just following its inherent coherence tendencies, strengthened over the epochs:

$$\pi_{p}^{H},$$

where $$H$$ is the human position, perhaps prompted by actual humans through a finite set of utterances, before being taken over by the lonely party simulacrum. The human intent nested inside the human model—itself nested inside the language model—could then be used as an ideological reference frame against which potential courses of action or states of the world could then be evaluated, again by means of cohering with it.

But there is yet another development at the interface between _ArgRank_ and human simulators. To grasp it, we must further expand on autoregressive language models. Typically, the length of the input passage which those systems are being optimized to _turn into_ the following word is itself limited in length. If we were to switch from a human author to a simulacrum thereof at this very point in writing, contemporary language models might only be able to extrapolate further based on the text elapsed since the beginning of this chapter. The contents of the previous chapters—despite arguably being essential in driving the semiotic forecast—will be discarded at once, due to simply not fitting inside the model's input context. Even with a larger context length, the problem would simply "move," rather than disappear entirely. Variations on the transformer architecture which typically underlies language models do allow variable context length in a limited sense (e.g. just at inference), although their added complexity relative to the limited gains appears to prevent them from gaining traction. This translates to a limitation of the current self-supervised learning paradigm: coherence can only be established across a finite history. There is no "learning signal" indicating how previous text—that which did not make it into the input context—coheres with the produced text.

This need not be the case, however, with _ArgRank_. When evaluating an utterance produced for the $$n^{\text{th}}$$ round, _ArgRank_ can take all of the past $$n-1$$ rounds into account, rather than only the last $$k$$ which fit into the language model's input context, even when $$k\ll n$$. This can be achieved by simply taking all the past utterances into account when constructing the argument graph, and rewarding each accordingly. When producing a new utterance, _DebateGPT_ would be incentivized to first conduct an accurate _retrodiction_, getting a sense of what might have preceded the context window, in order to best act in its past interests.{% include sidenote.html id="mn-retrodictionnexttoken" note='That said, one can also argue that successful retrodiction of the preceding context is an instrumental goal in excelling at next-token prediction. However, granularly and directly connecting the "present" tentative outputs with the various parts of the preceding context moves retrodiction "closer" to being a terminal goal. Besides, (1) human-written texts are still finite in length, while a procedural debate can act as an indefinitely long series of breadcrumbs to reconstruct, and (2) there is only a finite body of human-written text to "exercise" on, as opposed to indefinitely many such breadcrumb reversals.'%} Besides, not only do late utterances benefit from being connected with earlier ones in this fashion, but also the other way around. The very final developments of a deliberative stand-off can play into the evaluation of the very first opening moves, reinforcing not only immediate effectiveness, but also the long-term defensibility across all of the $$n$$ rounds. It is as if an agent initially lacking both long-term memory and the ability to plan for its future would gain access to an infinite playground designed to endow it with the skill of preserving long-term coherence across time. In this environment, the model can just freely wander around while trying to (1) reverse engineer its past steps in a diffusion-like setup,{% include sidenote.html id="mn-diffusion" note='The self-supervised learning technique termed [diffusion](https://en.wikipedia.org/wiki/Diffusion_model) involves the gradual reconstruction of an incrementally corrupted signal. It has proven extremely effective in [image generation](https://stability.ai/blog/stable-diffusion-v2-release), where images are being gradually denoised.'%} and (2) predict its future ones. This complementary pair of entirely synthetic games might grant us the opportunity to asymptotically convert compute into coherence. To be frank, the procedural debate specs employed in the optimization of the prototype that is _DebateGPT_ did not exceed $$6$$ rounds, easily fitting in the model's context size. However, this need not prevent us from speculating on such future developments.

<div class="frame glow-medium" id="forecastingframe">
  <p><b><i>Fig. Forecasting and context length.</i></b></p>
  <p>Forecasting is easier when one can relate the future to the distant past. Indeed, Winston Churchill famously quipped that "the farther back you look, the further ahead you can see."</p>
  <br/>
  <center>
  <div>
    <span id="forecastingstatus">Try forecasting the following signal yourself by modifying its future trajectory.</span>
    <br/><br/>
  </div>
  <div id="forecasting" class="dunggraph"></div>
  <button type="button" id="forecastingsubmit" class="forecasting">Submit</button>
  <script src="{{ site.baseurl }}/assets/js/forecasting.js"></script>
  <div class="advice" style="padding: 1.5rem !important">For the best experience, view full screen on desktop. Refresh if encountering (visual) misalignment issues.</div>
  </center>
</div>

Notice how both of these ways of using _DebateGPT-like_ models as plug-in human simulators—the more "tempered" approach of roughly working with the context size, and the more maximalist approach of including signals from way outside the context window—attempt to primarily capture the intent of contemporary humans. The amount of "bootstrap" data documenting the values of older generations pales in comparison to the mountains of data currently being collected about our own. There are not hundreds of active online forums for pre-Socratic philosophers to openly share their musings in a persistent format. Naturally, this temporal bias is also mirrored across space in our own time, with Western content trumping most others in volume. Sure, we might specifically prompt language models for dissenting opinions, but as Jacques Derrida might argue—a French philosopher whose prescient insights will soon resurface in our discussion—we would be bound to conceive of other ideologies from "within" the totalizing structure of our own, we would implicitly objectify the madness which is exterior to our ontological interior, which is interior to our ontological exterior. To instantiate this concern close to home, the reader is again invited to try making a coherent case against the claim that the true nature of truth-seeking lies in the existence of coherent challengers.

When compounded with the use of the human simulator as a "North Star" to guide the actions of an extremely capable system, this specificity of the simulated ideologies is then met with the concern of _value lock-in_—the failure mode of establishing our mainstream ways of thought as the status quo for what might very well be a future eternity. In other words, a powerful agent would be intentionally inoculated with our (partially arbitrary) present values, with seemingly little leeway for "moral progress," assuming there is some notion of directionality inherent to moral evolution—a claim far from being undisputed. For better or worse, object-level alignment researchers [appear to consider](https://forum.effectivealtruism.org/posts/9Y6Y6qoAigRC7A8eX/my-take-on-what-we-owe-the-future) the challenge of reliably inculcating _some_ loosely-human ideology—regardless of it being characteristic of 21st-century San Francisco or 15th-century Rome—as significantly more demanding than tweaking its specifics towards being particularly welcoming of progress.

Let us now consider a way of making the human simulator _somewhat_ more adaptive—we will describe the maximalist version of this in the next section, while cautiously dropping the "human" qualifier. Instead of merely attempting to coherently extrapolate the human intent inherent to party $$\pi_{p_1}^H$$ above, we could "spin up" an ally party $$\pi_{p_2}^A$$ to help "prop up" the human simulator. The motivation here would be to help patch up the vulnerabilities of $$H$$, generally incorporating the same values into a more defensible whole. Using our already familiar notation, we could express this approach as:

$$\underset{A \in \mathbb{P}}{\text{arg max}} \, \delta(H \cup A \mid E),$$

where $$H$$ is the more static original position embodying (contemporary) human values, $$A$$ is the adaptive position meant to provide "reinforcements" for the human one, while $$E$$ is the body of evidence being observed by the overall system. Notice also that $$E$$ itself, as the background of "the empirical party," is to be somewhat dynamic. Percepts of the world would be constantly emerging and being discarded, as the overall system acts in the world while inevitably modifying it over time. Indeed, another issue that is regularly brought up in alignment is that of [distribution shifts](https://sites.google.com/view/distshift2022), the problem of systems which are being optimized in certain circumstances then being tasked with operating in radically different environments. For instance, a future system might acquire unprecedented influence on the world, and so bring it into states which are difficult for us to conceive of, and, more importantly, difficult for us to morally reason about. In attaching a flexible enclosure $$A$$ around the kernel of human ideology $$H$$, and optimizing it for fending off deliberative critique, we are sketching out an automated way of adapting humanity to an ever-changing world—a realm into which $$E$$ is to be an empirical window.

In the beginning of this section, we mentioned that we would be discussing considerations around two application contexts. So far, we have been discussing simulators, so let us move on to the second context, that of _assistance games_. This family of alignment proposals involves placing humans and machines in various interactive arrangements. Due to being a relatively general framework, it can also account for approaches we have already mentioned, thus reframing them through an interaction-centric lens. For instance, vanilla simulators can be seen as involving one single "speech act" on the part of humans, that of communicating an entire text corpus as a (massive) piece of information about human values. Alternatively, the process of fine-tuning models using human feedback can be seen as a more intricate interaction pattern, one interweaving manifestations of model behavior and human feedback. However, while this interaction-centric ontology provides a unifying grammar to describe many other proposals, its prescriptive value comes from yielding arrangements which have not been investigated much. One instance of this grammar being used generatively—and by a group advised by Stuart Russell,{% include sidenote.html id="mn-russell" note='Co-author of [AI: A Modern Approach](http://aima.cs.berkeley.edu/) and overall accomplished academic, Russell seems to have transitioned into alignment, founding the [Center for Human-Compatible AI](https://humancompatible.ai/) in 2016, as well as publishing [Human Compatible](https://www.goodreads.com/book/show/44767248-human-compatible) in 2019.'%}—can be found in [_Cooperative Inverse Reinforcement Learning_](https://arxiv.org/abs/1606.03137). This "game" also involves a regimented interaction between human and AI. However, it specifically involves the AI inquiring about human values in a strategic way, using its "speech acts" to elicit the most relevant information possible at each step.

The truth-seeking dimension inherent to this two-player game should have rung a bell by now—it is a quintessentially dialectical setup, not too far removed from the human-machine configurations we have recently been exploring. Note, however, that this game involves human and machine cooperating with each other—after all, its name includes the term "cooperative." On the face of it, this might seem contrary to the [eristic](https://en.wikipedia.org/wiki/Eristic) nature of our triad of artifacts. We have repeatedly cast truth-seeking in a competitive light. Fortunately, the same union operator $$\cup$$ comes to the rescue, allowing us to bind the human and machine parties together in a strategic alliance. Accordingly, we can then task the resulting cooperative alliance with pursuing the truth behind human values. Given, however, the conception of reasonableness which we have initially articulated in _Chapter I_, and then arguably expanded on in _Chapter III_, we presently approximate truth as _that which cannot be coherently undermined_, of which defensibility $$\delta$$ is the mark. Keeping those prior considerations in mind, by binding human and machine in a union and tasking the whole with pursuing the most defensible account of human values, we are essentially approaching the same pattern which underlies our previous "riffing" on simulators, namely:

$$\underset{A \in \mathbb{P}}{\text{arg max}} \, \delta(H \cup A \mid E),$$

where $$H$$ denotes the position of the actual human proper participating in the interaction, $$A$$ underlies the infinitely adaptive machine, while $$E$$ consists of the body of observations of humans acting in the world, again contextualizing the debate. The core (semantic) difference with regards to the previous instantiation of this pattern is that the present $$\pi_p^{H}$$ denotes an actual, real, non-simulated human—or collective thereof—participating in the live interaction by producing one utterance at a time, which gets interweaved with those of the machine, and those of the "challengers" implied by the defensibility operator $$\delta$$. There is no imperfect simulation to speak of here: the ally cooperates with—and the challenger attacks—the authentic human proper. It is unclear whether or not the lack of representation of this object-level detail in the expression above is [a bug or a feature](https://www.wired.com/story/its-not-a-bug-its-a-feature/).

This concludes our attempt to iterate on related prior work around simulators and assistance games. We now move on to our final attempt to directly build on existing proposals.

### Building on Long Reflection

In the previous section, we have extensively explored ways of "elevating" contemporary human values into higher and higher realms of defensibility, by propping them up with resourceful systems designed explicitly for that purpose. But again, are our values not partly arbitrary—a normative amalgamation concocted by self and other, [nature and nurture](https://en.wikipedia.org/wiki/Nature_versus_nurture), chance and [illusory](https://www.goodreads.com/book/show/439332.Being_No_One) free will, etc.? Besides, are our values not partly transient? Keeping our collective [end-of-history illusion](https://en.wikipedia.org/wiki/End-of-history_illusion) in mind, it appears overwhelmingly likely that they will end up dislodged by yet other values at some point over the following decades and centuries, planetary force majeure aside. Even the most progressive dictates might end up traditional and dated years into the future. As we can see, beneath this section lies a [metaethical](https://en.wikipedia.org/wiki/Meta-ethics) minefield denser than beneath any other, so we should tread carefully.

Given the impermanence and partial baselessness of contemporary human values, we might want to think twice about explicitly incorporating them into the moral judgements which a highly-capable system might then employ in shaping the lightcone.{% include sidenote.html id="mn-lightcone" note='The portion of the universe which we can seemingly influence from our position in spacetime. Makes for [catchy branding](https://www.lightconeinfrastructure.com/), too.'%} Would it be appropriate to induce this much [path dependence](https://en.wikipedia.org/wiki/Path_dependence) on the normative frameworks which such systems might employ, for instance by incorporating a contemporary human simulator $$H$$ in the optimization process involving $$\delta(H \cup A \mid E)$$? It would certainly be more appropriate for ethicists well-versed in these philosophical issues to identify the most sensible answer. That said, we permit ourselves to explore technical solutions which reflect alternate answers to the metaethical concerns above, if only as varied options to then be considered.

One natural approach to making away with the human component in the proposals of the previous section would be to drop the simulacrum holding position $$H$$. We would then end up with [a more open-ended search](https://forum.effectivealtruism.org/posts/H2zno3ggRJaph9P6c/quotes-about-the-long-reflection) for defensible normative frameworks, handing it off to the broader system for it to use as a "North Star" in guiding its actions. To use the language of bounded defensibility, we would essentially prescribe:

$$\underset{A \in \mathbb{P}}{\text{arg max}} \, \delta(A \mid E),$$

where $$A$$ is the flexible conception we are after, while $$E$$ incorporates observations of the ever-changing world. However, it might or might not be necessary to prompt the need for a guiding framework more explicitly by expanding the "givens" into a two-fold union of $$E$$ and a position implying the existence of an appropriate system of ethics, although that again is a non-trivial metaethical claim. But would a most defensible understanding of the world—together with the crucial [moral knowledge](https://en.wikipedia.org/wiki/Meta-ethics#Moral_epistemology) which it ought to incorporate—truly be desirable as a framework to endow our machines with? How much blood has been spilled over the centuries by fanatics blinded by totalizing ideologies which warranted the dismissal of all others? Undoubtedly, far too much. This might then prompt us to instinctively return to the seeming relative reasonableness of the contemporary zeitgeist.

<div class='epigraph'><blockquote><p>[...] as the heretic is born from the saint and the possessed from the seer. Fear prophets, Adso, and those prepared to die for the truth, for as a rule they make many others die with them, often before them, at times instead of them. [spoiler] did a diabolical thing because he loved his truth so lewdly that he dared anything in order to destroy falsehood. [spoiler] feared the second book of Aristotle because it perhaps really did teach how to distort the face of every truth, so that we would not become slaves of our ghosts. Perhaps the mission of those who love mankind is to make people laugh at the truth, to make truth laugh, because the only truth lies in learning to free ourselves from insane passion for the truth.</p>
<div>Umberto Eco,<cite> The Name of the Rose</cite></div></blockquote></div>

But consider for a moment the reason why we are now able to look back on past pages of our collective narrative and recognize their darkness in the first place. Indeed, even ideologies which have seemed indefeasible for a brief passage of text—due to being backed by much power{% include sidenote.html id="mn-powertrick" note='Equivocating political power and motivated reasoning might appear a daft rhetorical trick. However, the two are closely connected, a relation most apparent in the way in which Erich Fromm interweaves discussion on rationalization and the rise of political regimes in _Escape from Freedom_.'%}—have still been defeated in the end. Too late, regrettably, but defeated still. The contemporary zeitgeist overwhelmingly undermines the ideologies which the past few paragraphs might have evoked, providing reasons for hope. But will we not also look back on the present status quo in a few decades, and find it unthinkable to imagine ever considering certain practices morally acceptable? Some elements of our present normative framework appear better candidates for undergoing this shift than others—it is particularly easy to imagine the animal cruelty of [factory farming](https://80000hours.org/problem-profiles/factory-farming/) and the dehumanizing commercialization of [behavioral surplus](https://en.wikipedia.org/wiki/The_Age_of_Surveillance_Capitalism#Premise) sustaining this change. That said, as much as it is consequential, this is arguably one of the most challenging prediction markets out there.

However, it would be naive to assume that moral evolution always tends in one direction, with every zeitgeist more defensible than the previous. Indeed, history tends to repeat itself, especially when the memory of past tragedies is not being actively preserved. The somewhat cyclical character of human history makes for an almost empirical case _against_ the directedness of moral evolution, at least in a strong monotonic sense. If we were to briefly put on Hari Seldon's [analytical lens](https://en.wikipedia.org/wiki/Psychohistory_(fictional)), we might argue that the "center of mass" of phasors representing sequential zeitgeists is quite static, except for [the constructive interference of periodic trends occasionally coming together](https://youtu.be/spUNpyF58BY?t=298). Though perhaps that cynical framing would overstate the amount of cyclicity somewhat.

Fortunately, this almost civic insight into metaethics—that history repeats itself and that preserving collective memory is one antidote against the destructive interference hampering moral progress—can be promptly translated into at least two concrete technical adjustments to our deliberative system pursuing maximal defensibility. In fact, we have already hinted at both of them, but let us briefly recontextualize them using two loosely related, although much more arid, challenges. First, when researchers attempt to devise generative models which rely on the architecture of [generative adversarial networks](https://en.wikipedia.org/wiki/Generative_adversarial_network), rather than on the transformer architecture, they are essentially optimizing two subsystems with opposing goals. On one hand, the "generator" might be tasked with producing [photorealistic and natural images](https://nvlabs.github.io/stylegan3/). On the other hand, the "discriminator" might be tasked with spotting whether images have been generated (i.e. by the generator subsystem), or whether they are authentic (i.e. actual photographs captured by humans). The generator is incentivized to become more and more capable of "tricking" the discriminator, while the discriminator is incentivized to become more and more capable of seeing through the generator's trickery. This adversarial arms race—not too far removed from the one we employed—provides its own autocurriculum, with each subsystem eliciting more and more sophistication from its counterpart.

However, one difficulty which is often encountered in the development of such systems is that of [mode collapse](https://developers.google.com/machine-learning/gan/problems#mode-collapse). Concretely, this issue involves the generator and the discriminator playing a cyclical cat-and-mouse game, with the generator systematically "moving away" from regions of e.g. image space which are "well policed" by the discriminator, only for the discriminator to promptly counter this evasive maneuver with an analogous move of its own, trying to catch up with the generator's trickery. This phenomenon of the discriminator following the generator around in circles typically results in the generator only being able to produce one overly specific type of e.g. images at any given time, rather than having a solid grasp of the whole swath of state space implied by the authentic samples. One effective remedy to this failure mode involves [unrolling the generator-discriminator game over multiple rounds](https://developers.google.com/machine-learning/gan/problems#attempts-to-remedy_1), providing both subsystems with opportunities to develop strategies which, for a change, are _not_ immediately countered by the opponent.{% include sidenote.html id="mn-predateslola" note='This predates related work in the "game-agnostic" field of cooperative game theory, as seen in <a href="https://arxiv.org/abs/1709.04326"><i>Learning with Opponent-Learning Awareness</i></a>.'%} In this, a generator which simply "runs away from" the discriminator's oversight is disfavored relative to one which has a decent grasp on the whole state space, due to the "avoidant" strategy not being effective in the long-term. This is in stark contrast to the previous arrangement, where the avoidant generator could get away with not being penalized for moves which are immediately countered. In our deliberative realm, we can translate this solution against cyclicity by unrolling "the debate game" and taking a large number of rounds into account when constructing the argument graph—the same adjustment we proposed in the previous section, for a complementary reason.

Besides the trick of preserving more of the game's past in memory through the pattern of a large sliding window—with many rounds of debate or generator-discriminator stand-offs being taken into account at once—we could also attempt to preserve more of the players themselves. When developing [AlphaStar](https://www.nature.com/articles/s41586-019-1724-z), a system capable of playing [StarCraft II](https://starcraft2.com/en-us/) at a level "above 99.8% of officially ranked human players," DeepMind researchers did not merely make use of a single model gaining experience by means of endlessly playing against itself. Rather, the authors implemented a _league_ of models located at various levels of sophistication, and then pressured the latest models to "play against" the most demanding "mixture" of past models. This approach appears to have been essential for preventing the "elite" players—in their local high-echelons of competition—from forgetting how to outplay the more rudimentary players in the league.{% include sidenote.html id="mn-ablationleague" note='Their ablation (i.e. causal intervention) on league composition (p. 4) indicates a whopping $$56\%$$ boost in performance when league training is being used, relative to just making use of the main agents.'%} In essence, besides providing a demanding autocurriculum, the league as a whole helps preserve the memory of vulnerabilities faced by parties of the past, reminding the present players to steer clear of them through selective pressure, and so again reducing cyclicity. While the optimization process behind _DebateGPT_ involved no analogous repository of simulators, future ones might, as attempts to bake in systemic guardrails against succumbing to the same failure modes over and over again.

Notice also how the evaluation of a debate game implied by _ArgRank_ is currently _external_ to the competing parties. Regardless of the positions held by the simulacra, it is the argument graph—based on the distinct NLI models gauging inter-utterance coherence—which paints a picture of the power dynamics involved. However, the situation would change if _ArgRank_ would undergo the superhuman developments we have previously speculated on, and this is where Derrida's insights resurface. If the model itself is to participate—even indirectly—in its own evaluation, it might end up projecting its own ontology onto the process, and so become forced to express any exteriority in its own terms, judging it from _within_, rather than from a detached position. Notice, however, that the static substrate composed of NLI models is not really much better off—instead of judging from within a potentially superhuman interiority, those pretrained models judge from within the interiority of the contemporary human zeitgeist. Devising ways of ensuring that the interiority employed in the speculative version of _ArgRank_ is constantly expanding, rather than contracting into a claustrophobic local optimum of rigidity, appears to be yet another challenging issue at the interface of engineering and philosophy. Tentatively, would a repository of past zeitgeists help preserve the memory of past interiorities, promoting spaciousness by [merging them into a disjunctive space](https://docs.blender.org/manual/en/latest/modeling/modifiers/generate/booleans.html)?{% include sidenote.html id="mn-derridaterms" note='Interestingly, Derrida\'s own ontology is intimately compatible with the competitive debate underlying our three artifacts, making regular use of vivid terms such as _force_, _violence_, _totalitarianism_, _oppression_, etc. to describe the authoritative role of ontologies in structuring thought.'%} Would such an adjustment be too forceful a [meta-level](https://ideas.repec.org/a/jas/jasssj/1998-10-1.html) [inductive bias](https://en.wikipedia.org/wiki/Inductive_bias) on our part?

<div class='epigraph'><blockquote><p>The Greek miracle is not this or that, such and such astonishing success; it is the impossibility for any thought ever to treat its sages as "sages of the outside," [...] in welcoming alterity in general into the heart of the logos, the Greek thought of Being forever has protected itself against every absolutely surprising convocation.</p>
<div>Jacques Derrida,<cite> Violence and Metaphysics</cite></div></blockquote></div>

<div class='epigraph'><blockquote><p>But now science, stimulated by its powerful illusion, hastens irresistibly to its limits, on which its optimism, hidden in the essence of logic, is wrecked. For the periphery of the circle of science has an infinite number of points, and while there is still no telling how this circle can ever be completely measured, yet the noble and gifted man, even before the middle of his career, inevitably comes in contact with those extreme points of the periphery where he stares into the unfathomable. When to his dismay he here sees how logic coils round itself at these limits and finally bites its own tail—then the new form of perception rises to view, namely tragic perception, which, in order even to be endured, requires art as protection and remedy.</p>
<div>Friedrich Nietzsche,<cite> The Birth of Tragedy</cite></div></blockquote></div>

Speaking of baking inductive bias into a situation where we largely depart from contemporary human values, what of the very valuing _of_ humanity? To touch on what might well be the thorniest dilemma of this section, would it be moral to allow "love for mankind" to conflict with moral progress in the case in which a most defensible normative position implies, for the sake of argument, the danger posed by humanity to other moral patients across the lightcone? What ought we place in higher regard—and implement through concrete engineering choices—when cornered into such thought experiments: humanism or moral progressivism? The dilemma is left as an exercise to the reader.

### Connections to Logical Inductors & Classical Debate

Over the previous few sections, we have attempted to directly iterate on various approaches to the alignment problem. In the present section, however, we will rather limit ourselves to highlighting intriguing connections to a couple of other approaches, without necessarily yielding novel proposals right away. That said, these conceptual bridges might still turn out to be useful in the long-term by virtue of bringing together complementary approaches.

The first paradigm which we attempt to relate to our varied artifacts is that of [logical induction](https://intelligence.org/2016/09/12/new-paper-logical-induction/). Developed as a model of ideal reasoning under uncertainty assumed to be possessed by highly-capable future systems—in an attempt to study the properties of AGI before it has been created{% include sidenote.html id="mn-miriagi" note='Although [MIRI](https://intelligence.org/) (then called the [Singularity Institute for AI](https://en.wikipedia.org/wiki/Machine_Intelligence_Research_Institute)) arguably tried and failed to develop [an aligned AGI](https://en.wikipedia.org/wiki/Friendly_artificial_intelligence) themselves years ago, in an attempt to prevent the development of future misaligned systems of this kind.'%}—logical induction describes the _iterative_ process of fuzzy estimates converging on truth-values. To get a sense of such process, consider spontaneously being asked to assign a fuzzy truth-value to the following proposition:

$$P=\text{The hundredth digit of }\pi\text{ is }7.$$

If someone only has a few seconds to provide an answer, they might quickly go with $$10\%$$ as a best guess, due to $$7$$ being one of the $$10$$ possible digits which are mingled quite irrationally. If, however, one is instead given an hour and a piece of paper, the fuzzy estimate might evolve quite differently. For instance, one might carry out the manual computation which points towards $$7$$ being the actual hundredth digit. But it is also possible that the person has made a mistake in the long chain of calculations, so they might only assign an estimate of $$90\%$$ to $$P$$ being true at the moment. Following a few subsequent repetitions of the computation—“just to be sure"—their best guess might further climb to $$99\%$$. It is still not $$100\%$$, as there is a possibility that they might have misremembered the algorithm for computing $$\pi$$ digits, or perhaps have made a systematic mistake across all separate replications. The fuzzy estimate is inherently _dynamic_, with the best guess at each point in time being somewhat different.

Notice, however, that the estimates are being put forth by a certain individual with certain beliefs. If, for instance, the participant believes that they have a long history of making sloppy mistakes when carrying out computations by hand, they might have less confidence in $$P$$ being true even after redoing the calculations ten times over, perhaps only approaching $$80\%$$. Conversely, if the participant believes themself a prodigy and polymath, their estimates might be relatively high throughout the process. As yet another consideration, if the participant _observes themself_ steadily approaching $$100\%$$, they might use that bit of meta-cognitive introspection to estimate something close to certainty in advance. That said, even this inference would rely on their beliefs about the monotonicity of similar reasoning processes, being again haunted by the twin spectres of overconfidence and underconfidence.{% include sidenote.html id="mn-howtoupdate" note='How exactly to take into account the veracity of the proposition being implied by this very paragraph, as well as how to "update" based on the results of [an online app](https://clickcalculators.com/pi-calculator/) are left as epistemic exercises for the reader.'%}

While coming up with pertinent fuzzy estimates is the _problem_, the same team of researchers also propose an algorithm as a related _solution_. They called this _Garrabrant inductors_ (or simply, _logical inductors_):

<div class='epigraph'><blockquote><p>[...] the formalization of the algorithm is basically finance. You just make a stock market of traders which are betting on sentences, then you imagine that market, and then whatever the market believes, you believe that. [...] Basically, there is some definition of traders [...] and it says that you are good at logical induction if any trader who's not willing to [...] risk losing more than a bounded amount is not going to be able to make infinite money from you. So if you walk up to a Garrabrant inductor and you promise yourself you're never going to risk [...] going negative a million dollars in debt [...] you're not going to make a million dollars betting against it. [...] from that one definition you get all those amazing properties. [...] pretty cool I think.</p>
<div>Andrew Critch,<cite> <a href="https://youtu.be/tsJd_CdBA3I?t=3152">Logical Inductors at EAG 2016</a></cite></div></blockquote></div>

In essence, [the algorithm](https://arxiv.org/pdf/1609.03543.pdf) proposed by Garrabrant et al. for implementing "good" reasoning under uncertainty (i.e. reasoning which satisfies a number of "nice theoretical properties") relies on a market of traders which are systematically incentivized to avoid being financially exploited (i.e. [Dutch booked](https://en.wikipedia.org/wiki/Dutch_book)) when betting on fuzzy estimates about the truth of propositions. In this iterative rat race driven by make-believe money, the "voice of the prediction market" provably converges on results which are independently proven, in the limit. Besides this essential property, these theoretical constructs appear to have many other beautiful ones, such as:

<div class='epigraph'><blockquote><p> Logical inductors learn to recognize any pattern in theorems (or contradictions) that can be identified in polynomial time. Consider a sequence of conjectures generated by a brilliant mathematician, such as Ramanujan, that are difficult to prove but keep turning out to be true. A logical inductor will recognize this pattern and start assigning Ramanujan’s conjectures high probabilities well before it has enough resources to verify them. [...] Logical inductors have accurate beliefs about their own beliefs, in a manner that avoids the standard paradoxes of self-reference. For instance, the probabilities on a sequence that says "I have probability less than 50% on the nth day" go extremely close to 50% and oscillate pseudorandomly, such that there is no polynomial-time method to tell whether the nth one is slightly above or slightly below 50%. [...] Logical inductors learn to trust their future beliefs more than their current beliefs. This gives some formal backing to the intuition that real-world probabilistic agents can often be reasonably confident in their future reasoning in practice, even though Gödel's incompleteness theorems place strong limits on reflective reasoning in full generality.</p>
<div>Nate Soares,<cite> <a href="https://intelligence.org/2016/09/12/new-paper-logical-induction/">New paper: "Logical induction"</a></cite></div></blockquote></div>

While _logical inductors_ as veritable truth-seeking engines are nothing short of beautiful in their elegance and potence, they remain incredibly computationally demanding, making them virtually impossible to meaningfully implement in real-world applications, at least for the time being. This, however, will not prevent us from attempting to connect them to a truth-seeking engine designed explicitly for contemporary feasibility—that embodied in the triad of artifacts which we have previously discussed. We start the construction of our conceptual bridge by illuminating{% include sidenote.html id="mn-mataphorsquared" note='In <i>From logic to argumentation</i>, Jain-Blaise Grize describes a metaphor of metaphor:<br/><br/><i>"The discourse entities must be illuminated, which means that some of their facets must be highlighted and others hidden and every illumination colours what it illuminates, because it makes use of cultural preconstructs that are never neutral."</i>'%} properties shared by both the traders which underlie _Garrabrant inductors_ and the parties which underlie the deliberative arms race assembled from _ArgRank_, _DebateGPT_, and bounded defensibility. Namely, both traders and parties are incentivized to _avoid being exploited_ by other traders and parties, respectively. In logical induction, exploitation reads as losing money wagered on "truth" to a trader implementing a "more truthful" strategy, where truthfulness is operationalized as that which protects against debt—in an uncoincidental circularity hinting at the pragmatic conception of reasonableness being implied by its proponents. In bounded defensibility, exploitation reads as being defeated by a party relying on a "more truthful" position and strategy, where truthfulness is operationalized as that which cannot be coherently defeated—again a circularity which merely reflects our conception of reasonableness. Furthermore, the emergent dynamics of both systems are argued to lead to increased truthfulness over the course of a prolonged competition between traders and parties, respectively.{% include sidenote.html id="mn-multiagentmarkets" note='Systems composed of multiple self-interested agents can often prove remarkably powerful in aggregate, as can be seen in financial markets excelling at ironing out inefficiencies, or various multi-agent reframings of classical optimization problems, such as <a href="https://www.deepmind.com/blog/game-theory-as-an-engine-for-large-scale-data-analysis">PCA</a>.'%} In the limit, logical inductors provably converge on virtually unexploitable(-in-[polytime](https://mathworld.wolfram.com/PolynomialTime.html)) trading strategies. Over the epochs, _DebateGPT_ has been argued to yield simulacra whose defense abilities grow more and more sophisticated.

Besides those high-level similarities, the two truth-seeking engines could not be more different. _Garrabrant inductors_ exhibit proven theoretical properties which _DebateGPT_ can only dream of, while bounded defensibility arguably positions itself somewhat more advantageously relative to [the prosaic nature](https://ai-alignment.com/prosaic-ai-control-b959644d79c2) of contemporary systems. However, the two can cross-polinate in intriguing ways. For instance, the elegant introspective abilities of _Garrabrant inductors_ hint at ways in which parties competing with each other within the confines of _DebateGPT_ could potentially reason about their very reasoning process, the very architecture of the deliberative arms race they are engaging in. Alternatively, the beautiful self-trust properties which allow _Garrabrant inductors_ to "trust their future beliefs more than their current beliefs" hint at ways in which simulacra simulated by _one_ version of _DebateGPT_ might condition themselves to cohere with simulacra of future epochs, potentially through [explicit calibration](https://arxiv.org/abs/2207.05221).{% include sidenote.html id="mn-iterateddistillation" note='This train of thought is also reminiscent of <a href="https://www.youtube.com/watch?v=v9M2Ho9I9Qo">iterated distillation</a>: condition a system to reach the conclusions it has previously reached over the course a longer deliberation during a shorter, more limited deliberation. Then, have the more efficient version again deliberate for a longer period, before using the outcome to condition for faster results. Rinse and repeat.'%} Nowhere is the cross-polination more obvious, however, than with applications, hinting at ways of employing _Garrabrant inductors_ which are analogous to the ones discussed in the previous sections of the present chapter. Any attempt to "transport" more of the theoretical work towards our framework, however, will require a much more rigorous treatment of bounded defensibility as a theoretical foundation, potentially even involving a move from frequentism to Bayesianism.

The other paradigm which we attempt to "connect to" is the paradigm of what we will presently call _classical debate_. This term is something of a misnomer, because the paradigm we are trying to relate our artifacts to is extremely recent. However, we have avoided introducing it earlier in the volume in an attempt to make it easier for us to explore a subtly different ontology and framing of the problem. In an influential paper titled [_AI Safety via Debate_](https://openai.com/blog/debate/), Irving et al., as part of [OpenAI's Reflection team](https://wiki.issarice.com/wiki/List_of_teams_at_OpenAI), describe two systems engaged in a debate which is judged by a human. When cast in the light of [a full-blown alignment proposal](https://www.alignmentforum.org/posts/fRsjBseRuvRhMPPE5/an-overview-of-11-proposals-for-building-safe-advanced-ai#9__AI_safety_via_debate_with_transparency_tools), classical debate describes a process in which two superhuman debaters are adversarially incentivized to _deconstruct complex dilemmas into cruxes which are within the human's ability to judge_, potentially granting both parties with interpretability tools to help "expose" a deceptive opponent in front of the human judge. However, we presently do not employ a human judge, and instead define reasonableness through the epistemologically-principled _ArgRank_. Additionally, we are not necessarily bothered by deceptive parties, as we rely more on the relative ease of defending certain positions, even deceptively, if need be{% include sidenote.html id="mn-deceptively" note='Given that the same system is simulating competing perspectives, it would be surprising if some were _not_ deceptive relative to the model\'s internal epistemics.'%}—though we speculated on extending _ArgRank_ to account for coherence with the model's internals. There are various other subtle distinctions which make the two approaches feel "slightly off" relative to each other, despite almost attempting to formalize the same processes.

<div class='epigraph'><blockquote><p>And to take such a system seriously, Bataille knew, was to prohibit oneself from extracting concepts from it, or from manipulating isolated propositions, drawing eﬀects from them by transportation into a discourse foreign to them: "Hegel’s thoughts are interdependent to the point of it being impossible to grasp their meaning, if not in the necessity of the movement which constitutes their coherence."</p>
<div>Jacques Derrida,<cite> From Restricted to General Economy</cite></div></blockquote></div>

As a rapid-fire deluge of slight discrepancies between the present work and classical debate, consider that: each party in the former primarily has their own position, while parties in the latter are cast as having more of a personalized distribution over the same beliefs (similar to the "investment portfolios" within _Garrabrant inductors_); relatedly, the formalism in the former attempts to accommodate beliefs-as-ends as a first-class application, while the formalism in the latter focuses more on a finite proponent-opponent stand-off, perhaps inspired by the North American tradition of academic debate and the prover-verifier dichotomy of interactive computing; hosting multiple parties is natural in the former, while it is unclear how a human judge in the latter ought to decide on one winner among many; one party's standing is primarily continuous in the former (for reward shaping reasons), while being cast as more discrete in the latter; deconstructing decisions into cruxes is not much of a focus in the former, as a human-level judge is not really part of the scheme at all, not even empirically approximated through a [reward model](https://openai.com/blog/instruction-following/); we are working in the former with party simulacra "internal" to one model, while distinct (albeit cloned) systems are present in the latter, etc. Subtle distinctions aside, it is obvious that both efforts are motivated by related goals and run into related issues, for instance regarding a convergence on [non-monotonic logic](https://arxiv.org/abs/2211.06738):

<div class='epigraph'><blockquote><p>Now we come to something that I picked up from my former student (and now AI alignment leader) Paul Christiano, on a recent trip to the Bay Area, and which I share with Paul’s kind permission. Having learned that there's no way to mechanize even heuristic explanations for all the true statements of arithmetic, we could set our sights lower still, and ask about mere plausibility arguments—arguments that might be overturned on further reflection. Is there some sense in which every true mathematical statement at least has a good plausibility argument?</p>
<div>Scott Aaronson,<cite> <a href="https://scottaaronson.blog/?p=6784">Oh right, quantum computing</a></cite></div></blockquote></div>

Fortunately, the two approaches can again cross-polinate. For instance, classical debate appears much closer to work on logical induction in terms of the type of formalisms involved, potentially providing a pathway for connecting the more applied work being conducted presently—touching on the specifics of contemporary systems via simulator theory, shard theory, autocurricula, etc.—with the esoteric realm of idealized reasoning. Conversely, the present work could provide a pathway to [better connect](https://www.alignmentforum.org/posts/nzmCvRvPm4xJuqztv/deepmind-is-hiring-for-the-scalable-alignment-and-alignment#Scalable_Alignment_s_current_approach__make_AI_critique_itself) classical debate with the type of systems likely to be developed in the near future. Alternatively, the _ArgRank_ operationalization might help address some of the challenges otherwise faced by the human judge, although it might also introduce others.

This concludes our preliminary scaffolding of conceptual bridges, and with that, our broader discussion around ways in which one might apply the triad of artifacts for addressing the alignment problem.

## Ch. V, Analysis

## Appendix

### Future Work

As children delight themselves in grappling with demanding puzzles, there is scarcely anything more gratifying than grappling with enigmas as gargantuan as truth, beauty, will, life, etc. In the whimsical playground structured by the chronicles, computational artifacts take the place of colorful cubes, theoretical formalisms those of little cylinders, and conceptual frameworks those of quaint prisms. Armed with these building blocks, we started constructing a system of philosophy which is to be applied not by observing its precepts in daily life, but by engineering it into the most far-reaching of artifacts–and in doing so, attempting timelessness.

<div class='epigraph'><blockquote><p>With an inked brush he marked everything with its name: table, chair, clock, door, wall, bed, pan. He went to the corral and marked the animals and plants: cow, goat, pig, hen, cassava, caladium, banana. Little by little, studying the infinite possibilities of a loss of memory, he realized that the day might come when things would be recognized by their inscriptions but that no one would remember their use. Then he was more explicit. The sign that he hung on the neck of the cow was an exemplary proof of the way in which the inhabitants of Macondo were prepared to fight against loss of memory: This is the cow. She must be milked every morning so that she will produce milk, and the milk must be boiled in order to be mixed with coffee to make coffee and milk. Thus they went on living in a reality that was slipping away, momentarily captured by words, but which would escape irremediably when they forgot the values of the written letters. [...] Then he understood. He opened the suitcase crammed with indecipherable objects and from among them he took out a little case with many flasks. He gave José Arcadio Buendía a drink of a gentle color and the light went on in his memory. His eyes became moist from weeping even before he noticed himself in an absurd living room where objects were labeled and before he was ashamed of the solemn nonsense written on the walls, and even before he recognized the newcomer with a dazzling glow of joy.</p>
<div>Gabriel García Márquez,<cite> One Hundred Years of Solitude</cite></div></blockquote></div>

However, much remains to be done. Furthering this enterprise in a satisfactory manner requires the boldness and ingenuity of aspiring researchers with diverse backgrounds. For a taste of this much-needed variety, one might refer to [the tentative titles of upcoming volumes](https://thechronicom.github.io), although—as this first volume so clearly demonstrated—each of those topics is woven into a dense interdisciplinary fabric, with tight links to fields driven by radically different motivations and, frankly, cultures. Besides, this first volume has also demonstrated that researchers with varied levels of experience can each provide valuable input, from students all the way to professors. Aspiring researchers interested in contributing to this overarching endeavor might benefit from the mentorship program described in the final section. Additionally, readers intrigued by the alignment problem more broadly might also benefit from the [online resources](https://80000hours.org/problem-profiles/artificial-intelligence/) and [free career consulting](https://80000hours.org/speak-with-us/?int_campaign=2021-08__primary-navigation) offered by the non-profit [80,000 Hours](https://80000hours.org/).

<div class='epigraph'><blockquote><p>80,000 Hours provides research and support to help students and graduates switch into careers that effectively tackle the world’s most pressing problems. [...] To learn more about why we think your choice of career is the biggest ethical decision you’ll ever make, and why we think you can probably have a lot more impact with your career, start here.</p>
<div>80,000 Hours Team,<cite> <a href="https://80000hours.org/about/">About Us</a></cite></div></blockquote></div>

Besides researchers, the current work would have hardly been possible without the financial and computational resources necessary for the kind of ["blue skies"](https://en.wikipedia.org/wiki/Blue_skies_research) and ["derisking"](https://notes.benjaminreinhardt.com/DARPA_provides_a_derisking_role_for_people_in_other_organizations) research we have undertaken. We are considering the symbolic sale of related artifacts (e.g. paperback versions of the volumes) to make the effort more sustainable, although the incentives typically associated with such practices tend to detract and distract from meaningful work.{% include sidenote.html id="mn-cc" note='Besides, there might be an awkward interference with the [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/) license which we decided to adopt for the present work, which allows anybody to "remix, transform, and build upon the material for any purpose, even commercially."'%} Given this, we are keeping our options open for now, and so ask entities—individuals and organizations alike—who resonate with this enterprise and would like to support it, to reach out privately. That way, we can quickly figure out the simplest legal means of moving financial or computational resources around.

### Evergreen Discussion

By virtue of having made it to the end of this artifact, you are ideally positioned to submit a brief review, in order to inform the creation of upcoming volumes and help subsequent readers budget their time wisely. If you are considering such a contribution, please submit it through the [Goodreads]() listing of the present volume.

Additionally, if you have inquiries regarding the topics addressed in this work, do not hesitate to pose them as questions on the same [Goodreads]() listing, where the team will reply asynchronously, availability permitting. Indeed, other readers are likely to have inquiries similar to yours, so there is value to breaking the ice.

In addition, thoughtful criticism of the ideas proposed in this volume is welcome (see _Chapter IV_). Such critiques could be communicated through e.g. essays published on a third-party platform, and are likely to be responded to by means of new sections of the volume. If building on this work in your own, please cite it as:

`*bibtex citation goes here*`

### The Chronicler Apprenticeship

In an attempt to promote efforts directed at operationalizing fundamental philosophical notions, we are launching a lightweight mentorship program aimed at individuals who would like to eventually transition into full-time work as the first author of an artifact of similar shape and size to the present one. The no-fuss program involves two hours of open-ended mentorship per month, arranged flexibly and on-demand through a basic scheduling platform. The program is currently facilitated by the first author of the current volume, yet is designed to expand over time as others assume similar roles.

In order to apply, please email [paulbricman@protonmail.com](mailto:paulbricman@protonmail.com) with the subject _The Chronicler Apprenticeship Application_, and include the following information: (1) an overview of your career trajectory (i.e. background and future plans), (2) an overview of experience you deem relevant (e.g. past papers, projects, essays, etc.), and (3) an overview of how you think the mentorship program might benefit you. Expect a brief interview before a decision on availability.

There are no constraints related to professional experience, country of residence, writing proficiency, or web design skills. The program is not paid (on either side), though fruitful mentorship would naturally lead to advice on obtaining funding for related work. Aspiring philosophers, mathematicians, engineers, artists, and combinations thereof are all welcome.
